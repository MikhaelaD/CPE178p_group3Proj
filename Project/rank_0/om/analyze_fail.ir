# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For primitive[Conv2D], the input type must be same.
name:[w]:Ref[Tensor[Float32]].
name:[x]:Tensor[UInt8].

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\utils\check_convert_utils.cc:1029 mindspore::CheckAndConvertUtils::_CheckTypeSame

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:424
        if self.return_grad:
# 2 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419
        loss = self.network(*inputs)
               ^
# 3 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 4 In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:207
# 5 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362
        if self.has_bias:
# 6 In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361
        output = self.conv2d(x, self.weight)
                 ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2
# Total subgraphs: 525

# Attrs:
training : 1

# Total params: 433
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (64, 3, 7, 7), ref_key=:conv1.weight>  :  has_default
%para4_bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:bn1.gamma>  :  has_default
%para5_bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:bn1.beta>  :  has_default
%para6_layer1.0.conv1.weight : <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:layer1.0.conv1.weight>  :  has_default
%para7_layer1.0.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.gamma>  :  has_default
%para8_layer1.0.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.beta>  :  has_default
%para9_layer1.0.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.0.conv2.weight>  :  has_default
%para10_layer1.0.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.gamma>  :  has_default
%para11_layer1.0.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.beta>  :  has_default
%para12_layer1.0.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.conv3.weight>  :  has_default
%para13_layer1.0.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.gamma>  :  has_default
%para14_layer1.0.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.beta>  :  has_default
%para15_layer1.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.down_sample_layer.0.weight>  :  has_default
%para16_layer1.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.gamma>  :  has_default
%para17_layer1.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.beta>  :  has_default
%para18_layer1.1.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.1.conv1.weight>  :  has_default
%para19_layer1.1.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.gamma>  :  has_default
%para20_layer1.1.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.beta>  :  has_default
%para21_layer1.1.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.1.conv2.weight>  :  has_default
%para22_layer1.1.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.gamma>  :  has_default
%para23_layer1.1.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.beta>  :  has_default
%para24_layer1.1.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.1.conv3.weight>  :  has_default
%para25_layer1.1.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.gamma>  :  has_default
%para26_layer1.1.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.beta>  :  has_default
%para27_layer1.2.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.2.conv1.weight>  :  has_default
%para28_layer1.2.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.gamma>  :  has_default
%para29_layer1.2.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.beta>  :  has_default
%para30_layer1.2.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.2.conv2.weight>  :  has_default
%para31_layer1.2.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.gamma>  :  has_default
%para32_layer1.2.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.beta>  :  has_default
%para33_layer1.2.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.2.conv3.weight>  :  has_default
%para34_layer1.2.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.gamma>  :  has_default
%para35_layer1.2.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.beta>  :  has_default
%para36_layer2.0.conv1.weight : <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:layer2.0.conv1.weight>  :  has_default
%para37_layer2.0.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.gamma>  :  has_default
%para38_layer2.0.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.beta>  :  has_default
%para39_layer2.0.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.0.conv2.weight>  :  has_default
%para40_layer2.0.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.gamma>  :  has_default
%para41_layer2.0.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.beta>  :  has_default
%para42_layer2.0.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.0.conv3.weight>  :  has_default
%para43_layer2.0.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.gamma>  :  has_default
%para44_layer2.0.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.beta>  :  has_default
%para45_layer2.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:layer2.0.down_sample_layer.0.weight>  :  has_default
%para46_layer2.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.gamma>  :  has_default
%para47_layer2.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.beta>  :  has_default
%para48_layer2.1.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.1.conv1.weight>  :  has_default
%para49_layer2.1.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.gamma>  :  has_default
%para50_layer2.1.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.beta>  :  has_default
%para51_layer2.1.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.1.conv2.weight>  :  has_default
%para52_layer2.1.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.gamma>  :  has_default
%para53_layer2.1.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.beta>  :  has_default
%para54_layer2.1.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.1.conv3.weight>  :  has_default
%para55_layer2.1.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.gamma>  :  has_default
%para56_layer2.1.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.beta>  :  has_default
%para57_layer2.2.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.2.conv1.weight>  :  has_default
%para58_layer2.2.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.gamma>  :  has_default
%para59_layer2.2.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.beta>  :  has_default
%para60_layer2.2.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.2.conv2.weight>  :  has_default
%para61_layer2.2.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.gamma>  :  has_default
%para62_layer2.2.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.beta>  :  has_default
%para63_layer2.2.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.2.conv3.weight>  :  has_default
%para64_layer2.2.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.gamma>  :  has_default
%para65_layer2.2.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.beta>  :  has_default
%para66_layer2.3.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.3.conv1.weight>  :  has_default
%para67_layer2.3.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.gamma>  :  has_default
%para68_layer2.3.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.beta>  :  has_default
%para69_layer2.3.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.3.conv2.weight>  :  has_default
%para70_layer2.3.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.gamma>  :  has_default
%para71_layer2.3.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.beta>  :  has_default
%para72_layer2.3.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.3.conv3.weight>  :  has_default
%para73_layer2.3.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.gamma>  :  has_default
%para74_layer2.3.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.beta>  :  has_default
%para75_layer3.0.conv1.weight : <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:layer3.0.conv1.weight>  :  has_default
%para76_layer3.0.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.gamma>  :  has_default
%para77_layer3.0.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.beta>  :  has_default
%para78_layer3.0.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.0.conv2.weight>  :  has_default
%para79_layer3.0.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.gamma>  :  has_default
%para80_layer3.0.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.beta>  :  has_default
%para81_layer3.0.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.0.conv3.weight>  :  has_default
%para82_layer3.0.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.gamma>  :  has_default
%para83_layer3.0.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.beta>  :  has_default
%para84_layer3.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:layer3.0.down_sample_layer.0.weight>  :  has_default
%para85_layer3.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>  :  has_default
%para86_layer3.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>  :  has_default
%para87_layer3.1.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.1.conv1.weight>  :  has_default
%para88_layer3.1.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.gamma>  :  has_default
%para89_layer3.1.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.beta>  :  has_default
%para90_layer3.1.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.1.conv2.weight>  :  has_default
%para91_layer3.1.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.gamma>  :  has_default
%para92_layer3.1.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.beta>  :  has_default
%para93_layer3.1.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.1.conv3.weight>  :  has_default
%para94_layer3.1.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.gamma>  :  has_default
%para95_layer3.1.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.beta>  :  has_default
%para96_layer3.2.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.2.conv1.weight>  :  has_default
%para97_layer3.2.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.gamma>  :  has_default
%para98_layer3.2.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.beta>  :  has_default
%para99_layer3.2.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.2.conv2.weight>  :  has_default
%para100_layer3.2.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.gamma>  :  has_default
%para101_layer3.2.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.beta>  :  has_default
%para102_layer3.2.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.2.conv3.weight>  :  has_default
%para103_layer3.2.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.gamma>  :  has_default
%para104_layer3.2.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.beta>  :  has_default
%para105_layer3.3.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.3.conv1.weight>  :  has_default
%para106_layer3.3.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.gamma>  :  has_default
%para107_layer3.3.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.beta>  :  has_default
%para108_layer3.3.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.3.conv2.weight>  :  has_default
%para109_layer3.3.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.gamma>  :  has_default
%para110_layer3.3.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.beta>  :  has_default
%para111_layer3.3.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.3.conv3.weight>  :  has_default
%para112_layer3.3.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.gamma>  :  has_default
%para113_layer3.3.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.beta>  :  has_default
%para114_layer3.4.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.4.conv1.weight>  :  has_default
%para115_layer3.4.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.gamma>  :  has_default
%para116_layer3.4.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.beta>  :  has_default
%para117_layer3.4.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.4.conv2.weight>  :  has_default
%para118_layer3.4.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.gamma>  :  has_default
%para119_layer3.4.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.beta>  :  has_default
%para120_layer3.4.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.4.conv3.weight>  :  has_default
%para121_layer3.4.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.gamma>  :  has_default
%para122_layer3.4.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.beta>  :  has_default
%para123_layer3.5.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.5.conv1.weight>  :  has_default
%para124_layer3.5.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.gamma>  :  has_default
%para125_layer3.5.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.beta>  :  has_default
%para126_layer3.5.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.5.conv2.weight>  :  has_default
%para127_layer3.5.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.gamma>  :  has_default
%para128_layer3.5.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.beta>  :  has_default
%para129_layer3.5.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.5.conv3.weight>  :  has_default
%para130_layer3.5.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.gamma>  :  has_default
%para131_layer3.5.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.beta>  :  has_default
%para132_layer4.0.conv1.weight : <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:layer4.0.conv1.weight>  :  has_default
%para133_layer4.0.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.gamma>  :  has_default
%para134_layer4.0.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.beta>  :  has_default
%para135_layer4.0.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.0.conv2.weight>  :  has_default
%para136_layer4.0.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.gamma>  :  has_default
%para137_layer4.0.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.beta>  :  has_default
%para138_layer4.0.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.0.conv3.weight>  :  has_default
%para139_layer4.0.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.gamma>  :  has_default
%para140_layer4.0.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.beta>  :  has_default
%para141_layer4.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:layer4.0.down_sample_layer.0.weight>  :  has_default
%para142_layer4.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>  :  has_default
%para143_layer4.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>  :  has_default
%para144_layer4.1.conv1.weight : <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.1.conv1.weight>  :  has_default
%para145_layer4.1.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.gamma>  :  has_default
%para146_layer4.1.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.beta>  :  has_default
%para147_layer4.1.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.1.conv2.weight>  :  has_default
%para148_layer4.1.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.gamma>  :  has_default
%para149_layer4.1.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.beta>  :  has_default
%para150_layer4.1.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.1.conv3.weight>  :  has_default
%para151_layer4.1.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.gamma>  :  has_default
%para152_layer4.1.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.beta>  :  has_default
%para153_layer4.2.conv1.weight : <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.2.conv1.weight>  :  has_default
%para154_layer4.2.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.gamma>  :  has_default
%para155_layer4.2.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.beta>  :  has_default
%para156_layer4.2.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.2.conv2.weight>  :  has_default
%para157_layer4.2.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.gamma>  :  has_default
%para158_layer4.2.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.beta>  :  has_default
%para159_layer4.2.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.2.conv3.weight>  :  has_default
%para160_layer4.2.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.gamma>  :  has_default
%para161_layer4.2.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.beta>  :  has_default
%para162_end_point.weight : <Ref[Tensor[Float32]], (4, 2048), ref_key=:end_point.weight>  :  has_default
%para163_end_point.bias : <Ref[Tensor[Float32]], (4), ref_key=:end_point.bias>  :  has_default
%para164_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para165_moments.conv1.weight : <Ref[Tensor[Float32]], (64, 3, 7, 7), ref_key=:moments.conv1.weight>  :  has_default
%para166_moments.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.bn1.gamma>  :  has_default
%para167_moments.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.bn1.beta>  :  has_default
%para168_moments.layer1.0.conv1.weight : <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:moments.layer1.0.conv1.weight>  :  has_default
%para169_moments.layer1.0.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn1.gamma>  :  has_default
%para170_moments.layer1.0.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn1.beta>  :  has_default
%para171_moments.layer1.0.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.0.conv2.weight>  :  has_default
%para172_moments.layer1.0.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn2.gamma>  :  has_default
%para173_moments.layer1.0.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn2.beta>  :  has_default
%para174_moments.layer1.0.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.0.conv3.weight>  :  has_default
%para175_moments.layer1.0.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.bn3.gamma>  :  has_default
%para176_moments.layer1.0.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.bn3.beta>  :  has_default
%para177_moments.layer1.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.0.down_sample_layer.0.weight>  :  has_default
%para178_moments.layer1.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.down_sample_layer.1.gamma>  :  has_default
%para179_moments.layer1.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.down_sample_layer.1.beta>  :  has_default
%para180_moments.layer1.1.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moments.layer1.1.conv1.weight>  :  has_default
%para181_moments.layer1.1.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn1.gamma>  :  has_default
%para182_moments.layer1.1.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn1.beta>  :  has_default
%para183_moments.layer1.1.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.1.conv2.weight>  :  has_default
%para184_moments.layer1.1.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn2.gamma>  :  has_default
%para185_moments.layer1.1.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn2.beta>  :  has_default
%para186_moments.layer1.1.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.1.conv3.weight>  :  has_default
%para187_moments.layer1.1.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.1.bn3.gamma>  :  has_default
%para188_moments.layer1.1.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.1.bn3.beta>  :  has_default
%para189_moments.layer1.2.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moments.layer1.2.conv1.weight>  :  has_default
%para190_moments.layer1.2.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn1.gamma>  :  has_default
%para191_moments.layer1.2.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn1.beta>  :  has_default
%para192_moments.layer1.2.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.2.conv2.weight>  :  has_default
%para193_moments.layer1.2.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn2.gamma>  :  has_default
%para194_moments.layer1.2.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn2.beta>  :  has_default
%para195_moments.layer1.2.conv3.weight : <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.2.conv3.weight>  :  has_default
%para196_moments.layer1.2.bn3.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.2.bn3.gamma>  :  has_default
%para197_moments.layer1.2.bn3.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.2.bn3.beta>  :  has_default
%para198_moments.layer2.0.conv1.weight : <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:moments.layer2.0.conv1.weight>  :  has_default
%para199_moments.layer2.0.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn1.gamma>  :  has_default
%para200_moments.layer2.0.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn1.beta>  :  has_default
%para201_moments.layer2.0.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.0.conv2.weight>  :  has_default
%para202_moments.layer2.0.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn2.gamma>  :  has_default
%para203_moments.layer2.0.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn2.beta>  :  has_default
%para204_moments.layer2.0.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.0.conv3.weight>  :  has_default
%para205_moments.layer2.0.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.bn3.gamma>  :  has_default
%para206_moments.layer2.0.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.bn3.beta>  :  has_default
%para207_moments.layer2.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moments.layer2.0.down_sample_layer.0.weight>  :  has_default
%para208_moments.layer2.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.down_sample_layer.1.gamma>  :  has_default
%para209_moments.layer2.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.down_sample_layer.1.beta>  :  has_default
%para210_moments.layer2.1.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.1.conv1.weight>  :  has_default
%para211_moments.layer2.1.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn1.gamma>  :  has_default
%para212_moments.layer2.1.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn1.beta>  :  has_default
%para213_moments.layer2.1.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.1.conv2.weight>  :  has_default
%para214_moments.layer2.1.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn2.gamma>  :  has_default
%para215_moments.layer2.1.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn2.beta>  :  has_default
%para216_moments.layer2.1.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.1.conv3.weight>  :  has_default
%para217_moments.layer2.1.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.1.bn3.gamma>  :  has_default
%para218_moments.layer2.1.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.1.bn3.beta>  :  has_default
%para219_moments.layer2.2.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.2.conv1.weight>  :  has_default
%para220_moments.layer2.2.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn1.gamma>  :  has_default
%para221_moments.layer2.2.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn1.beta>  :  has_default
%para222_moments.layer2.2.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.2.conv2.weight>  :  has_default
%para223_moments.layer2.2.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn2.gamma>  :  has_default
%para224_moments.layer2.2.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn2.beta>  :  has_default
%para225_moments.layer2.2.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.2.conv3.weight>  :  has_default
%para226_moments.layer2.2.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.2.bn3.gamma>  :  has_default
%para227_moments.layer2.2.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.2.bn3.beta>  :  has_default
%para228_moments.layer2.3.conv1.weight : <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.3.conv1.weight>  :  has_default
%para229_moments.layer2.3.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn1.gamma>  :  has_default
%para230_moments.layer2.3.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn1.beta>  :  has_default
%para231_moments.layer2.3.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.3.conv2.weight>  :  has_default
%para232_moments.layer2.3.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn2.gamma>  :  has_default
%para233_moments.layer2.3.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn2.beta>  :  has_default
%para234_moments.layer2.3.conv3.weight : <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.3.conv3.weight>  :  has_default
%para235_moments.layer2.3.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.3.bn3.gamma>  :  has_default
%para236_moments.layer2.3.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.3.bn3.beta>  :  has_default
%para237_moments.layer3.0.conv1.weight : <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:moments.layer3.0.conv1.weight>  :  has_default
%para238_moments.layer3.0.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn1.gamma>  :  has_default
%para239_moments.layer3.0.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn1.beta>  :  has_default
%para240_moments.layer3.0.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.0.conv2.weight>  :  has_default
%para241_moments.layer3.0.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn2.gamma>  :  has_default
%para242_moments.layer3.0.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn2.beta>  :  has_default
%para243_moments.layer3.0.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.0.conv3.weight>  :  has_default
%para244_moments.layer3.0.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.bn3.gamma>  :  has_default
%para245_moments.layer3.0.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.bn3.beta>  :  has_default
%para246_moments.layer3.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:moments.layer3.0.down_sample_layer.0.weight>  :  has_default
%para247_moments.layer3.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.down_sample_layer.1.gamma>  :  has_default
%para248_moments.layer3.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.down_sample_layer.1.beta>  :  has_default
%para249_moments.layer3.1.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.1.conv1.weight>  :  has_default
%para250_moments.layer3.1.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn1.gamma>  :  has_default
%para251_moments.layer3.1.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn1.beta>  :  has_default
%para252_moments.layer3.1.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.1.conv2.weight>  :  has_default
%para253_moments.layer3.1.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn2.gamma>  :  has_default
%para254_moments.layer3.1.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn2.beta>  :  has_default
%para255_moments.layer3.1.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.1.conv3.weight>  :  has_default
%para256_moments.layer3.1.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.1.bn3.gamma>  :  has_default
%para257_moments.layer3.1.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.1.bn3.beta>  :  has_default
%para258_moments.layer3.2.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.2.conv1.weight>  :  has_default
%para259_moments.layer3.2.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn1.gamma>  :  has_default
%para260_moments.layer3.2.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn1.beta>  :  has_default
%para261_moments.layer3.2.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.2.conv2.weight>  :  has_default
%para262_moments.layer3.2.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn2.gamma>  :  has_default
%para263_moments.layer3.2.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn2.beta>  :  has_default
%para264_moments.layer3.2.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.2.conv3.weight>  :  has_default
%para265_moments.layer3.2.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.2.bn3.gamma>  :  has_default
%para266_moments.layer3.2.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.2.bn3.beta>  :  has_default
%para267_moments.layer3.3.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.3.conv1.weight>  :  has_default
%para268_moments.layer3.3.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn1.gamma>  :  has_default
%para269_moments.layer3.3.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn1.beta>  :  has_default
%para270_moments.layer3.3.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.3.conv2.weight>  :  has_default
%para271_moments.layer3.3.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn2.gamma>  :  has_default
%para272_moments.layer3.3.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn2.beta>  :  has_default
%para273_moments.layer3.3.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.3.conv3.weight>  :  has_default
%para274_moments.layer3.3.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.3.bn3.gamma>  :  has_default
%para275_moments.layer3.3.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.3.bn3.beta>  :  has_default
%para276_moments.layer3.4.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.4.conv1.weight>  :  has_default
%para277_moments.layer3.4.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn1.gamma>  :  has_default
%para278_moments.layer3.4.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn1.beta>  :  has_default
%para279_moments.layer3.4.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.4.conv2.weight>  :  has_default
%para280_moments.layer3.4.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn2.gamma>  :  has_default
%para281_moments.layer3.4.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn2.beta>  :  has_default
%para282_moments.layer3.4.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.4.conv3.weight>  :  has_default
%para283_moments.layer3.4.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.4.bn3.gamma>  :  has_default
%para284_moments.layer3.4.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.4.bn3.beta>  :  has_default
%para285_moments.layer3.5.conv1.weight : <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.5.conv1.weight>  :  has_default
%para286_moments.layer3.5.bn1.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn1.gamma>  :  has_default
%para287_moments.layer3.5.bn1.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn1.beta>  :  has_default
%para288_moments.layer3.5.conv2.weight : <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.5.conv2.weight>  :  has_default
%para289_moments.layer3.5.bn2.gamma : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn2.gamma>  :  has_default
%para290_moments.layer3.5.bn2.beta : <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn2.beta>  :  has_default
%para291_moments.layer3.5.conv3.weight : <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.5.conv3.weight>  :  has_default
%para292_moments.layer3.5.bn3.gamma : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.5.bn3.gamma>  :  has_default
%para293_moments.layer3.5.bn3.beta : <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.5.bn3.beta>  :  has_default
%para294_moments.layer4.0.conv1.weight : <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:moments.layer4.0.conv1.weight>  :  has_default
%para295_moments.layer4.0.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn1.gamma>  :  has_default
%para296_moments.layer4.0.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn1.beta>  :  has_default
%para297_moments.layer4.0.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.0.conv2.weight>  :  has_default
%para298_moments.layer4.0.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn2.gamma>  :  has_default
%para299_moments.layer4.0.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn2.beta>  :  has_default
%para300_moments.layer4.0.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.0.conv3.weight>  :  has_default
%para301_moments.layer4.0.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.bn3.gamma>  :  has_default
%para302_moments.layer4.0.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.bn3.beta>  :  has_default
%para303_moments.layer4.0.down_sample_layer.0.weight : <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:moments.layer4.0.down_sample_layer.0.weight>  :  has_default
%para304_moments.layer4.0.down_sample_layer.1.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.down_sample_layer.1.gamma>  :  has_default
%para305_moments.layer4.0.down_sample_layer.1.beta : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.down_sample_layer.1.beta>  :  has_default
%para306_moments.layer4.1.conv1.weight : <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:moments.layer4.1.conv1.weight>  :  has_default
%para307_moments.layer4.1.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn1.gamma>  :  has_default
%para308_moments.layer4.1.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn1.beta>  :  has_default
%para309_moments.layer4.1.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.1.conv2.weight>  :  has_default
%para310_moments.layer4.1.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn2.gamma>  :  has_default
%para311_moments.layer4.1.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn2.beta>  :  has_default
%para312_moments.layer4.1.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.1.conv3.weight>  :  has_default
%para313_moments.layer4.1.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.1.bn3.gamma>  :  has_default
%para314_moments.layer4.1.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.1.bn3.beta>  :  has_default
%para315_moments.layer4.2.conv1.weight : <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:moments.layer4.2.conv1.weight>  :  has_default
%para316_moments.layer4.2.bn1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn1.gamma>  :  has_default
%para317_moments.layer4.2.bn1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn1.beta>  :  has_default
%para318_moments.layer4.2.conv2.weight : <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.2.conv2.weight>  :  has_default
%para319_moments.layer4.2.bn2.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn2.gamma>  :  has_default
%para320_moments.layer4.2.bn2.beta : <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn2.beta>  :  has_default
%para321_moments.layer4.2.conv3.weight : <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.2.conv3.weight>  :  has_default
%para322_moments.layer4.2.bn3.gamma : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.2.bn3.gamma>  :  has_default
%para323_moments.layer4.2.bn3.beta : <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.2.bn3.beta>  :  has_default
%para324_moments.end_point.weight : <Ref[Tensor[Float32]], (4, 2048), ref_key=:moments.end_point.weight>  :  has_default
%para325_moments.end_point.bias : <Ref[Tensor[Float32]], (4), ref_key=:moments.end_point.bias>  :  has_default
%para326_momentum : <Ref[Tensor[Float32]], (), ref_key=:momentum>  :  has_default
%para327_bn1.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:bn1.moving_mean>  :  has_default
%para328_bn1.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:bn1.moving_variance>  :  has_default
%para329_learning_rate : <Ref[Tensor[Float32]], (885), ref_key=:learning_rate>  :  has_default
%para330_layer4.0.bn3.moving_mean : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.moving_mean>  :  has_default
%para331_layer4.0.bn3.moving_variance : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.moving_variance>  :  has_default
%para332_layer4.1.bn3.moving_mean : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.moving_mean>  :  has_default
%para333_layer4.1.bn3.moving_variance : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.moving_variance>  :  has_default
%para334_layer4.2.bn3.moving_mean : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.moving_mean>  :  has_default
%para335_layer4.2.bn3.moving_variance : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.moving_variance>  :  has_default
%para336_layer3.0.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.moving_mean>  :  has_default
%para337_layer3.0.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.moving_variance>  :  has_default
%para338_layer3.1.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.moving_mean>  :  has_default
%para339_layer3.1.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.moving_variance>  :  has_default
%para340_layer3.2.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.moving_mean>  :  has_default
%para341_layer3.2.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.moving_variance>  :  has_default
%para342_layer3.3.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.moving_mean>  :  has_default
%para343_layer3.3.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.moving_variance>  :  has_default
%para344_layer3.4.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.moving_mean>  :  has_default
%para345_layer3.4.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.moving_variance>  :  has_default
%para346_layer3.5.bn3.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.moving_mean>  :  has_default
%para347_layer3.5.bn3.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.moving_variance>  :  has_default
%para348_layer2.0.bn3.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.moving_mean>  :  has_default
%para349_layer2.0.bn3.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.moving_variance>  :  has_default
%para350_layer2.1.bn3.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.moving_mean>  :  has_default
%para351_layer2.1.bn3.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.moving_variance>  :  has_default
%para352_layer2.2.bn3.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.moving_mean>  :  has_default
%para353_layer2.2.bn3.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.moving_variance>  :  has_default
%para354_layer2.3.bn3.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.moving_mean>  :  has_default
%para355_layer2.3.bn3.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.moving_variance>  :  has_default
%para356_layer4.0.bn2.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.moving_mean>  :  has_default
%para357_layer4.0.bn2.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.moving_variance>  :  has_default
%para358_layer4.1.bn2.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.moving_mean>  :  has_default
%para359_layer4.1.bn2.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.moving_variance>  :  has_default
%para360_layer4.2.bn2.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.moving_mean>  :  has_default
%para361_layer4.2.bn2.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.moving_variance>  :  has_default
%para362_layer1.0.bn3.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.moving_mean>  :  has_default
%para363_layer1.0.bn3.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.moving_variance>  :  has_default
%para364_layer1.1.bn3.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.moving_mean>  :  has_default
%para365_layer1.1.bn3.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.moving_variance>  :  has_default
%para366_layer1.2.bn3.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.moving_mean>  :  has_default
%para367_layer1.2.bn3.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.moving_variance>  :  has_default
%para368_layer3.0.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.moving_mean>  :  has_default
%para369_layer3.0.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.moving_variance>  :  has_default
%para370_layer3.1.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.moving_mean>  :  has_default
%para371_layer3.1.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.moving_variance>  :  has_default
%para372_layer3.2.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.moving_mean>  :  has_default
%para373_layer3.2.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.moving_variance>  :  has_default
%para374_layer3.3.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.moving_mean>  :  has_default
%para375_layer3.3.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.moving_variance>  :  has_default
%para376_layer3.4.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.moving_mean>  :  has_default
%para377_layer3.4.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.moving_variance>  :  has_default
%para378_layer3.5.bn2.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.moving_mean>  :  has_default
%para379_layer3.5.bn2.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.moving_variance>  :  has_default
%para380_layer2.0.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.moving_mean>  :  has_default
%para381_layer2.0.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.moving_variance>  :  has_default
%para382_layer2.1.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.moving_mean>  :  has_default
%para383_layer2.1.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.moving_variance>  :  has_default
%para384_layer2.2.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.moving_mean>  :  has_default
%para385_layer2.2.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.moving_variance>  :  has_default
%para386_layer2.3.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.moving_mean>  :  has_default
%para387_layer2.3.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.moving_variance>  :  has_default
%para388_layer4.0.bn1.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.moving_mean>  :  has_default
%para389_layer4.0.bn1.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.moving_variance>  :  has_default
%para390_layer4.1.bn1.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.moving_mean>  :  has_default
%para391_layer4.1.bn1.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.moving_variance>  :  has_default
%para392_layer4.2.bn1.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.moving_mean>  :  has_default
%para393_layer4.2.bn1.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.moving_variance>  :  has_default
%para394_layer1.0.bn2.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.moving_mean>  :  has_default
%para395_layer1.0.bn2.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.moving_variance>  :  has_default
%para396_layer1.1.bn2.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.moving_mean>  :  has_default
%para397_layer1.1.bn2.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.moving_variance>  :  has_default
%para398_layer1.2.bn2.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.moving_mean>  :  has_default
%para399_layer1.2.bn2.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.moving_variance>  :  has_default
%para400_layer3.0.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.moving_mean>  :  has_default
%para401_layer3.0.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.moving_variance>  :  has_default
%para402_layer3.1.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.moving_mean>  :  has_default
%para403_layer3.1.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.moving_variance>  :  has_default
%para404_layer3.2.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.moving_mean>  :  has_default
%para405_layer3.2.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.moving_variance>  :  has_default
%para406_layer3.3.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.moving_mean>  :  has_default
%para407_layer3.3.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.moving_variance>  :  has_default
%para408_layer3.4.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.moving_mean>  :  has_default
%para409_layer3.4.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.moving_variance>  :  has_default
%para410_layer3.5.bn1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.moving_mean>  :  has_default
%para411_layer3.5.bn1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.moving_variance>  :  has_default
%para412_layer2.0.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.moving_mean>  :  has_default
%para413_layer2.0.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.moving_variance>  :  has_default
%para414_layer2.1.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.moving_mean>  :  has_default
%para415_layer2.1.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.moving_variance>  :  has_default
%para416_layer2.2.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.moving_mean>  :  has_default
%para417_layer2.2.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.moving_variance>  :  has_default
%para418_layer2.3.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.moving_mean>  :  has_default
%para419_layer2.3.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.moving_variance>  :  has_default
%para420_layer1.0.bn1.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.moving_mean>  :  has_default
%para421_layer1.0.bn1.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.moving_variance>  :  has_default
%para422_layer1.1.bn1.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.moving_mean>  :  has_default
%para423_layer1.1.bn1.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.moving_variance>  :  has_default
%para424_layer1.2.bn1.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.moving_mean>  :  has_default
%para425_layer1.2.bn1.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.moving_variance>  :  has_default
%para426_layer4.0.down_sample_layer.1.moving_mean : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_mean>  :  has_default
%para427_layer4.0.down_sample_layer.1.moving_variance : <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_variance>  :  has_default
%para428_layer3.0.down_sample_layer.1.moving_mean : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_mean>  :  has_default
%para429_layer3.0.down_sample_layer.1.moving_variance : <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_variance>  :  has_default
%para430_layer2.0.down_sample_layer.1.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.moving_mean>  :  has_default
%para431_layer2.0.down_sample_layer.1.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.moving_variance>  :  has_default
%para432_layer1.0.down_sample_layer.1.moving_mean : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.moving_mean>  :  has_default
%para433_layer1.0.down_sample_layer.1.moving_variance : <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.moving_variance>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2 : 0000029BC72B3330
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_layer1.0.conv1.weight, %para7_layer1.0.bn1.gamma, %para8_layer1.0.bn1.beta, %para9_layer1.0.conv2.weight, %para10_layer1.0.bn2.gamma, %para11_layer1.0.bn2.beta, %para12_layer1.0.conv3.weight, %para13_layer1.0.bn3.gamma, %para14_layer1.0.bn3.beta, %para15_layer1.0.down_sample_layer.0.weight, %para16_layer1.0.down_sample_layer.1.gamma, %para17_layer1.0.down_sample_layer.1.beta, %para18_layer1.1.conv1.weight, %para19_layer1.1.bn1.gamma, %para20_layer1.1.bn1.beta, %para21_layer1.1.conv2.weight, %para22_layer1.1.bn2.gamma, %para23_layer1.1.bn2.beta, %para24_layer1.1.conv3.weight, %para25_layer1.1.bn3.gamma, %para26_layer1.1.bn3.beta, %para27_layer1.2.conv1.weight, %para28_layer1.2.bn1.gamma, %para29_layer1.2.bn1.beta, %para30_layer1.2.conv2.weight, %para31_layer1.2.bn2.gamma, %para32_layer1.2.bn2.beta, %para33_layer1.2.conv3.weight, %para34_layer1.2.bn3.gamma, %para35_layer1.2.bn3.beta, %para36_layer2.0.conv1.weight, %para37_layer2.0.bn1.gamma, %para38_layer2.0.bn1.beta, %para39_layer2.0.conv2.weight, %para40_layer2.0.bn2.gamma, %para41_layer2.0.bn2.beta, %para42_layer2.0.conv3.weight, %para43_layer2.0.bn3.gamma, %para44_layer2.0.bn3.beta, %para45_layer2.0.down_sample_layer.0.weight, %para46_layer2.0.down_sample_layer.1.gamma, %para47_layer2.0.down_sample_layer.1.beta, %para48_layer2.1.conv1.weight, %para49_layer2.1.bn1.gamma, %para50_layer2.1.bn1.beta, %para51_layer2.1.conv2.weight, %para52_layer2.1.bn2.gamma, %para53_layer2.1.bn2.beta, %para54_layer2.1.conv3.weight, %para55_layer2.1.bn3.gamma, %para56_layer2.1.bn3.beta, %para57_layer2.2.conv1.weight, %para58_layer2.2.bn1.gamma, %para59_layer2.2.bn1.beta, %para60_layer2.2.conv2.weight, %para61_layer2.2.bn2.gamma, %para62_layer2.2.bn2.beta, %para63_layer2.2.conv3.weight, %para64_layer2.2.bn3.gamma, %para65_layer2.2.bn3.beta, %para66_layer2.3.conv1.weight, %para67_layer2.3.bn1.gamma, %para68_layer2.3.bn1.beta, %para69_layer2.3.conv2.weight, %para70_layer2.3.bn2.gamma, %para71_layer2.3.bn2.beta, %para72_layer2.3.conv3.weight, %para73_layer2.3.bn3.gamma, %para74_layer2.3.bn3.beta, %para75_layer3.0.conv1.weight, %para76_layer3.0.bn1.gamma, %para77_layer3.0.bn1.beta, %para78_layer3.0.conv2.weight, %para79_layer3.0.bn2.gamma, %para80_layer3.0.bn2.beta, %para81_layer3.0.conv3.weight, %para82_layer3.0.bn3.gamma, %para83_layer3.0.bn3.beta, %para84_layer3.0.down_sample_layer.0.weight, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para87_layer3.1.conv1.weight, %para88_layer3.1.bn1.gamma, %para89_layer3.1.bn1.beta, %para90_layer3.1.conv2.weight, %para91_layer3.1.bn2.gamma, %para92_layer3.1.bn2.beta, %para93_layer3.1.conv3.weight, %para94_layer3.1.bn3.gamma, %para95_layer3.1.bn3.beta, %para96_layer3.2.conv1.weight, %para97_layer3.2.bn1.gamma, %para98_layer3.2.bn1.beta, %para99_layer3.2.conv2.weight, %para100_layer3.2.bn2.gamma, %para101_layer3.2.bn2.beta, %para102_layer3.2.conv3.weight, %para103_layer3.2.bn3.gamma, %para104_layer3.2.bn3.beta, %para105_layer3.3.conv1.weight, %para106_layer3.3.bn1.gamma, %para107_layer3.3.bn1.beta, %para108_layer3.3.conv2.weight, %para109_layer3.3.bn2.gamma, %para110_layer3.3.bn2.beta, %para111_layer3.3.conv3.weight, %para112_layer3.3.bn3.gamma, %para113_layer3.3.bn3.beta, %para114_layer3.4.conv1.weight, %para115_layer3.4.bn1.gamma, %para116_layer3.4.bn1.beta, %para117_layer3.4.conv2.weight, %para118_layer3.4.bn2.gamma, %para119_layer3.4.bn2.beta, %para120_layer3.4.conv3.weight, %para121_layer3.4.bn3.gamma, %para122_layer3.4.bn3.beta, %para123_layer3.5.conv1.weight, %para124_layer3.5.bn1.gamma, %para125_layer3.5.bn1.beta, %para126_layer3.5.conv2.weight, %para127_layer3.5.bn2.gamma, %para128_layer3.5.bn2.beta, %para129_layer3.5.conv3.weight, %para130_layer3.5.bn3.gamma, %para131_layer3.5.bn3.beta, %para132_layer4.0.conv1.weight, %para133_layer4.0.bn1.gamma, %para134_layer4.0.bn1.beta, %para135_layer4.0.conv2.weight, %para136_layer4.0.bn2.gamma, %para137_layer4.0.bn2.beta, %para138_layer4.0.conv3.weight, %para139_layer4.0.bn3.gamma, %para140_layer4.0.bn3.beta, %para141_layer4.0.down_sample_layer.0.weight, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para144_layer4.1.conv1.weight, %para145_layer4.1.bn1.gamma, %para146_layer4.1.bn1.beta, %para147_layer4.1.conv2.weight, %para148_layer4.1.bn2.gamma, %para149_layer4.1.bn2.beta, %para150_layer4.1.conv3.weight, %para151_layer4.1.bn3.gamma, %para152_layer4.1.bn3.beta, %para153_layer4.2.conv1.weight, %para154_layer4.2.bn1.gamma, %para155_layer4.2.bn1.beta, %para156_layer4.2.conv2.weight, %para157_layer4.2.bn2.gamma, %para158_layer4.2.bn2.beta, %para159_layer4.2.conv3.weight, %para160_layer4.2.bn3.gamma, %para161_layer4.2.bn3.beta, %para162_end_point.weight, %para163_end_point.bias, %para164_global_step, %para165_moments.conv1.weight, %para166_moments.bn1.gamma, %para167_moments.bn1.beta, %para168_moments.layer1.0.conv1.weight, %para169_moments.layer1.0.bn1.gamma, %para170_moments.layer1.0.bn1.beta, %para171_moments.layer1.0.conv2.weight, %para172_moments.layer1.0.bn2.gamma, %para173_moments.layer1.0.bn2.beta, %para174_moments.layer1.0.conv3.weight, %para175_moments.layer1.0.bn3.gamma, %para176_moments.layer1.0.bn3.beta, %para177_moments.layer1.0.down_sample_layer.0.weight, %para178_moments.layer1.0.down_sample_layer.1.gamma, %para179_moments.layer1.0.down_sample_layer.1.beta, %para180_moments.layer1.1.conv1.weight, %para181_moments.layer1.1.bn1.gamma, %para182_moments.layer1.1.bn1.beta, %para183_moments.layer1.1.conv2.weight, %para184_moments.layer1.1.bn2.gamma, %para185_moments.layer1.1.bn2.beta, %para186_moments.layer1.1.conv3.weight, %para187_moments.layer1.1.bn3.gamma, %para188_moments.layer1.1.bn3.beta, %para189_moments.layer1.2.conv1.weight, %para190_moments.layer1.2.bn1.gamma, %para191_moments.layer1.2.bn1.beta, %para192_moments.layer1.2.conv2.weight, %para193_moments.layer1.2.bn2.gamma, %para194_moments.layer1.2.bn2.beta, %para195_moments.layer1.2.conv3.weight, %para196_moments.layer1.2.bn3.gamma, %para197_moments.layer1.2.bn3.beta, %para198_moments.layer2.0.conv1.weight, %para199_moments.layer2.0.bn1.gamma, %para200_moments.layer2.0.bn1.beta, %para201_moments.layer2.0.conv2.weight, %para202_moments.layer2.0.bn2.gamma, %para203_moments.layer2.0.bn2.beta, %para204_moments.layer2.0.conv3.weight, %para205_moments.layer2.0.bn3.gamma, %para206_moments.layer2.0.bn3.beta, %para207_moments.layer2.0.down_sample_layer.0.weight, %para208_moments.layer2.0.down_sample_layer.1.gamma, %para209_moments.layer2.0.down_sample_layer.1.beta, %para210_moments.layer2.1.conv1.weight, %para211_moments.layer2.1.bn1.gamma, %para212_moments.layer2.1.bn1.beta, %para213_moments.layer2.1.conv2.weight, %para214_moments.layer2.1.bn2.gamma, %para215_moments.layer2.1.bn2.beta, %para216_moments.layer2.1.conv3.weight, %para217_moments.layer2.1.bn3.gamma, %para218_moments.layer2.1.bn3.beta, %para219_moments.layer2.2.conv1.weight, %para220_moments.layer2.2.bn1.gamma, %para221_moments.layer2.2.bn1.beta, %para222_moments.layer2.2.conv2.weight, %para223_moments.layer2.2.bn2.gamma, %para224_moments.layer2.2.bn2.beta, %para225_moments.layer2.2.conv3.weight, %para226_moments.layer2.2.bn3.gamma, %para227_moments.layer2.2.bn3.beta, %para228_moments.layer2.3.conv1.weight, %para229_moments.layer2.3.bn1.gamma, %para230_moments.layer2.3.bn1.beta, %para231_moments.layer2.3.conv2.weight, %para232_moments.layer2.3.bn2.gamma, %para233_moments.layer2.3.bn2.beta, %para234_moments.layer2.3.conv3.weight, %para235_moments.layer2.3.bn3.gamma, %para236_moments.layer2.3.bn3.beta, %para237_moments.layer3.0.conv1.weight, %para238_moments.layer3.0.bn1.gamma, %para239_moments.layer3.0.bn1.beta, %para240_moments.layer3.0.conv2.weight, %para241_moments.layer3.0.bn2.gamma, %para242_moments.layer3.0.bn2.beta, %para243_moments.layer3.0.conv3.weight, %para244_moments.layer3.0.bn3.gamma, %para245_moments.layer3.0.bn3.beta, %para246_moments.layer3.0.down_sample_layer.0.weight, %para247_moments.layer3.0.down_sample_layer.1.gamma, %para248_moments.layer3.0.down_sample_layer.1.beta, %para249_moments.layer3.1.conv1.weight, %para250_moments.layer3.1.bn1.gamma, %para251_moments.layer3.1.bn1.beta, %para252_moments.layer3.1.conv2.weight, %para253_moments.layer3.1.bn2.gamma, %para254_moments.layer3.1.bn2.beta, %para255_moments.layer3.1.conv3.weight, %para256_moments.layer3.1.bn3.gamma, %para257_moments.layer3.1.bn3.beta, %para258_moments.layer3.2.conv1.weight, %para259_moments.layer3.2.bn1.gamma, %para260_moments.layer3.2.bn1.beta, %para261_moments.layer3.2.conv2.weight, %para262_moments.layer3.2.bn2.gamma, %para263_moments.layer3.2.bn2.beta, %para264_moments.layer3.2.conv3.weight, %para265_moments.layer3.2.bn3.gamma, %para266_moments.layer3.2.bn3.beta, %para267_moments.layer3.3.conv1.weight, %para268_moments.layer3.3.bn1.gamma, %para269_moments.layer3.3.bn1.beta, %para270_moments.layer3.3.conv2.weight, %para271_moments.layer3.3.bn2.gamma, %para272_moments.layer3.3.bn2.beta, %para273_moments.layer3.3.conv3.weight, %para274_moments.layer3.3.bn3.gamma, %para275_moments.layer3.3.bn3.beta, %para276_moments.layer3.4.conv1.weight, %para277_moments.layer3.4.bn1.gamma, %para278_moments.layer3.4.bn1.beta, %para279_moments.layer3.4.conv2.weight, %para280_moments.layer3.4.bn2.gamma, %para281_moments.layer3.4.bn2.beta, %para282_moments.layer3.4.conv3.weight, %para283_moments.layer3.4.bn3.gamma, %para284_moments.layer3.4.bn3.beta, %para285_moments.layer3.5.conv1.weight, %para286_moments.layer3.5.bn1.gamma, %para287_moments.layer3.5.bn1.beta, %para288_moments.layer3.5.conv2.weight, %para289_moments.layer3.5.bn2.gamma, %para290_moments.layer3.5.bn2.beta, %para291_moments.layer3.5.conv3.weight, %para292_moments.layer3.5.bn3.gamma, %para293_moments.layer3.5.bn3.beta, %para294_moments.layer4.0.conv1.weight, %para295_moments.layer4.0.bn1.gamma, %para296_moments.layer4.0.bn1.beta, %para297_moments.layer4.0.conv2.weight, %para298_moments.layer4.0.bn2.gamma, %para299_moments.layer4.0.bn2.beta, %para300_moments.layer4.0.conv3.weight, %para301_moments.layer4.0.bn3.gamma, %para302_moments.layer4.0.bn3.beta, %para303_moments.layer4.0.down_sample_layer.0.weight, %para304_moments.layer4.0.down_sample_layer.1.gamma, %para305_moments.layer4.0.down_sample_layer.1.beta, %para306_moments.layer4.1.conv1.weight, %para307_moments.layer4.1.bn1.gamma, %para308_moments.layer4.1.bn1.beta, %para309_moments.layer4.1.conv2.weight, %para310_moments.layer4.1.bn2.gamma, %para311_moments.layer4.1.bn2.beta, %para312_moments.layer4.1.conv3.weight, %para313_moments.layer4.1.bn3.gamma, %para314_moments.layer4.1.bn3.beta, %para315_moments.layer4.2.conv1.weight, %para316_moments.layer4.2.bn1.gamma, %para317_moments.layer4.2.bn1.beta, %para318_moments.layer4.2.conv2.weight, %para319_moments.layer4.2.bn2.gamma, %para320_moments.layer4.2.bn2.beta, %para321_moments.layer4.2.conv3.weight, %para322_moments.layer4.2.bn3.gamma, %para323_moments.layer4.2.bn3.beta, %para324_moments.end_point.weight, %para325_moments.end_point.bias, %para326_momentum, %para327_bn1.moving_mean, %para328_bn1.moving_variance, %para329_learning_rate, %para330_layer4.0.bn3.moving_mean, %para331_layer4.0.bn3.moving_variance, %para332_layer4.1.bn3.moving_mean, %para333_layer4.1.bn3.moving_variance, %para334_layer4.2.bn3.moving_mean, %para335_layer4.2.bn3.moving_variance, %para336_layer3.0.bn3.moving_mean, %para337_layer3.0.bn3.moving_variance, %para338_layer3.1.bn3.moving_mean, %para339_layer3.1.bn3.moving_variance, %para340_layer3.2.bn3.moving_mean, %para341_layer3.2.bn3.moving_variance, %para342_layer3.3.bn3.moving_mean, %para343_layer3.3.bn3.moving_variance, %para344_layer3.4.bn3.moving_mean, %para345_layer3.4.bn3.moving_variance, %para346_layer3.5.bn3.moving_mean, %para347_layer3.5.bn3.moving_variance, %para348_layer2.0.bn3.moving_mean, %para349_layer2.0.bn3.moving_variance, %para350_layer2.1.bn3.moving_mean, %para351_layer2.1.bn3.moving_variance, %para352_layer2.2.bn3.moving_mean, %para353_layer2.2.bn3.moving_variance, %para354_layer2.3.bn3.moving_mean, %para355_layer2.3.bn3.moving_variance, %para356_layer4.0.bn2.moving_mean, %para357_layer4.0.bn2.moving_variance, %para358_layer4.1.bn2.moving_mean, %para359_layer4.1.bn2.moving_variance, %para360_layer4.2.bn2.moving_mean, %para361_layer4.2.bn2.moving_variance, %para362_layer1.0.bn3.moving_mean, %para363_layer1.0.bn3.moving_variance, %para364_layer1.1.bn3.moving_mean, %para365_layer1.1.bn3.moving_variance, %para366_layer1.2.bn3.moving_mean, %para367_layer1.2.bn3.moving_variance, %para368_layer3.0.bn2.moving_mean, %para369_layer3.0.bn2.moving_variance, %para370_layer3.1.bn2.moving_mean, %para371_layer3.1.bn2.moving_variance, %para372_layer3.2.bn2.moving_mean, %para373_layer3.2.bn2.moving_variance, %para374_layer3.3.bn2.moving_mean, %para375_layer3.3.bn2.moving_variance, %para376_layer3.4.bn2.moving_mean, %para377_layer3.4.bn2.moving_variance, %para378_layer3.5.bn2.moving_mean, %para379_layer3.5.bn2.moving_variance, %para380_layer2.0.bn2.moving_mean, %para381_layer2.0.bn2.moving_variance, %para382_layer2.1.bn2.moving_mean, %para383_layer2.1.bn2.moving_variance, %para384_layer2.2.bn2.moving_mean, %para385_layer2.2.bn2.moving_variance, %para386_layer2.3.bn2.moving_mean, %para387_layer2.3.bn2.moving_variance, %para388_layer4.0.bn1.moving_mean, %para389_layer4.0.bn1.moving_variance, %para390_layer4.1.bn1.moving_mean, %para391_layer4.1.bn1.moving_variance, %para392_layer4.2.bn1.moving_mean, %para393_layer4.2.bn1.moving_variance, %para394_layer1.0.bn2.moving_mean, %para395_layer1.0.bn2.moving_variance, %para396_layer1.1.bn2.moving_mean, %para397_layer1.1.bn2.moving_variance, %para398_layer1.2.bn2.moving_mean, %para399_layer1.2.bn2.moving_variance, %para400_layer3.0.bn1.moving_mean, %para401_layer3.0.bn1.moving_variance, %para402_layer3.1.bn1.moving_mean, %para403_layer3.1.bn1.moving_variance, %para404_layer3.2.bn1.moving_mean, %para405_layer3.2.bn1.moving_variance, %para406_layer3.3.bn1.moving_mean, %para407_layer3.3.bn1.moving_variance, %para408_layer3.4.bn1.moving_mean, %para409_layer3.4.bn1.moving_variance, %para410_layer3.5.bn1.moving_mean, %para411_layer3.5.bn1.moving_variance, %para412_layer2.0.bn1.moving_mean, %para413_layer2.0.bn1.moving_variance, %para414_layer2.1.bn1.moving_mean, %para415_layer2.1.bn1.moving_variance, %para416_layer2.2.bn1.moving_mean, %para417_layer2.2.bn1.moving_variance, %para418_layer2.3.bn1.moving_mean, %para419_layer2.3.bn1.moving_variance, %para420_layer1.0.bn1.moving_mean, %para421_layer1.0.bn1.moving_variance, %para422_layer1.1.bn1.moving_mean, %para423_layer1.1.bn1.moving_variance, %para424_layer1.2.bn1.moving_mean, %para425_layer1.2.bn1.moving_variance, %para426_layer4.0.down_sample_layer.1.moving_mean, %para427_layer4.0.down_sample_layer.1.moving_variance, %para428_layer3.0.down_sample_layer.1.moving_mean, %para429_layer3.0.down_sample_layer.1.moving_variance, %para430_layer2.0.down_sample_layer.1.moving_mean, %para431_layer2.0.down_sample_layer.1.moving_variance, %para432_layer1.0.down_sample_layer.1.moving_mean, %para433_layer1.0.down_sample_layer.1.moving_variance) {

#------------------------> 0
  %1(CNode_15) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2:CNode_15{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2:CNode_16{[0]: ValueNode<Primitive> Return, [1]: CNode_15}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3 : 0000029BC72B2DE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2]() {

#------------------------> 1
  %1(CNode_17) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3:CNode_17{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_3:CNode_18{[0]: ValueNode<Primitive> Return, [1]: CNode_17}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4 : 0000029BC72B72F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2]() {

#------------------------> 2
  %1(CNode_19) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:424/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:424/        if self.return_grad:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.20, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10, [2]: CNode_21}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_22{[0]: ValueNode<Primitive> getattr, [1]: loss, [2]: ValueNode<StringImm> dtype}
#   3: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_23{[0]: ValueNode<Primitive> getattr, [1]: loss, [2]: ValueNode<StringImm> shape}
#   4: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:sens{[0]: ValueNode<FuncGraph> fill_24, [1]: CNode_22, [2]: CNode_23, [3]: ValueNode<Int64Imm> 1024}
#   5: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_25{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: sens}
#   6: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10, [2]: CNode_21, [3]: CNode_25}
#   7: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_26}
#   8: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.27, [1]: grads, [2]: CNode_21, [3]: CNode_25}
#   9: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_28, [1]: grads}
#  10: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_29{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_30, [1]: grads}
#  11: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_29}
#  12: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_19{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5}
#  13: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4:CNode_31{[0]: ValueNode<Primitive> Return, [1]: CNode_19}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5 : 0000029BC77A1050
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {

#------------------------> 3
  %1(CNode_32) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_6()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:424/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:424/        if self.return_grad:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5:CNode_32{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_6}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_5:CNode_33{[0]: ValueNode<Primitive> Return, [1]: CNode_32}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_6 : 0000029BC77A6AA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_6 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4]() {
  %1(CNode_21) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[UInt8], (127076)>, <Tensor[Int32], ()>) -> (<Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 4
  %2(loss) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10, %1)
      : (<Func, NoShape>, <Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/
  %3(CNode_22) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):getattr(%2, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:420/        sens = F.fill(loss.dtype, loss.shape, self.sens)/
  %4(CNode_23) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):getattr(%2, "shape")
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:420/        sens = F.fill(loss.dtype, loss.shape, self.sens)/
  %5(sens) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):call @fill_24(%3, %4, I64(1024))
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:420/        sens = F.fill(loss.dtype, loss.shape, self.sens)/
  %6(CNode_25) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):S_Prim_MakeTuple(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:421/        grads = self.grad(self.network, self.weights)(*inputs, sens)/
  %7(grads) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10, %1, %6)
      : (<null>, <Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:421/        grads = self.grad(self.network, self.weights)(*inputs, sens)/
  %8(CNode_26) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_layer1.0.conv1.weight, %para7_layer1.0.bn1.gamma, %para8_layer1.0.bn1.beta, %para9_layer1.0.conv2.weight, %para10_layer1.0.bn2.gamma, %para11_layer1.0.bn2.beta, %para12_layer1.0.conv3.weight, %para13_layer1.0.bn3.gamma, %para14_layer1.0.bn3.beta, %para15_layer1.0.down_sample_layer.0.weight, %para16_layer1.0.down_sample_layer.1.gamma, %para17_layer1.0.down_sample_layer.1.beta, %para18_layer1.1.conv1.weight, %para19_layer1.1.bn1.gamma, %para20_layer1.1.bn1.beta, %para21_layer1.1.conv2.weight, %para22_layer1.1.bn2.gamma, %para23_layer1.1.bn2.beta, %para24_layer1.1.conv3.weight, %para25_layer1.1.bn3.gamma, %para26_layer1.1.bn3.beta, %para27_layer1.2.conv1.weight, %para28_layer1.2.bn1.gamma, %para29_layer1.2.bn1.beta, %para30_layer1.2.conv2.weight, %para31_layer1.2.bn2.gamma, %para32_layer1.2.bn2.beta, %para33_layer1.2.conv3.weight, %para34_layer1.2.bn3.gamma, %para35_layer1.2.bn3.beta, %para36_layer2.0.conv1.weight, %para37_layer2.0.bn1.gamma, %para38_layer2.0.bn1.beta, %para39_layer2.0.conv2.weight, %para40_layer2.0.bn2.gamma, %para41_layer2.0.bn2.beta, %para42_layer2.0.conv3.weight, %para43_layer2.0.bn3.gamma, %para44_layer2.0.bn3.beta, %para45_layer2.0.down_sample_layer.0.weight, %para46_layer2.0.down_sample_layer.1.gamma, %para47_layer2.0.down_sample_layer.1.beta, %para48_layer2.1.conv1.weight, %para49_layer2.1.bn1.gamma, %para50_layer2.1.bn1.beta, %para51_layer2.1.conv2.weight, %para52_layer2.1.bn2.gamma, %para53_layer2.1.bn2.beta, %para54_layer2.1.conv3.weight, %para55_layer2.1.bn3.gamma, %para56_layer2.1.bn3.beta, %para57_layer2.2.conv1.weight, %para58_layer2.2.bn1.gamma, %para59_layer2.2.bn1.beta, %para60_layer2.2.conv2.weight, %para61_layer2.2.bn2.gamma, %para62_layer2.2.bn2.beta, %para63_layer2.2.conv3.weight, %para64_layer2.2.bn3.gamma, %para65_layer2.2.bn3.beta, %para66_layer2.3.conv1.weight, %para67_layer2.3.bn1.gamma, %para68_layer2.3.bn1.beta, %para69_layer2.3.conv2.weight, %para70_layer2.3.bn2.gamma, %para71_layer2.3.bn2.beta, %para72_layer2.3.conv3.weight, %para73_layer2.3.bn3.gamma, %para74_layer2.3.bn3.beta, %para75_layer3.0.conv1.weight, %para76_layer3.0.bn1.gamma, %para77_layer3.0.bn1.beta, %para78_layer3.0.conv2.weight, %para79_layer3.0.bn2.gamma, %para80_layer3.0.bn2.beta, %para81_layer3.0.conv3.weight, %para82_layer3.0.bn3.gamma, %para83_layer3.0.bn3.beta, %para84_layer3.0.down_sample_layer.0.weight, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para87_layer3.1.conv1.weight, %para88_layer3.1.bn1.gamma, %para89_layer3.1.bn1.beta, %para90_layer3.1.conv2.weight, %para91_layer3.1.bn2.gamma, %para92_layer3.1.bn2.beta, %para93_layer3.1.conv3.weight, %para94_layer3.1.bn3.gamma, %para95_layer3.1.bn3.beta, %para96_layer3.2.conv1.weight, %para97_layer3.2.bn1.gamma, %para98_layer3.2.bn1.beta, %para99_layer3.2.conv2.weight, %para100_layer3.2.bn2.gamma, %para101_layer3.2.bn2.beta, %para102_layer3.2.conv3.weight, %para103_layer3.2.bn3.gamma, %para104_layer3.2.bn3.beta, %para105_layer3.3.conv1.weight, %para106_layer3.3.bn1.gamma, %para107_layer3.3.bn1.beta, %para108_layer3.3.conv2.weight, %para109_layer3.3.bn2.gamma, %para110_layer3.3.bn2.beta, %para111_layer3.3.conv3.weight, %para112_layer3.3.bn3.gamma, %para113_layer3.3.bn3.beta, %para114_layer3.4.conv1.weight, %para115_layer3.4.bn1.gamma, %para116_layer3.4.bn1.beta, %para117_layer3.4.conv2.weight, %para118_layer3.4.bn2.gamma, %para119_layer3.4.bn2.beta, %para120_layer3.4.conv3.weight, %para121_layer3.4.bn3.gamma, %para122_layer3.4.bn3.beta, %para123_layer3.5.conv1.weight, %para124_layer3.5.bn1.gamma, %para125_layer3.5.bn1.beta, %para126_layer3.5.conv2.weight, %para127_layer3.5.bn2.gamma, %para128_layer3.5.bn2.beta, %para129_layer3.5.conv3.weight, %para130_layer3.5.bn3.gamma, %para131_layer3.5.bn3.beta, %para132_layer4.0.conv1.weight, %para133_layer4.0.bn1.gamma, %para134_layer4.0.bn1.beta, %para135_layer4.0.conv2.weight, %para136_layer4.0.bn2.gamma, %para137_layer4.0.bn2.beta, %para138_layer4.0.conv3.weight, %para139_layer4.0.bn3.gamma, %para140_layer4.0.bn3.beta, %para141_layer4.0.down_sample_layer.0.weight, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para144_layer4.1.conv1.weight, %para145_layer4.1.bn1.gamma, %para146_layer4.1.bn1.beta, %para147_layer4.1.conv2.weight, %para148_layer4.1.bn2.gamma, %para149_layer4.1.bn2.beta, %para150_layer4.1.conv3.weight, %para151_layer4.1.bn3.gamma, %para152_layer4.1.bn3.beta, %para153_layer4.2.conv1.weight, %para154_layer4.2.bn1.gamma, %para155_layer4.2.bn1.beta, %para156_layer4.2.conv2.weight, %para157_layer4.2.bn2.gamma, %para158_layer4.2.bn2.beta, %para159_layer4.2.conv3.weight, %para160_layer4.2.bn3.gamma, %para161_layer4.2.bn3.beta, %para162_end_point.weight, %para163_end_point.bias)
      : (<Ref[Tensor[Float32]], (64, 3, 7, 7)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 1, 1)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (256, 64, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 64, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (64, 256, 1, 1)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (256, 64, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (64, 256, 1, 1)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (256, 64, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (128, 256, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (512, 128, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512, 256, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (128, 512, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (512, 128, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (128, 512, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (512, 128, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (128, 512, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (512, 128, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (256, 512, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024, 512, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (256, 1024, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (256, 1024, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (256, 1024, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (256, 1024, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (256, 1024, 1, 1)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256, 256, 3, 3)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (1024, 256, 1, 1)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (512, 1024, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512, 512, 3, 3)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (2048, 512, 1, 1)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (2048, 1024, 1, 1)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (512, 2048, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512, 512, 3, 3)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (2048, 512, 1, 1)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (512, 2048, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512, 512, 3, 3)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (2048, 512, 1, 1)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (2048)>, <Ref[Tensor[Float32]], (4, 2048)>, <Ref[Tensor[Float32]], (4)>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:421/        grads = self.grad(self.network, self.weights)(*inputs, sens)/
  %9(grads) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):S_Prim_grad(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:421/        grads = self.grad(self.network, self.weights)(*inputs, sens)/
  %10(grads) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):UnpackCall_unpack_call(%9, %1, %6)
      : (<null>, <Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:421/        grads = self.grad(self.network, self.weights)(*inputs, sens)/
  %11(grads) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):call @mindspore_nn_layer_basic_Identity_construct_28(%10)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:422/        grads = self.grad_reducer(grads)/
  %12(CNode_29) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):call @mindspore_nn_optim_momentum_Momentum_construct_30(%11)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:423/        loss = F.depend(loss, self.optimizer(grads))/
  %13(loss) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_4):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:423/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%13)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:429/        return loss/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_6:CNode_34{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_7 : 0000029BC779F5C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/
subgraph @UnpackCall_7(%para434_, %para435_) {
  %1(loss) = TupleGetItem(%para435_9, I64(0))
      : (<Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>, <Int64, NoShape>) -> (<Tensor[UInt8], (127076)>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para435_9, I64(1))
      : (<Tuple[Tensor[UInt8],Tensor[Int32]], TupleShape((127076), ())>, <Int64, NoShape>) -> (<Tensor[Int32], ()>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/

#------------------------> 5
  %3(loss) = %para434_8(%1, %2)
      : (<Tensor[UInt8], (127076)>, <Tensor[Int32], ()>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:419/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_7:loss{[0]: param_8, [1]: loss, [2]: loss}
#   2: @UnpackCall_7:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10 : 0000029BC72BC7F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para436_data, %para437_label) {

#------------------------> 6
  %1(out) = call @__main___ResNet_construct_11(%para436_data)
      : (<Tensor[UInt8], (127076)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_36) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35(%1, %para437_label)
      : (<null>, <Tensor[Int32], ()>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10:out{[0]: ValueNode<FuncGraph> __main___ResNet_construct_11, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10:CNode_36{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_10:CNode_37{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
training : 1
subgraph instance: __main___ResNet_construct_11 : 0000029BC72BB800
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:206/
subgraph @__main___ResNet_construct_11 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para438_x) {

#------------------------> 7
  %1(x) = call @mindspore_nn_layer_conv_Conv2d_construct_12(%para438_x)
      : (<Tensor[UInt8], (127076)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:207/
  %2(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_38(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:208/
  %3(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:209/
  %4(c1) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_39(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:210/
  %5(c2) = call @mindspore_nn_layer_container_SequentialCell_construct_40(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:211/
  %6(c3) = call @mindspore_nn_layer_container_SequentialCell_construct_41(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:212/
  %7(c4) = call @mindspore_nn_layer_container_SequentialCell_construct_42(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:213/
  %8(c5) = call @mindspore_nn_layer_container_SequentialCell_construct_43(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:214/
  %9(CNode_44) = S_Prim_MakeTuple(I64(2), I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:215/
  %10(out) = S_Prim_ReduceMean[keep_dims: Bool(1), input_names: ["input_x", "axis"], output_names: ["y"]](%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:215/
  %11(out) = call @mindspore_nn_layer_basic_Flatten_construct_45(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:216/
  %12(out) = call @mindspore_nn_layer_basic_Dense_construct_46(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:217/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:218/
}
# Order:
#   1: @__main___ResNet_construct_11:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_12, [1]: param_x}
#   2: @__main___ResNet_construct_11:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_38, [1]: x}
#   3: @__main___ResNet_construct_11:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#   4: @__main___ResNet_construct_11:c1{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_39, [1]: x}
#   5: @__main___ResNet_construct_11:c2{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_40, [1]: c1}
#   6: @__main___ResNet_construct_11:c3{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_41, [1]: c2}
#   7: @__main___ResNet_construct_11:c4{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_42, [1]: c3}
#   8: @__main___ResNet_construct_11:c5{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_43, [1]: c4}
#   9: @__main___ResNet_construct_11:CNode_44{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 2, [2]: ValueNode<Int64Imm> 3}
#  10: @__main___ResNet_construct_11:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: c5, [2]: CNode_44}
#  11: @__main___ResNet_construct_11:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_45, [1]: out}
#  12: @__main___ResNet_construct_11:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_46, [1]: out}
#  13: @__main___ResNet_construct_11:CNode_47{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_12 : 0000029BC72C2790
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_12 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para439_x) {

#------------------------> 8
  %1(CNode_48) = call @mindspore_nn_layer_conv_Conv2d_construct_13()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_12:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_12:CNode_48{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_13}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_12:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_13 : 0000029BC72BE7D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_13 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_12]() {

#------------------------> 9
  %1(CNode_50) = call @mindspore_nn_layer_conv_Conv2d_construct_14()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_13:CNode_50{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_14}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_13:CNode_51{[0]: ValueNode<Primitive> Return, [1]: CNode_50}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_14 : 0000029BC72C5CB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_14 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_12]() {

#------------------------> 10
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_12):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(7), I64(7)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para439_x, %para3_conv1.weight)
      : (<Tensor[UInt8], (127076)>, <Ref[Tensor[Float32]], (64, 3, 7, 7)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_14:CNode_52{[0]: ValueNode<Primitive> Return, [1]: output}


# ===============================================================================================
# The total of function graphs in evaluation stack: 11/12 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_30 : 0000029BC72B7840
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_30 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para440_gradients) {
  %1(CNode_53) = S_Prim_AssignAdd[input_names: ["ref", "value"], output_names: ["ref"], side_effect_mem: Bool(1)](%para164_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:223/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_54) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:214/    @jit/
  %3(CNode_56) = call @mindspore_nn_optim_momentum_Momentum_construct_55()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
  %4(CNode_57) = Depend[side_effect_propagate: I64(1)](%3, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_30:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_58, [1]: param_gradients}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_30:gradients{[0]: ValueNode<FuncGraph> decay_weight_59, [1]: gradients}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_30:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_60, [1]: gradients}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_30:gradients{[0]: ValueNode<FuncGraph> scale_grad_61, [1]: gradients}
#   5: @mindspore_nn_optim_momentum_Momentum_construct_30:lr{[0]: ValueNode<FuncGraph> get_lr_62}
#   6: @mindspore_nn_optim_momentum_Momentum_construct_30:CNode_53{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   7: @mindspore_nn_optim_momentum_Momentum_construct_30:CNode_56{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_55}
#   8: @mindspore_nn_optim_momentum_Momentum_construct_30:CNode_63{[0]: ValueNode<Primitive> Return, [1]: CNode_57}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_28 : 0000029BC72B4870
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_28(%para441_x) {
  Return(%para441_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_28:CNode_64{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
subgraph instance: fill_24 : 0000029BC72B82E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:712/def fill(type, shape, value):  # pylint: disable=redefined-outer-name/
subgraph @fill_24(%para442_type, %para443_shape, %para444_value) {
  %1(CNode_66) = call @_get_cache_prim_65(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:746/    return _get_cache_prim(P.FillV2)()(shape, value)/
  %2(CNode_67) = %1()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:746/    return _get_cache_prim(P.FillV2)()(shape, value)/
  %3(value) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para444_value, %para442_type)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:745/    value = cast_(value, type)/
  %4(CNode_68) = %2(%para443_shape, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:746/    return _get_cache_prim(P.FillV2)()(shape, value)/
  Return(%4)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:746/    return _get_cache_prim(P.FillV2)()(shape, value)/
}
# Order:
#   1: @fill_24:value{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_value, [2]: param_type}
#   2: @fill_24:CNode_66{[0]: ValueNode<FuncGraph> _get_cache_prim_65, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.FillV2'}
#   3: @fill_24:CNode_67{[0]: CNode_66}
#   4: @fill_24:CNode_68{[0]: CNode_67, [1]: param_shape, [2]: value}
#   5: @fill_24:CNode_69{[0]: ValueNode<Primitive> Return, [1]: CNode_68}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_61 : 0000029BC72BD7E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_61(%para445_gradients) {
  %1(CNode_71) = call @scale_grad_70()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_61:CNode_71{[0]: ValueNode<FuncGraph> scale_grad_70}
#   2: @scale_grad_61:CNode_72{[0]: ValueNode<Primitive> Return, [1]: CNode_71}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_60 : 0000029BC72BFD10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_60(%para446_gradients) {
  %1(CNode_74) = call @gradients_centralization_73()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_60:CNode_74{[0]: ValueNode<FuncGraph> gradients_centralization_73}
#   2: @gradients_centralization_60:CNode_75{[0]: ValueNode<Primitive> Return, [1]: CNode_74}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_59 : 0000029BC72C0D00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_59 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para447_gradients) {
  %1(CNode_77) = call @decay_weight_76()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_59:CNode_77{[0]: ValueNode<FuncGraph> decay_weight_76}
#   2: @decay_weight_59:CNode_78{[0]: ValueNode<Primitive> Return, [1]: CNode_77}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_58 : 0000029BC72B92D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_58(%para448_gradients) {
  %1(CNode_80) = call @flatten_gradients_79()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_58:CNode_80{[0]: ValueNode<FuncGraph> flatten_gradients_79}
#   2: @flatten_gradients_58:CNode_81{[0]: ValueNode<Primitive> Return, [1]: CNode_80}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_62 : 0000029BC72BDD30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_62 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2]() {
  %1(CNode_83) = call @get_lr_82()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_62:CNode_83{[0]: ValueNode<FuncGraph> get_lr_82}
#   2: @get_lr_62:CNode_84{[0]: ValueNode<Primitive> Return, [1]: CNode_83}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_55 : 0000029BC72BBD50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_55 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_30]() {
  %1(CNode_86) = call @mindspore_nn_optim_momentum_Momentum_construct_85()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_55:CNode_86{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_85}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_55:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_86}


subgraph attr:
subgraph instance: _get_cache_prim_65 : 0000029BC72BA2C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_65(%para449_cls) {
  %1(CNode_89) = call @_get_cache_prim_88()
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_65:CNode_89{[0]: ValueNode<FuncGraph> _get_cache_prim_88}
#   2: @_get_cache_prim_65:CNode_90{[0]: ValueNode<Primitive> Return, [1]: CNode_89}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35 : 0000029BC77A5AB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35(%para450_logits, %para451_labels) {
  %1(CNode_91) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para450_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_92) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para451_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_93) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_94) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_96) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_97) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35:CNode_91{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35:CNode_92{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35:CNode_96{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35:CNode_98{[0]: ValueNode<Primitive> Return, [1]: CNode_97}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_70 : 0000029BC72C3230
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_70 parent: [subgraph @scale_grad_61]() {
  %1(CNode_100) = call @scale_grad_99()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:486/            gradients = self.map_(F.partial(_grad_scale, self.reciprocal_scale), gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:486/            gradients = self.map_(F.partial(_grad_scale, self.reciprocal_scale), gradients)/
}
# Order:
#   1: @scale_grad_70:CNode_101{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_grad_scale, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.000976562)}
#   2: @scale_grad_70:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_101, [2]: param_gradients}
#   3: @scale_grad_70:CNode_100{[0]: ValueNode<FuncGraph> scale_grad_99}
#   4: @scale_grad_70:CNode_102{[0]: ValueNode<Primitive> Return, [1]: CNode_100}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_73 : 0000029BC72C2CE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_73 parent: [subgraph @gradients_centralization_60]() {
  %1(CNode_104) = call @gradients_centralization_103()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_73:CNode_104{[0]: ValueNode<FuncGraph> gradients_centralization_103}
#   2: @gradients_centralization_73:CNode_105{[0]: ValueNode<Primitive> Return, [1]: CNode_104}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_76 : 0000029BC72BA810
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_76 parent: [subgraph @decay_weight_59]() {
  %1(CNode_107) = call @decay_weight_106()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:445/            if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:445/            if self.is_group:/
}
# Order:
#   1: @decay_weight_76:weight_decay{[0]: ValueNode<FuncGraph> get_weight_decay_108}
#   2: @decay_weight_76:CNode_107{[0]: ValueNode<FuncGraph> decay_weight_106}
#   3: @decay_weight_76:CNode_109{[0]: ValueNode<Primitive> Return, [1]: CNode_107}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_79 : 0000029BC72B7D90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_79 parent: [subgraph @flatten_gradients_58]() {
  %1(CNode_111) = call @flatten_gradients_110()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_79:CNode_111{[0]: ValueNode<FuncGraph> flatten_gradients_110}
#   2: @flatten_gradients_79:CNode_112{[0]: ValueNode<Primitive> Return, [1]: CNode_111}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_82 : 0000029BC72C1250
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_82 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2]() {
  %1(CNode_114) = call @get_lr_113()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:749/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:749/            if self.is_group_lr:/
}
# Order:
#   1: @get_lr_82:CNode_114{[0]: ValueNode<FuncGraph> get_lr_113}
#   2: @get_lr_82:CNode_115{[0]: ValueNode<Primitive> Return, [1]: CNode_114}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_85 : 0000029BC72BC2A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_85 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_30]() {
  %1(CNode_117) = call @mindspore_nn_optim_momentum_Momentum_construct_116()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_85:CNode_118{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_momentum_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_ApplyMomentum, [3]: param_momentum, [4]: lr}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_85:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_hyper_map, [1]: CNode_118, [2]: gradients, [3]: CNode_119, [4]: CNode_120, [5]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false), [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false)}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_85:CNode_117{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_116}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_85:CNode_121{[0]: ValueNode<Primitive> Return, [1]: CNode_117}


subgraph attr:
subgraph instance: _get_cache_prim_88 : 0000029BC72B3880
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_88 parent: [subgraph @_get_cache_prim_65]() {
  Return(@_new_prim_for_graph_122)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @_get_cache_prim_88:CNode_123{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_122}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95 : 0000029BC77A4570
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35]() {
  %1(CNode_124) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_125) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_126) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_129) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95:CNode_124{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95:CNode_125{[0]: ValueNode<Primitive> Cond, [1]: CNode_124, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95:CNode_126{[0]: ValueNode<Primitive> Switch, [1]: CNode_125, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95:CNode_129{[0]: CNode_126}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_95:CNode_130{[0]: ValueNode<Primitive> Return, [1]: CNode_129}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_46 : 0000029BC7592A00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_46 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para452_x) {
  %1(x_shape) = S_Prim_Shape(%para452_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_131) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_132) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_133) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_134) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_135) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_136) = Switch(%6, @mindspore_nn_layer_basic_Dense_construct_137, @mindspore_nn_layer_basic_Dense_construct_138)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_139) = %7()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_141) = call @mindspore_nn_layer_basic_Dense_construct_140(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:217/
  %10(CNode_142) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:217/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_46:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dense_construct_46:CNode_131{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @mindspore_nn_layer_basic_Dense_construct_46:CNode_133{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @mindspore_nn_layer_basic_Dense_construct_46:CNode_134{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_133, [2]: ValueNode<Int64Imm> 2}
#   5: @mindspore_nn_layer_basic_Dense_construct_46:CNode_135{[0]: ValueNode<Primitive> Cond, [1]: CNode_134, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dense_construct_46:CNode_136{[0]: ValueNode<Primitive> Switch, [1]: CNode_135, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_137, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_138}
#   7: @mindspore_nn_layer_basic_Dense_construct_46:CNode_139{[0]: CNode_136}
#   8: @mindspore_nn_layer_basic_Dense_construct_46:CNode_141{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_140, [1]: CNode_139}
#   9: @mindspore_nn_layer_basic_Dense_construct_46:CNode_142{[0]: ValueNode<Primitive> Depend, [1]: CNode_141, [2]: CNode_132}
#  10: @mindspore_nn_layer_basic_Dense_construct_46:CNode_143{[0]: ValueNode<Primitive> Return, [1]: CNode_142}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_45 : 0000029BC75BE1F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_45(%para453_x) {
  %1(x_rank) = call @rank_144(%para453_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_145) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_146) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_147) = Switch(%3, @mindspore_nn_layer_basic_Flatten_construct_148, @mindspore_nn_layer_basic_Flatten_construct_149)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_151) = call @check_axis_valid_150(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_152) = call @check_axis_valid_150(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_153) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %9(CNode_154) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %10(CNode_155) = S_Prim_MakeTuple(%para453_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_156) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_157) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_158) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_159) = UnpackCall_unpack_call(@flatten_160, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_161) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_45:x_rank{[0]: ValueNode<FuncGraph> rank_144, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_145{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_146{[0]: ValueNode<Primitive> Cond, [1]: CNode_145, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_147{[0]: ValueNode<Primitive> Switch, [1]: CNode_146, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_148, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_149}
#   5: @mindspore_nn_layer_basic_Flatten_construct_45:ndim{[0]: CNode_147}
#   6: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_151{[0]: ValueNode<FuncGraph> check_axis_valid_150, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_152{[0]: ValueNode<FuncGraph> check_axis_valid_150, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_155{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_156{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_157{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_158{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_156, [2]: CNode_157}
#  12: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_159{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.162, [1]: ValueNode<FuncGraph> flatten_160, [2]: CNode_155, [3]: CNode_158}
#  13: @mindspore_nn_layer_basic_Flatten_construct_45:CNode_163{[0]: ValueNode<Primitive> Return, [1]: CNode_161}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_43 : 0000029BC75A58D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_43 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para454_input_data) {
  %1(CNode_165) = call @mindspore_nn_layer_container_SequentialCell_construct_164(I64(0), %para454_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_43:CNode_166{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_167}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_43:CNode_165{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_164, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_43:CNode_168{[0]: ValueNode<Primitive> Return, [1]: CNode_165}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_42 : 0000029BC750AC90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_42 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para455_input_data) {
  %1(CNode_170) = call @mindspore_nn_layer_container_SequentialCell_construct_169(I64(0), %para455_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_42:CNode_171{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_172}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_42:CNode_170{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_169, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_42:CNode_173{[0]: ValueNode<Primitive> Return, [1]: CNode_170}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_41 : 0000029BC74E1480
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_41 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para456_input_data) {
  %1(CNode_175) = call @mindspore_nn_layer_container_SequentialCell_construct_174(I64(0), %para456_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_41:CNode_176{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_177}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_41:CNode_175{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_174, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_41:CNode_178{[0]: ValueNode<Primitive> Return, [1]: CNode_175}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_40 : 0000029BC7290560
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_40 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para457_input_data) {
  %1(CNode_180) = call @mindspore_nn_layer_container_SequentialCell_construct_179(I64(0), %para457_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_40:CNode_181{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_182}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_40:CNode_180{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_179, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_40:CNode_183{[0]: ValueNode<Primitive> Return, [1]: CNode_180}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_39 : 0000029BC72C5760
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_39(%para458_x) {
  %1(CNode_184) = getattr(%para458_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_185) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_186) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_187) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_188, @mindspore_nn_layer_pooling_MaxPool2d_construct_189)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_190) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_184{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_185{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_184, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_186{[0]: ValueNode<Primitive> Cond, [1]: CNode_185, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_187{[0]: ValueNode<Primitive> Switch, [1]: CNode_186, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_188, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_189}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_190{[0]: CNode_187}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_39:CNode_191{[0]: ValueNode<Primitive> Return, [1]: CNode_190}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_38 : 0000029BC72BE280
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_38 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para459_x) {
  %1(CNode_193) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para459_x, %para4_bn1.gamma, %para5_bn1.beta, %para327_bn1.moving_mean, %para328_bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_38:CNode_193{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_bn1.gamma, [3]: param_bn1.beta, [4]: param_bn1.moving_mean, [5]: param_bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_38:CNode_194{[0]: ValueNode<Primitive> Return, [1]: CNode_193}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_99 : 0000029BC72C1CF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_99 parent: [subgraph @scale_grad_70]() {
  %1(CNode_101) = $(scale_grad_70):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_grad_scale, Tensor(shape=[], dtype=Float32, value=0.000976562))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:486/            gradients = self.map_(F.partial(_grad_scale, self.reciprocal_scale), gradients)/
  %2(gradients) = $(scale_grad_70):S_Prim_map(%1, %para445_gradients)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:486/            gradients = self.map_(F.partial(_grad_scale, self.reciprocal_scale), gradients)/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:488/        return gradients/
}
# Order:
#   1: @scale_grad_99:CNode_195{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_103 : 0000029BC72BCD40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_103 parent: [subgraph @gradients_centralization_60]() {
  Return(%para446_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:469/        return gradients/
}
# Order:
#   1: @gradients_centralization_103:CNode_196{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_weight_decay_108 : 0000029BC72BAD60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:720/    def get_weight_decay(self):/
subgraph @get_weight_decay_108() {
  %1(CNode_198) = call @get_weight_decay_197()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:729/        if self.dynamic_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:729/        if self.dynamic_weight_decay:/
}
# Order:
#   1: @get_weight_decay_108:CNode_198{[0]: ValueNode<FuncGraph> get_weight_decay_197}
#   2: @get_weight_decay_108:CNode_199{[0]: ValueNode<Primitive> Return, [1]: CNode_198}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_106 : 0000029BC72C0260
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_106 parent: [subgraph @decay_weight_76]() {
  %1(CNode_201) = call @decay_weight_200()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:448/                gradients = self.map_(F.partial(_apply_decay, weight_decay), self.decay_flags, params, gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:448/                gradients = self.map_(F.partial(_apply_decay, weight_decay), self.decay_flags, params, gradients)/
}
# Order:
#   1: @decay_weight_106:CNode_202{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_apply_decay, [2]: weight_decay}
#   2: @decay_weight_106:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_202, [2]: ValueNode<ValueTuple> (true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true), [3]: CNode_203, [4]: param_gradients}
#   3: @decay_weight_106:CNode_201{[0]: ValueNode<FuncGraph> decay_weight_200}
#   4: @decay_weight_106:CNode_204{[0]: ValueNode<Primitive> Return, [1]: CNode_201}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_110 : 0000029BC72B2890
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_110 parent: [subgraph @flatten_gradients_58]() {
  Return(%para448_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:427/        return gradients/
}
# Order:
#   1: @flatten_gradients_110:CNode_205{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_113 : 0000029BC72BB2B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_113 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2]() {
  %1(CNode_207) = call @get_lr_206()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:755/                lr = self.learning_rate(self.global_step).reshape(())/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:755/                lr = self.learning_rate(self.global_step).reshape(())/
}
# Order:
#   1: @get_lr_113:CNode_208{[0]: ValueNode<FuncGraph> mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209, [1]: param_global_step}
#   2: @get_lr_113:CNode_210{[0]: ValueNode<Primitive> getattr, [1]: CNode_208, [2]: ValueNode<StringImm> reshape}
#   3: @get_lr_113:lr{[0]: CNode_210, [1]: ValueNode<ValueTuple> ()}
#   4: @get_lr_113:CNode_207{[0]: ValueNode<FuncGraph> get_lr_206}
#   5: @get_lr_113:CNode_211{[0]: ValueNode<Primitive> Return, [1]: CNode_207}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_116 : 0000029BC72BD290
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_116 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_85]() {
  %1(CNode_213) = call @mindspore_nn_optim_momentum_Momentum_construct_212()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_116:CNode_213{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_212}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_116:CNode_214{[0]: ValueNode<Primitive> Return, [1]: CNode_213}


subgraph attr:
subgraph instance: _new_prim_for_graph_122 : 0000029BC72B4DC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_122 parent: [subgraph @_get_cache_prim_65](%para460_args, %para461_kwargs) {
  %1(CNode_215) = UnpackCall_unpack_call(%para449_cls, %para460_args, %para461_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_122:CNode_215{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.216, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_122:CNode_217{[0]: ValueNode<Primitive> Return, [1]: CNode_215}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127 : 0000029BC77A6550
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para450_logits, %para451_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_127:CNode_218{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128 : 0000029BC779E080
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35]() {
  %1(CNode_220) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128:CNode_220{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_128:CNode_221{[0]: ValueNode<Primitive> Return, [1]: CNode_220}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_140 : 0000029BC75934A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_140 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_46](%para462_) {
  %1(CNode_223) = call @mindspore_nn_layer_basic_Dense_construct_222()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_140:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_end_point.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_140:CNode_223{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_222}
#   3: @mindspore_nn_layer_basic_Dense_construct_140:CNode_224{[0]: ValueNode<Primitive> Return, [1]: CNode_223}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_137 : 0000029BC7591A10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_137 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_46]() {
  %1(CNode_225) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(mindspore_nn_layer_basic_Dense_construct_46):S_Prim_Shape(%para452_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_226) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_227) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_228) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para452_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_137:CNode_225{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dense_construct_137:CNode_226{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_layer_basic_Dense_construct_137:CNode_227{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_226}
#   4: @mindspore_nn_layer_basic_Dense_construct_137:CNode_228{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_225, [2]: CNode_227}
#   5: @mindspore_nn_layer_basic_Dense_construct_137:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_228}
#   6: @mindspore_nn_layer_basic_Dense_construct_137:CNode_229{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_138 : 0000029BC7594F30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_138 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_46]() {
  Return(%para452_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_138:CNode_230{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_150 : 0000029BC75BE740
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_150(%para463_axis, %para464_ndim) {
  %1(CNode_231) = S_Prim_negative(%para464_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_232) = S_Prim_less(%para463_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_233) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_234) = Switch(%3, @check_axis_valid_235, @check_axis_valid_236)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_237) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_238) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_239) = Switch(%6, @check_axis_valid_240, @check_axis_valid_241)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_242) = %7()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_150:CNode_231{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_150:CNode_232{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_231}
#   3: @check_axis_valid_150:CNode_233{[0]: ValueNode<Primitive> Cond, [1]: CNode_232, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_150:CNode_234{[0]: ValueNode<Primitive> Switch, [1]: CNode_233, [2]: ValueNode<FuncGraph> check_axis_valid_235, [3]: ValueNode<FuncGraph> check_axis_valid_236}
#   5: @check_axis_valid_150:CNode_237{[0]: CNode_234}
#   6: @check_axis_valid_150:CNode_238{[0]: ValueNode<Primitive> Cond, [1]: CNode_237, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_150:CNode_239{[0]: ValueNode<Primitive> Switch, [1]: CNode_238, [2]: ValueNode<FuncGraph> check_axis_valid_240, [3]: ValueNode<FuncGraph> check_axis_valid_241}
#   8: @check_axis_valid_150:CNode_242{[0]: CNode_239}
#   9: @check_axis_valid_150:CNode_243{[0]: ValueNode<Primitive> Return, [1]: CNode_242}


subgraph attr:
subgraph instance: rank_144 : 0000029BC758DFA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1541/def rank(input_x):/
subgraph @rank_144(%para465_input_x) {
  %1(CNode_244) = S_Prim_Rank(%para465_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_144:CNode_244{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_144:CNode_245{[0]: ValueNode<Primitive> Return, [1]: CNode_244}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_148 : 0000029BC7589A90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_148 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_45]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_45):call @rank_144(%para453_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_148:CNode_246{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_149 : 0000029BC758E4F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_149() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_149:CNode_247{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_160 : 0000029BC75BFC80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_160(%para466_input, %para467_order, %para468_start_dim, %para469_end_dim) {
  %1(CNode_248) = S_Prim_isinstance(%para466_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_249) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_250) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_251) = Switch(%3, @flatten_252, @flatten_253)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_254) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_160:CNode_248{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_160:CNode_249{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_248}
#   3: @flatten_160:CNode_250{[0]: ValueNode<Primitive> Cond, [1]: CNode_249, [2]: ValueNode<BoolImm> false}
#   4: @flatten_160:CNode_251{[0]: ValueNode<Primitive> Switch, [1]: CNode_250, [2]: ValueNode<FuncGraph> flatten_252, [3]: ValueNode<FuncGraph> flatten_253}
#   5: @flatten_160:CNode_254{[0]: CNode_251}
#   6: @flatten_160:CNode_255{[0]: ValueNode<Primitive> Return, [1]: CNode_254}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_256 : 0000029BC75B6270
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_256 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para470_x) {
  %1(CNode_258) = call @__main___ResidualBlock_construct_257()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_259, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_260, [1]: out}
#   3: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_261, [1]: out}
#   4: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_262, [1]: out}
#   5: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_263, [1]: out}
#   6: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_261, [1]: out}
#   7: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_264, [1]: out}
#   8: @__main___ResidualBlock_construct_256:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_265, [1]: out}
#   9: @__main___ResidualBlock_construct_256:CNode_258{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_257}
#  10: @__main___ResidualBlock_construct_256:CNode_266{[0]: ValueNode<Primitive> Return, [1]: CNode_258}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_267 : 0000029BC75AC860
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_267 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para471_x) {
  %1(CNode_269) = call @__main___ResidualBlock_construct_268()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_270, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_271, [1]: out}
#   3: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_272, [1]: out}
#   4: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_273, [1]: out}
#   5: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_274, [1]: out}
#   6: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_272, [1]: out}
#   7: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_275, [1]: out}
#   8: @__main___ResidualBlock_construct_267:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_276, [1]: out}
#   9: @__main___ResidualBlock_construct_267:CNode_269{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_268}
#  10: @__main___ResidualBlock_construct_267:CNode_277{[0]: ValueNode<Primitive> Return, [1]: CNode_269}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_278 : 0000029BC75A5E20
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_278 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para472_x) {
  %1(CNode_280) = call @__main___ResidualBlock_construct_279()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_281, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_282, [1]: out}
#   3: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_283, [1]: out}
#   4: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_284, [1]: out}
#   5: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_285, [1]: out}
#   6: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_283, [1]: out}
#   7: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_286, [1]: out}
#   8: @__main___ResidualBlock_construct_278:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_287, [1]: out}
#   9: @__main___ResidualBlock_construct_278:CNode_280{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_279}
#  10: @__main___ResidualBlock_construct_278:CNode_288{[0]: ValueNode<Primitive> Return, [1]: CNode_280}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_164 : 0000029BC75BC760
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_164 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_43](%para473_, %para474_) {
  %1(CNode_167) = $(mindspore_nn_layer_container_SequentialCell_construct_43):MakeTuple(@__main___ResidualBlock_construct_256, @__main___ResidualBlock_construct_267, @__main___ResidualBlock_construct_278)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_166) = $(mindspore_nn_layer_container_SequentialCell_construct_43):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_289) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para473_@CNode_290, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_291) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_292, @mindspore_nn_layer_container_SequentialCell_construct_293)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_294) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_164:CNode_289{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_290, [2]: CNode_166}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_164:CNode_291{[0]: ValueNode<Primitive> Switch, [1]: CNode_289, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_292, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_293}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_164:CNode_294{[0]: CNode_291}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_164:CNode_296{[0]: ValueNode<Primitive> Return, [1]: CNode_294}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_297 : 0000029BC7599EE0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_297 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para475_x) {
  %1(CNode_299) = call @__main___ResidualBlock_construct_298()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_300, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_301, [1]: out}
#   3: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_302, [1]: out}
#   4: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_303, [1]: out}
#   5: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_304, [1]: out}
#   6: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_302, [1]: out}
#   7: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_305, [1]: out}
#   8: @__main___ResidualBlock_construct_297:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_306, [1]: out}
#   9: @__main___ResidualBlock_construct_297:CNode_299{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_298}
#  10: @__main___ResidualBlock_construct_297:CNode_307{[0]: ValueNode<Primitive> Return, [1]: CNode_299}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_308 : 0000029BC74E09E0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_308 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para476_x) {
  %1(CNode_310) = call @__main___ResidualBlock_construct_309()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_311, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_312, [1]: out}
#   3: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_313, [1]: out}
#   4: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_314, [1]: out}
#   5: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_315, [1]: out}
#   6: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_313, [1]: out}
#   7: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_316, [1]: out}
#   8: @__main___ResidualBlock_construct_308:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_317, [1]: out}
#   9: @__main___ResidualBlock_construct_308:CNode_310{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_309}
#  10: @__main___ResidualBlock_construct_308:CNode_318{[0]: ValueNode<Primitive> Return, [1]: CNode_310}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_319 : 0000029BC74DBA30
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_319 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para477_x) {
  %1(CNode_321) = call @__main___ResidualBlock_construct_320()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_322, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_323, [1]: out}
#   3: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_324, [1]: out}
#   4: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_325, [1]: out}
#   5: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_326, [1]: out}
#   6: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_324, [1]: out}
#   7: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_327, [1]: out}
#   8: @__main___ResidualBlock_construct_319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_328, [1]: out}
#   9: @__main___ResidualBlock_construct_319:CNode_321{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_320}
#  10: @__main___ResidualBlock_construct_319:CNode_329{[0]: ValueNode<Primitive> Return, [1]: CNode_321}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_330 : 0000029BC74CE5B0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_330 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para478_x) {
  %1(CNode_332) = call @__main___ResidualBlock_construct_331()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_333, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_334, [1]: out}
#   3: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_335, [1]: out}
#   4: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_336, [1]: out}
#   5: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_337, [1]: out}
#   6: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_335, [1]: out}
#   7: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_338, [1]: out}
#   8: @__main___ResidualBlock_construct_330:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_339, [1]: out}
#   9: @__main___ResidualBlock_construct_330:CNode_332{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_331}
#  10: @__main___ResidualBlock_construct_330:CNode_340{[0]: ValueNode<Primitive> Return, [1]: CNode_332}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_341 : 0000029BC74CC080
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_341 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para479_x) {
  %1(CNode_343) = call @__main___ResidualBlock_construct_342()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_344, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_345, [1]: out}
#   3: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_346, [1]: out}
#   4: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_347, [1]: out}
#   5: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_348, [1]: out}
#   6: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_346, [1]: out}
#   7: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_349, [1]: out}
#   8: @__main___ResidualBlock_construct_341:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_350, [1]: out}
#   9: @__main___ResidualBlock_construct_341:CNode_343{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_342}
#  10: @__main___ResidualBlock_construct_341:CNode_351{[0]: ValueNode<Primitive> Return, [1]: CNode_343}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_352 : 0000029BC7508CB0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_352 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para480_x) {
  %1(CNode_354) = call @__main___ResidualBlock_construct_353()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_355, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_356, [1]: out}
#   3: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_357, [1]: out}
#   4: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_358, [1]: out}
#   5: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_359, [1]: out}
#   6: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_357, [1]: out}
#   7: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_360, [1]: out}
#   8: @__main___ResidualBlock_construct_352:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_361, [1]: out}
#   9: @__main___ResidualBlock_construct_352:CNode_354{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_353}
#  10: @__main___ResidualBlock_construct_352:CNode_362{[0]: ValueNode<Primitive> Return, [1]: CNode_354}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_169 : 0000029BC75A03D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_169 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_42](%para481_, %para482_) {
  %1(CNode_172) = $(mindspore_nn_layer_container_SequentialCell_construct_42):MakeTuple(@__main___ResidualBlock_construct_297, @__main___ResidualBlock_construct_308, @__main___ResidualBlock_construct_319, @__main___ResidualBlock_construct_330, @__main___ResidualBlock_construct_341, @__main___ResidualBlock_construct_352)
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_171) = $(mindspore_nn_layer_container_SequentialCell_construct_42):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_363) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para481_@CNode_364, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_365) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_366, @mindspore_nn_layer_container_SequentialCell_construct_367)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_368) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_169:CNode_363{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_364, [2]: CNode_171}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_169:CNode_365{[0]: ValueNode<Primitive> Switch, [1]: CNode_363, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_366, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_367}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_169:CNode_368{[0]: CNode_365}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_169:CNode_369{[0]: ValueNode<Primitive> Return, [1]: CNode_368}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_370 : 0000029BC74F8860
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_370 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para483_x) {
  %1(CNode_372) = call @__main___ResidualBlock_construct_371()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_373, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_374, [1]: out}
#   3: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_375, [1]: out}
#   4: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_376, [1]: out}
#   5: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_377, [1]: out}
#   6: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_375, [1]: out}
#   7: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_378, [1]: out}
#   8: @__main___ResidualBlock_construct_370:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_379, [1]: out}
#   9: @__main___ResidualBlock_construct_370:CNode_372{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_371}
#  10: @__main___ResidualBlock_construct_370:CNode_380{[0]: ValueNode<Primitive> Return, [1]: CNode_372}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_381 : 0000029BC74FB2E0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_381 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para484_x) {
  %1(CNode_383) = call @__main___ResidualBlock_construct_382()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_384, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_385, [1]: out}
#   3: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_386, [1]: out}
#   4: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_387, [1]: out}
#   5: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_388, [1]: out}
#   6: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_386, [1]: out}
#   7: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_389, [1]: out}
#   8: @__main___ResidualBlock_construct_381:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_390, [1]: out}
#   9: @__main___ResidualBlock_construct_381:CNode_383{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_382}
#  10: @__main___ResidualBlock_construct_381:CNode_391{[0]: ValueNode<Primitive> Return, [1]: CNode_383}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_392 : 0000029BC74F38B0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_392 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para485_x) {
  %1(CNode_394) = call @__main___ResidualBlock_construct_393()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_395, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_396, [1]: out}
#   3: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_397, [1]: out}
#   4: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_398, [1]: out}
#   5: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_399, [1]: out}
#   6: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_397, [1]: out}
#   7: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_400, [1]: out}
#   8: @__main___ResidualBlock_construct_392:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_401, [1]: out}
#   9: @__main___ResidualBlock_construct_392:CNode_394{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_393}
#  10: @__main___ResidualBlock_construct_392:CNode_402{[0]: ValueNode<Primitive> Return, [1]: CNode_394}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_403 : 0000029BC74E9400
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_403 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para486_x) {
  %1(CNode_405) = call @__main___ResidualBlock_construct_404()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_406, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_407, [1]: out}
#   3: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_408, [1]: out}
#   4: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_409, [1]: out}
#   5: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_410, [1]: out}
#   6: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_408, [1]: out}
#   7: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_411, [1]: out}
#   8: @__main___ResidualBlock_construct_403:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_412, [1]: out}
#   9: @__main___ResidualBlock_construct_403:CNode_405{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_404}
#  10: @__main___ResidualBlock_construct_403:CNode_413{[0]: ValueNode<Primitive> Return, [1]: CNode_405}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_174 : 0000029BC75007E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_174 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_41](%para487_, %para488_) {
  %1(CNode_177) = $(mindspore_nn_layer_container_SequentialCell_construct_41):MakeTuple(@__main___ResidualBlock_construct_370, @__main___ResidualBlock_construct_381, @__main___ResidualBlock_construct_392, @__main___ResidualBlock_construct_403)
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_176) = $(mindspore_nn_layer_container_SequentialCell_construct_41):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_414) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para487_@CNode_415, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_416) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_417, @mindspore_nn_layer_container_SequentialCell_construct_418)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_419) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_174:CNode_414{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_415, [2]: CNode_176}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_174:CNode_416{[0]: ValueNode<Primitive> Switch, [1]: CNode_414, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_417, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_418}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_174:CNode_419{[0]: CNode_416}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_174:CNode_420{[0]: ValueNode<Primitive> Return, [1]: CNode_419}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_421 : 0000029BC7292FE0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_421 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para489_x) {
  %1(CNode_423) = call @__main___ResidualBlock_construct_422()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_424, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_425, [1]: out}
#   3: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_426, [1]: out}
#   4: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_427, [1]: out}
#   5: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_428, [1]: out}
#   6: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_426, [1]: out}
#   7: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_429, [1]: out}
#   8: @__main___ResidualBlock_construct_421:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_430, [1]: out}
#   9: @__main___ResidualBlock_construct_421:CNode_423{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_422}
#  10: @__main___ResidualBlock_construct_421:CNode_431{[0]: ValueNode<Primitive> Return, [1]: CNode_423}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_432 : 0000029BC728B060
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_432 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para490_x) {
  %1(CNode_434) = call @__main___ResidualBlock_construct_433()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_435, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_436, [1]: out}
#   3: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_437, [1]: out}
#   4: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_438, [1]: out}
#   5: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_439, [1]: out}
#   6: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_437, [1]: out}
#   7: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_440, [1]: out}
#   8: @__main___ResidualBlock_construct_432:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_441, [1]: out}
#   9: @__main___ResidualBlock_construct_432:CNode_434{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_433}
#  10: @__main___ResidualBlock_construct_432:CNode_442{[0]: ValueNode<Primitive> Return, [1]: CNode_434}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_443 : 0000029BC728DAE0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_443 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para491_x) {
  %1(CNode_445) = call @__main___ResidualBlock_construct_444()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_446, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_447, [1]: out}
#   3: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_448, [1]: out}
#   4: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_449, [1]: out}
#   5: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_450, [1]: out}
#   6: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_448, [1]: out}
#   7: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_451, [1]: out}
#   8: @__main___ResidualBlock_construct_443:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_452, [1]: out}
#   9: @__main___ResidualBlock_construct_443:CNode_445{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_444}
#  10: @__main___ResidualBlock_construct_443:CNode_453{[0]: ValueNode<Primitive> Return, [1]: CNode_445}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_179 : 0000029BC74E8EB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_179 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_40](%para492_, %para493_) {
  %1(CNode_182) = $(mindspore_nn_layer_container_SequentialCell_construct_40):MakeTuple(@__main___ResidualBlock_construct_421, @__main___ResidualBlock_construct_432, @__main___ResidualBlock_construct_443)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_181) = $(mindspore_nn_layer_container_SequentialCell_construct_40):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_454) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para492_@CNode_455, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_456) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_457, @mindspore_nn_layer_container_SequentialCell_construct_458)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_459) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_179:CNode_454{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_455, [2]: CNode_181}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_179:CNode_456{[0]: ValueNode<Primitive> Switch, [1]: CNode_454, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_457, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_458}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_179:CNode_459{[0]: CNode_456}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_179:CNode_460{[0]: ValueNode<Primitive> Return, [1]: CNode_459}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_188 : 0000029BC728D590
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_188 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_39]() {
  %1(CNode_461) = getattr(%para458_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_463) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_462(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:210/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_188:CNode_461{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_188:x{[0]: CNode_461, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_188:CNode_464{[0]: ValueNode<Primitive> Return, [1]: CNode_463}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_188:CNode_463{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_462, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_189 : 0000029BC72C6750
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_189 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_39]() {
  %1(CNode_465) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_462(%para458_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:210/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_189:CNode_466{[0]: ValueNode<Primitive> Return, [1]: CNode_465}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_189:CNode_465{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_462, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192 : 0000029BC72C3780
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para494_x, %para495_, %para496_, %para497_, %para498_) {
  %1(CNode_467) = S_Prim_Shape(%para494_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_468) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_469) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_470) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_471) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_472) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_475) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_476) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_467{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_468{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_467, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_470{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_471{[0]: ValueNode<Primitive> Cond, [1]: CNode_470, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_472{[0]: ValueNode<Primitive> Switch, [1]: CNode_471, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_475{[0]: CNode_472}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192:CNode_194{[0]: ValueNode<Primitive> Return, [1]: CNode_476}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_weight_decay_197 : 0000029BC72C07B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:720/    def get_weight_decay(self):/
subgraph @get_weight_decay_197() {
  %1(CNode_478) = call @get_weight_decay_477()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:729/        if self.dynamic_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:729/        if self.dynamic_weight_decay:/
}
# Order:
#   1: @get_weight_decay_197:CNode_478{[0]: ValueNode<FuncGraph> get_weight_decay_477}
#   2: @get_weight_decay_197:CNode_479{[0]: ValueNode<Primitive> Return, [1]: CNode_478}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_200 : 0000029BC72B9D70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_200 parent: [subgraph @decay_weight_106]() {
  %1(CNode_481) = call @decay_weight_480()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:443/            params = self._parameters/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:443/            params = self._parameters/
}
# Order:
#   1: @decay_weight_200:CNode_481{[0]: ValueNode<FuncGraph> decay_weight_480}
#   2: @decay_weight_200:CNode_482{[0]: ValueNode<Primitive> Return, [1]: CNode_481}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209 : 0000029BC72BF270
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:1018/    def construct(self, global_step):/
subgraph @mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para499_global_step) {
  %1(CNode_483) = S_Prim_Gather[batch_dims: I64(0), output_names: ["output"], input_names: ["params", "indices", "axis"]](%para329_learning_rate, %para499_global_step, I64(0))
      : (<Ref[Tensor[Float32]], (885), ref_key=:learning_rate>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum/learning_rate-_IteratorLearningRate)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:1019/        return self.gather(self.learning_rate, global_step, 0)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum/learning_rate-_IteratorLearningRate)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:1019/        return self.gather(self.learning_rate, global_step, 0)/
}
# Order:
#   1: @mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209:CNode_483{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Gather, [1]: param_learning_rate, [2]: param_global_step, [3]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209:CNode_484{[0]: ValueNode<Primitive> Return, [1]: CNode_483}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_206 : 0000029BC72C17A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_206 parent: [subgraph @get_lr_113]() {
  %1(CNode_486) = call @get_lr_485()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:749/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:749/            if self.is_group_lr:/
}
# Order:
#   1: @get_lr_206:CNode_486{[0]: ValueNode<FuncGraph> get_lr_485}
#   2: @get_lr_206:CNode_487{[0]: ValueNode<Primitive> Return, [1]: CNode_486}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_212 : 0000029BC72C2240
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_212 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_85]() {
  %1(lr) = $(mindspore_nn_optim_momentum_Momentum_construct_30):call @get_lr_62()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:222/        lr = self.get_lr()/
  %2(CNode_118) = $(mindspore_nn_optim_momentum_Momentum_construct_85):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_momentum_opt, S_Prim_ApplyMomentum[use_nesterov: Bool(0), use_locking: Bool(0), gradient_scale: F32(1), input_names: ["variable", "accumulation", "learning_rate", "gradient", "momentum"], output_names: ["output"], side_effect_mem: Bool(1)], %para326_momentum, %1)
      : (<null>, <null>, <Ref[Tensor[Float32]], (), ref_key=:momentum>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %3(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_30):call @flatten_gradients_58(%para440_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:218/        gradients = self.flatten_gradients(gradients)/
  %4(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_30):call @decay_weight_59(%3)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:219/        gradients = self.decay_weight(gradients)/
  %5(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_30):call @gradients_centralization_60(%4)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:220/        gradients = self.gradients_centralization(gradients)/
  %6(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_30):call @scale_grad_61(%5)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:221/        gradients = self.scale_grad(gradients)/
  %7(CNode_119) = $(mindspore_nn_optim_momentum_Momentum_construct_30):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_layer1.0.conv1.weight, %para7_layer1.0.bn1.gamma, %para8_layer1.0.bn1.beta, %para9_layer1.0.conv2.weight, %para10_layer1.0.bn2.gamma, %para11_layer1.0.bn2.beta, %para12_layer1.0.conv3.weight, %para13_layer1.0.bn3.gamma, %para14_layer1.0.bn3.beta, %para15_layer1.0.down_sample_layer.0.weight, %para16_layer1.0.down_sample_layer.1.gamma, %para17_layer1.0.down_sample_layer.1.beta, %para18_layer1.1.conv1.weight, %para19_layer1.1.bn1.gamma, %para20_layer1.1.bn1.beta, %para21_layer1.1.conv2.weight, %para22_layer1.1.bn2.gamma, %para23_layer1.1.bn2.beta, %para24_layer1.1.conv3.weight, %para25_layer1.1.bn3.gamma, %para26_layer1.1.bn3.beta, %para27_layer1.2.conv1.weight, %para28_layer1.2.bn1.gamma, %para29_layer1.2.bn1.beta, %para30_layer1.2.conv2.weight, %para31_layer1.2.bn2.gamma, %para32_layer1.2.bn2.beta, %para33_layer1.2.conv3.weight, %para34_layer1.2.bn3.gamma, %para35_layer1.2.bn3.beta, %para36_layer2.0.conv1.weight, %para37_layer2.0.bn1.gamma, %para38_layer2.0.bn1.beta, %para39_layer2.0.conv2.weight, %para40_layer2.0.bn2.gamma, %para41_layer2.0.bn2.beta, %para42_layer2.0.conv3.weight, %para43_layer2.0.bn3.gamma, %para44_layer2.0.bn3.beta, %para45_layer2.0.down_sample_layer.0.weight, %para46_layer2.0.down_sample_layer.1.gamma, %para47_layer2.0.down_sample_layer.1.beta, %para48_layer2.1.conv1.weight, %para49_layer2.1.bn1.gamma, %para50_layer2.1.bn1.beta, %para51_layer2.1.conv2.weight, %para52_layer2.1.bn2.gamma, %para53_layer2.1.bn2.beta, %para54_layer2.1.conv3.weight, %para55_layer2.1.bn3.gamma, %para56_layer2.1.bn3.beta, %para57_layer2.2.conv1.weight, %para58_layer2.2.bn1.gamma, %para59_layer2.2.bn1.beta, %para60_layer2.2.conv2.weight, %para61_layer2.2.bn2.gamma, %para62_layer2.2.bn2.beta, %para63_layer2.2.conv3.weight, %para64_layer2.2.bn3.gamma, %para65_layer2.2.bn3.beta, %para66_layer2.3.conv1.weight, %para67_layer2.3.bn1.gamma, %para68_layer2.3.bn1.beta, %para69_layer2.3.conv2.weight, %para70_layer2.3.bn2.gamma, %para71_layer2.3.bn2.beta, %para72_layer2.3.conv3.weight, %para73_layer2.3.bn3.gamma, %para74_layer2.3.bn3.beta, %para75_layer3.0.conv1.weight, %para76_layer3.0.bn1.gamma, %para77_layer3.0.bn1.beta, %para78_layer3.0.conv2.weight, %para79_layer3.0.bn2.gamma, %para80_layer3.0.bn2.beta, %para81_layer3.0.conv3.weight, %para82_layer3.0.bn3.gamma, %para83_layer3.0.bn3.beta, %para84_layer3.0.down_sample_layer.0.weight, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para87_layer3.1.conv1.weight, %para88_layer3.1.bn1.gamma, %para89_layer3.1.bn1.beta, %para90_layer3.1.conv2.weight, %para91_layer3.1.bn2.gamma, %para92_layer3.1.bn2.beta, %para93_layer3.1.conv3.weight, %para94_layer3.1.bn3.gamma, %para95_layer3.1.bn3.beta, %para96_layer3.2.conv1.weight, %para97_layer3.2.bn1.gamma, %para98_layer3.2.bn1.beta, %para99_layer3.2.conv2.weight, %para100_layer3.2.bn2.gamma, %para101_layer3.2.bn2.beta, %para102_layer3.2.conv3.weight, %para103_layer3.2.bn3.gamma, %para104_layer3.2.bn3.beta, %para105_layer3.3.conv1.weight, %para106_layer3.3.bn1.gamma, %para107_layer3.3.bn1.beta, %para108_layer3.3.conv2.weight, %para109_layer3.3.bn2.gamma, %para110_layer3.3.bn2.beta, %para111_layer3.3.conv3.weight, %para112_layer3.3.bn3.gamma, %para113_layer3.3.bn3.beta, %para114_layer3.4.conv1.weight, %para115_layer3.4.bn1.gamma, %para116_layer3.4.bn1.beta, %para117_layer3.4.conv2.weight, %para118_layer3.4.bn2.gamma, %para119_layer3.4.bn2.beta, %para120_layer3.4.conv3.weight, %para121_layer3.4.bn3.gamma, %para122_layer3.4.bn3.beta, %para123_layer3.5.conv1.weight, %para124_layer3.5.bn1.gamma, %para125_layer3.5.bn1.beta, %para126_layer3.5.conv2.weight, %para127_layer3.5.bn2.gamma, %para128_layer3.5.bn2.beta, %para129_layer3.5.conv3.weight, %para130_layer3.5.bn3.gamma, %para131_layer3.5.bn3.beta, %para132_layer4.0.conv1.weight, %para133_layer4.0.bn1.gamma, %para134_layer4.0.bn1.beta, %para135_layer4.0.conv2.weight, %para136_layer4.0.bn2.gamma, %para137_layer4.0.bn2.beta, %para138_layer4.0.conv3.weight, %para139_layer4.0.bn3.gamma, %para140_layer4.0.bn3.beta, %para141_layer4.0.down_sample_layer.0.weight, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para144_layer4.1.conv1.weight, %para145_layer4.1.bn1.gamma, %para146_layer4.1.bn1.beta, %para147_layer4.1.conv2.weight, %para148_layer4.1.bn2.gamma, %para149_layer4.1.bn2.beta, %para150_layer4.1.conv3.weight, %para151_layer4.1.bn3.gamma, %para152_layer4.1.bn3.beta, %para153_layer4.2.conv1.weight, %para154_layer4.2.bn1.gamma, %para155_layer4.2.bn1.beta, %para156_layer4.2.conv2.weight, %para157_layer4.2.bn2.gamma, %para158_layer4.2.bn2.beta, %para159_layer4.2.conv3.weight, %para160_layer4.2.bn3.gamma, %para161_layer4.2.bn3.beta, %para162_end_point.weight, %para163_end_point.bias)
      : (<Ref[Tensor[Float32]], (64, 3, 7, 7), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:layer1.0.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.0.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.1.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.1.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.1.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.2.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.2.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.2.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:layer2.0.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.0.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.0.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:layer2.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.1.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.1.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.1.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.2.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.2.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.2.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.3.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.3.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.3.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:layer3.0.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.0.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.0.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.beta>, <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:layer3.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.1.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.1.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.1.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.2.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.2.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.2.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.3.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.3.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.3.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.4.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.4.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.4.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.5.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.5.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.5.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.beta>, <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:layer4.0.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.0.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.0.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.beta>, <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:layer4.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.1.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.1.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.1.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.2.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.2.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.2.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.beta>, <Ref[Tensor[Float32]], (4, 2048), ref_key=:end_point.weight>, <Ref[Tensor[Float32]], (4), ref_key=:end_point.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:216/        params = self.params/
  %8(CNode_120) = $(mindspore_nn_optim_momentum_Momentum_construct_30):MakeTuple(%para165_moments.conv1.weight, %para166_moments.bn1.gamma, %para167_moments.bn1.beta, %para168_moments.layer1.0.conv1.weight, %para169_moments.layer1.0.bn1.gamma, %para170_moments.layer1.0.bn1.beta, %para171_moments.layer1.0.conv2.weight, %para172_moments.layer1.0.bn2.gamma, %para173_moments.layer1.0.bn2.beta, %para174_moments.layer1.0.conv3.weight, %para175_moments.layer1.0.bn3.gamma, %para176_moments.layer1.0.bn3.beta, %para177_moments.layer1.0.down_sample_layer.0.weight, %para178_moments.layer1.0.down_sample_layer.1.gamma, %para179_moments.layer1.0.down_sample_layer.1.beta, %para180_moments.layer1.1.conv1.weight, %para181_moments.layer1.1.bn1.gamma, %para182_moments.layer1.1.bn1.beta, %para183_moments.layer1.1.conv2.weight, %para184_moments.layer1.1.bn2.gamma, %para185_moments.layer1.1.bn2.beta, %para186_moments.layer1.1.conv3.weight, %para187_moments.layer1.1.bn3.gamma, %para188_moments.layer1.1.bn3.beta, %para189_moments.layer1.2.conv1.weight, %para190_moments.layer1.2.bn1.gamma, %para191_moments.layer1.2.bn1.beta, %para192_moments.layer1.2.conv2.weight, %para193_moments.layer1.2.bn2.gamma, %para194_moments.layer1.2.bn2.beta, %para195_moments.layer1.2.conv3.weight, %para196_moments.layer1.2.bn3.gamma, %para197_moments.layer1.2.bn3.beta, %para198_moments.layer2.0.conv1.weight, %para199_moments.layer2.0.bn1.gamma, %para200_moments.layer2.0.bn1.beta, %para201_moments.layer2.0.conv2.weight, %para202_moments.layer2.0.bn2.gamma, %para203_moments.layer2.0.bn2.beta, %para204_moments.layer2.0.conv3.weight, %para205_moments.layer2.0.bn3.gamma, %para206_moments.layer2.0.bn3.beta, %para207_moments.layer2.0.down_sample_layer.0.weight, %para208_moments.layer2.0.down_sample_layer.1.gamma, %para209_moments.layer2.0.down_sample_layer.1.beta, %para210_moments.layer2.1.conv1.weight, %para211_moments.layer2.1.bn1.gamma, %para212_moments.layer2.1.bn1.beta, %para213_moments.layer2.1.conv2.weight, %para214_moments.layer2.1.bn2.gamma, %para215_moments.layer2.1.bn2.beta, %para216_moments.layer2.1.conv3.weight, %para217_moments.layer2.1.bn3.gamma, %para218_moments.layer2.1.bn3.beta, %para219_moments.layer2.2.conv1.weight, %para220_moments.layer2.2.bn1.gamma, %para221_moments.layer2.2.bn1.beta, %para222_moments.layer2.2.conv2.weight, %para223_moments.layer2.2.bn2.gamma, %para224_moments.layer2.2.bn2.beta, %para225_moments.layer2.2.conv3.weight, %para226_moments.layer2.2.bn3.gamma, %para227_moments.layer2.2.bn3.beta, %para228_moments.layer2.3.conv1.weight, %para229_moments.layer2.3.bn1.gamma, %para230_moments.layer2.3.bn1.beta, %para231_moments.layer2.3.conv2.weight, %para232_moments.layer2.3.bn2.gamma, %para233_moments.layer2.3.bn2.beta, %para234_moments.layer2.3.conv3.weight, %para235_moments.layer2.3.bn3.gamma, %para236_moments.layer2.3.bn3.beta, %para237_moments.layer3.0.conv1.weight, %para238_moments.layer3.0.bn1.gamma, %para239_moments.layer3.0.bn1.beta, %para240_moments.layer3.0.conv2.weight, %para241_moments.layer3.0.bn2.gamma, %para242_moments.layer3.0.bn2.beta, %para243_moments.layer3.0.conv3.weight, %para244_moments.layer3.0.bn3.gamma, %para245_moments.layer3.0.bn3.beta, %para246_moments.layer3.0.down_sample_layer.0.weight, %para247_moments.layer3.0.down_sample_layer.1.gamma, %para248_moments.layer3.0.down_sample_layer.1.beta, %para249_moments.layer3.1.conv1.weight, %para250_moments.layer3.1.bn1.gamma, %para251_moments.layer3.1.bn1.beta, %para252_moments.layer3.1.conv2.weight, %para253_moments.layer3.1.bn2.gamma, %para254_moments.layer3.1.bn2.beta, %para255_moments.layer3.1.conv3.weight, %para256_moments.layer3.1.bn3.gamma, %para257_moments.layer3.1.bn3.beta, %para258_moments.layer3.2.conv1.weight, %para259_moments.layer3.2.bn1.gamma, %para260_moments.layer3.2.bn1.beta, %para261_moments.layer3.2.conv2.weight, %para262_moments.layer3.2.bn2.gamma, %para263_moments.layer3.2.bn2.beta, %para264_moments.layer3.2.conv3.weight, %para265_moments.layer3.2.bn3.gamma, %para266_moments.layer3.2.bn3.beta, %para267_moments.layer3.3.conv1.weight, %para268_moments.layer3.3.bn1.gamma, %para269_moments.layer3.3.bn1.beta, %para270_moments.layer3.3.conv2.weight, %para271_moments.layer3.3.bn2.gamma, %para272_moments.layer3.3.bn2.beta, %para273_moments.layer3.3.conv3.weight, %para274_moments.layer3.3.bn3.gamma, %para275_moments.layer3.3.bn3.beta, %para276_moments.layer3.4.conv1.weight, %para277_moments.layer3.4.bn1.gamma, %para278_moments.layer3.4.bn1.beta, %para279_moments.layer3.4.conv2.weight, %para280_moments.layer3.4.bn2.gamma, %para281_moments.layer3.4.bn2.beta, %para282_moments.layer3.4.conv3.weight, %para283_moments.layer3.4.bn3.gamma, %para284_moments.layer3.4.bn3.beta, %para285_moments.layer3.5.conv1.weight, %para286_moments.layer3.5.bn1.gamma, %para287_moments.layer3.5.bn1.beta, %para288_moments.layer3.5.conv2.weight, %para289_moments.layer3.5.bn2.gamma, %para290_moments.layer3.5.bn2.beta, %para291_moments.layer3.5.conv3.weight, %para292_moments.layer3.5.bn3.gamma, %para293_moments.layer3.5.bn3.beta, %para294_moments.layer4.0.conv1.weight, %para295_moments.layer4.0.bn1.gamma, %para296_moments.layer4.0.bn1.beta, %para297_moments.layer4.0.conv2.weight, %para298_moments.layer4.0.bn2.gamma, %para299_moments.layer4.0.bn2.beta, %para300_moments.layer4.0.conv3.weight, %para301_moments.layer4.0.bn3.gamma, %para302_moments.layer4.0.bn3.beta, %para303_moments.layer4.0.down_sample_layer.0.weight, %para304_moments.layer4.0.down_sample_layer.1.gamma, %para305_moments.layer4.0.down_sample_layer.1.beta, %para306_moments.layer4.1.conv1.weight, %para307_moments.layer4.1.bn1.gamma, %para308_moments.layer4.1.bn1.beta, %para309_moments.layer4.1.conv2.weight, %para310_moments.layer4.1.bn2.gamma, %para311_moments.layer4.1.bn2.beta, %para312_moments.layer4.1.conv3.weight, %para313_moments.layer4.1.bn3.gamma, %para314_moments.layer4.1.bn3.beta, %para315_moments.layer4.2.conv1.weight, %para316_moments.layer4.2.bn1.gamma, %para317_moments.layer4.2.bn1.beta, %para318_moments.layer4.2.conv2.weight, %para319_moments.layer4.2.bn2.gamma, %para320_moments.layer4.2.bn2.beta, %para321_moments.layer4.2.conv3.weight, %para322_moments.layer4.2.bn3.gamma, %para323_moments.layer4.2.bn3.beta, %para324_moments.end_point.weight, %para325_moments.end_point.bias)
      : (<Ref[Tensor[Float32]], (64, 3, 7, 7), ref_key=:moments.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:moments.layer1.0.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.0.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.0.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.0.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.bn3.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moments.layer1.1.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.1.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.1.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.1.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.1.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.1.bn3.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moments.layer1.2.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moments.layer1.2.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moments.layer1.2.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:moments.layer1.2.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.2.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer1.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:moments.layer2.0.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.0.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.0.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.0.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moments.layer2.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.1.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.1.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.1.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.1.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.1.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.1.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.2.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.2.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.2.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.2.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.2.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:moments.layer2.3.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moments.layer2.3.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moments.layer2.3.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:moments.layer2.3.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.3.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer2.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:moments.layer3.0.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.0.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.0.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.0.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.bn3.beta>, <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:moments.layer3.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.1.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.1.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.1.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.1.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.1.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.1.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.2.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.2.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.2.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.2.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.2.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.2.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.3.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.3.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.3.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.3.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.3.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.4.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.4.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.4.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.4.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.4.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.4.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:moments.layer3.5.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:moments.layer3.5.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:moments.layer3.5.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:moments.layer3.5.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.5.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:moments.layer3.5.bn3.beta>, <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:moments.layer4.0.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.0.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.0.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.0.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.bn3.beta>, <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:moments.layer4.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:moments.layer4.1.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.1.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.1.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.1.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.1.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.1.bn3.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:moments.layer4.2.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:moments.layer4.2.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moments.layer4.2.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:moments.layer4.2.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.2.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:moments.layer4.2.bn3.beta>, <Ref[Tensor[Float32]], (4, 2048), ref_key=:moments.end_point.weight>, <Ref[Tensor[Float32]], (4), ref_key=:moments.end_point.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:217/        moments = self.moments/
  %9(success) = $(mindspore_nn_optim_momentum_Momentum_construct_85):S_Prim_hyper_map(%2, %6, %7, %8, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%9)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:240/        return success/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_212:CNode_488{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219 : 0000029BC77A4AC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_35]() {
  %1(CNode_490) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:CNode_491{[0]: ValueNode<FuncGraph> shape_492, [1]: param_logits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:CNode_493{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:CNode_494{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_491, [2]: CNode_493}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_494, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:CNode_490{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219:CNode_495{[0]: ValueNode<Primitive> Return, [1]: CNode_490}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_222 : 0000029BC7591F60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_222 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_140]() {
  %1(CNode_497) = call @mindspore_nn_layer_basic_Dense_construct_496()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_222:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_end_point.bias}
#   2: @mindspore_nn_layer_basic_Dense_construct_222:CNode_497{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_496}
#   3: @mindspore_nn_layer_basic_Dense_construct_222:CNode_498{[0]: ValueNode<Primitive> Return, [1]: CNode_497}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_240 : 0000029BC75C01D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_240() {
  %1(CNode_499) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_240:CNode_499{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_240:CNode_500{[0]: ValueNode<Primitive> Return, [1]: CNode_499}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_241 : 0000029BC75BF730
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_241() {
  %1(CNode_502) = call @check_axis_valid_501()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_241:CNode_502{[0]: ValueNode<FuncGraph> check_axis_valid_501}
#   2: @check_axis_valid_241:CNode_503{[0]: ValueNode<Primitive> Return, [1]: CNode_502}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_235 : 0000029BC75BF1E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_235 parent: [subgraph @check_axis_valid_150]() {
  %1(CNode_231) = $(check_axis_valid_150):S_Prim_negative(%para464_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_232) = $(check_axis_valid_150):S_Prim_less(%para463_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_235:CNode_504{[0]: ValueNode<Primitive> Return, [1]: CNode_232}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_236 : 0000029BC75BEC90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_236 parent: [subgraph @check_axis_valid_150]() {
  %1(CNode_505) = S_Prim_greater_equal(%para463_axis, %para464_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_236:CNode_505{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_236:CNode_506{[0]: ValueNode<Primitive> Return, [1]: CNode_505}


subgraph attr:
subgraph instance: flatten_252 : 0000029BC75904D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_252() {
  %1(CNode_507) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_508) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @flatten_252:CNode_507{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @flatten_252:CNode_508{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_507, [3]: ValueNode<StringImm> None}
#   3: @flatten_252:CNode_509{[0]: ValueNode<Primitive> Return, [1]: CNode_508}


subgraph attr:
subgraph instance: flatten_253 : 0000029BC7584AE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_253 parent: [subgraph @flatten_160]() {
  %1(CNode_511) = call @flatten_510()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_253:CNode_511{[0]: ValueNode<FuncGraph> flatten_510}
#   2: @flatten_253:CNode_512{[0]: ValueNode<Primitive> Return, [1]: CNode_511}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_265 : 0000029BC75B37F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_265 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para500_x) {
  %1(CNode_514) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513(%para500_x, %para139_layer4.0.bn3.gamma, %para140_layer4.0.bn3.beta, %para330_layer4.0.bn3.moving_mean, %para331_layer4.0.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_265:CNode_514{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513, [1]: param_x, [2]: param_layer4.0.bn3.gamma, [3]: param_layer4.0.bn3.beta, [4]: param_layer4.0.bn3.moving_mean, [5]: param_layer4.0.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_265:CNode_515{[0]: ValueNode<Primitive> Return, [1]: CNode_514}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_264 : 0000029BC75B4290
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_264 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para501_x) {
  %1(CNode_517) = call @mindspore_nn_layer_conv_Conv2d_construct_516()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_264:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.0.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_264:CNode_517{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_516}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_264:CNode_518{[0]: ValueNode<Primitive> Return, [1]: CNode_517}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_261 : 0000029BC75B5D20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_261(%para502_x) {
  %1(CNode_519) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para502_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_261:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_261:CNode_520{[0]: ValueNode<Primitive> Return, [1]: CNode_519}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_263 : 0000029BC75B7D00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_263 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para503_x) {
  %1(CNode_522) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para503_x, %para136_layer4.0.bn2.gamma, %para137_layer4.0.bn2.beta, %para356_layer4.0.bn2.moving_mean, %para357_layer4.0.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_263:CNode_522{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.0.bn2.gamma, [3]: param_layer4.0.bn2.beta, [4]: param_layer4.0.bn2.moving_mean, [5]: param_layer4.0.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_263:CNode_523{[0]: ValueNode<Primitive> Return, [1]: CNode_522}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_262 : 0000029BC75B7260
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_262 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para504_x) {
  %1(CNode_525) = call @mindspore_nn_layer_conv_Conv2d_construct_524()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_262:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.0.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_262:CNode_525{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_524}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_262:CNode_526{[0]: ValueNode<Primitive> Return, [1]: CNode_525}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_260 : 0000029BC75B3D40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_260 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para505_x) {
  %1(CNode_527) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para505_x, %para133_layer4.0.bn1.gamma, %para134_layer4.0.bn1.beta, %para388_layer4.0.bn1.moving_mean, %para389_layer4.0.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_260:CNode_527{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.0.bn1.gamma, [3]: param_layer4.0.bn1.beta, [4]: param_layer4.0.bn1.moving_mean, [5]: param_layer4.0.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_260:CNode_528{[0]: ValueNode<Primitive> Return, [1]: CNode_527}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_259 : 0000029BC75B8CF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_259 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para506_x) {
  %1(CNode_530) = call @mindspore_nn_layer_conv_Conv2d_construct_529()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_259:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.0.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_259:CNode_530{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_529}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_259:CNode_531{[0]: ValueNode<Primitive> Return, [1]: CNode_530}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_257 : 0000029BC75B9240
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_257 parent: [subgraph @__main___ResidualBlock_construct_256]() {
  %1(CNode_533) = call @__main___ResidualBlock_construct_532()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
}
# Order:
#   1: @__main___ResidualBlock_construct_257:identity{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_534, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_257:CNode_533{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_532}
#   3: @__main___ResidualBlock_construct_257:CNode_535{[0]: ValueNode<Primitive> Return, [1]: CNode_533}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_276 : 0000029BC75AADD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_276 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para507_x) {
  %1(CNode_536) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513(%para507_x, %para151_layer4.1.bn3.gamma, %para152_layer4.1.bn3.beta, %para332_layer4.1.bn3.moving_mean, %para333_layer4.1.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_276:CNode_536{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513, [1]: param_x, [2]: param_layer4.1.bn3.gamma, [3]: param_layer4.1.bn3.beta, [4]: param_layer4.1.bn3.moving_mean, [5]: param_layer4.1.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_276:CNode_537{[0]: ValueNode<Primitive> Return, [1]: CNode_536}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_275 : 0000029BC75ADDA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_275 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para508_x) {
  %1(CNode_539) = call @mindspore_nn_layer_conv_Conv2d_construct_538()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_275:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.1.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_275:CNode_539{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_538}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_275:CNode_540{[0]: ValueNode<Primitive> Return, [1]: CNode_539}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_272 : 0000029BC75B0D70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_272(%para509_x) {
  %1(CNode_541) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para509_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_272:CNode_541{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_272:CNode_542{[0]: ValueNode<Primitive> Return, [1]: CNode_541}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_274 : 0000029BC75B0820
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_274 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para510_x) {
  %1(CNode_543) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para510_x, %para148_layer4.1.bn2.gamma, %para149_layer4.1.bn2.beta, %para358_layer4.1.bn2.moving_mean, %para359_layer4.1.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_274:CNode_543{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.1.bn2.gamma, [3]: param_layer4.1.bn2.beta, [4]: param_layer4.1.bn2.moving_mean, [5]: param_layer4.1.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_274:CNode_544{[0]: ValueNode<Primitive> Return, [1]: CNode_543}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_273 : 0000029BC75B12C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_273 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para511_x) {
  %1(CNode_546) = call @mindspore_nn_layer_conv_Conv2d_construct_545()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_273:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.1.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_273:CNode_546{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_545}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_273:CNode_547{[0]: ValueNode<Primitive> Return, [1]: CNode_546}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_271 : 0000029BC75AF830
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_271 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para512_x) {
  %1(CNode_548) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para512_x, %para145_layer4.1.bn1.gamma, %para146_layer4.1.bn1.beta, %para390_layer4.1.bn1.moving_mean, %para391_layer4.1.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_271:CNode_548{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.1.bn1.gamma, [3]: param_layer4.1.bn1.beta, [4]: param_layer4.1.bn1.moving_mean, [5]: param_layer4.1.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_271:CNode_549{[0]: ValueNode<Primitive> Return, [1]: CNode_548}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_270 : 0000029BC75AD300
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_270 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para513_x) {
  %1(CNode_551) = call @mindspore_nn_layer_conv_Conv2d_construct_550()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_270:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.1.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_270:CNode_551{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_550}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_270:CNode_552{[0]: ValueNode<Primitive> Return, [1]: CNode_551}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_268 : 0000029BC75AA880
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_268 parent: [subgraph @__main___ResidualBlock_construct_267]() {
  %1(CNode_554) = call @__main___ResidualBlock_construct_553()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_268:CNode_554{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_553}
#   2: @__main___ResidualBlock_construct_268:CNode_555{[0]: ValueNode<Primitive> Return, [1]: CNode_554}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_287 : 0000029BC75A9890
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_287 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para514_x) {
  %1(CNode_556) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513(%para514_x, %para160_layer4.2.bn3.gamma, %para161_layer4.2.bn3.beta, %para334_layer4.2.bn3.moving_mean, %para335_layer4.2.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_287:CNode_556{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513, [1]: param_x, [2]: param_layer4.2.bn3.gamma, [3]: param_layer4.2.bn3.beta, [4]: param_layer4.2.bn3.moving_mean, [5]: param_layer4.2.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_287:CNode_557{[0]: ValueNode<Primitive> Return, [1]: CNode_556}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_286 : 0000029BC75A88A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_286 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para515_x) {
  %1(CNode_559) = call @mindspore_nn_layer_conv_Conv2d_construct_558()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_286:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.2.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_286:CNode_559{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_558}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_286:CNode_560{[0]: ValueNode<Primitive> Return, [1]: CNode_559}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_283 : 0000029BC75A8350
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_283(%para516_x) {
  %1(CNode_561) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para516_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_283:CNode_561{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_283:CNode_562{[0]: ValueNode<Primitive> Return, [1]: CNode_561}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_285 : 0000029BC75AA330
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_285 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para517_x) {
  %1(CNode_563) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para517_x, %para157_layer4.2.bn2.gamma, %para158_layer4.2.bn2.beta, %para360_layer4.2.bn2.moving_mean, %para361_layer4.2.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_285:CNode_563{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.2.bn2.gamma, [3]: param_layer4.2.bn2.beta, [4]: param_layer4.2.bn2.moving_mean, [5]: param_layer4.2.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_285:CNode_564{[0]: ValueNode<Primitive> Return, [1]: CNode_563}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_284 : 0000029BC75A7360
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_284 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para518_x) {
  %1(CNode_566) = call @mindspore_nn_layer_conv_Conv2d_construct_565()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_284:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.2.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_284:CNode_566{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_565}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_284:CNode_567{[0]: ValueNode<Primitive> Return, [1]: CNode_566}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_282 : 0000029BC75A6E10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_282 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para519_x) {
  %1(CNode_568) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para519_x, %para154_layer4.2.bn1.gamma, %para155_layer4.2.bn1.beta, %para392_layer4.2.bn1.moving_mean, %para393_layer4.2.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_282:CNode_568{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer4.2.bn1.gamma, [3]: param_layer4.2.bn1.beta, [4]: param_layer4.2.bn1.moving_mean, [5]: param_layer4.2.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_282:CNode_569{[0]: ValueNode<Primitive> Return, [1]: CNode_568}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_281 : 0000029BC75A6370
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_281 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para520_x) {
  %1(CNode_571) = call @mindspore_nn_layer_conv_Conv2d_construct_570()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_281:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.2.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_281:CNode_571{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_570}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_281:CNode_572{[0]: ValueNode<Primitive> Return, [1]: CNode_571}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_279 : 0000029BC75AC310
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_279 parent: [subgraph @__main___ResidualBlock_construct_278]() {
  %1(CNode_574) = call @__main___ResidualBlock_construct_573()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_279:CNode_574{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_573}
#   2: @__main___ResidualBlock_construct_279:CNode_575{[0]: ValueNode<Primitive> Return, [1]: CNode_574}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_292 : 0000029BC75BDCA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_292 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_164]() {
  %1(CNode_290) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para473_@CNode_290, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_576) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_167) = $(mindspore_nn_layer_container_SequentialCell_construct_43):MakeTuple(@__main___ResidualBlock_construct_256, @__main___ResidualBlock_construct_267, @__main___ResidualBlock_construct_278)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_578) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para473_@CNode_290)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para474_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_579) = call @mindspore_nn_layer_container_SequentialCell_construct_164(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_580) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_292:CNode_578{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_167}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_292:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_578, [2]: param_@CNode_290}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_292:CNode_290{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_290, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_292:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_292:CNode_579{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_164, [1]: CNode_290, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_292:CNode_582{[0]: ValueNode<Primitive> Return, [1]: CNode_580}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_293 : 0000029BC75BD750
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_293 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_164]() {
  Return(%para474_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_293:CNode_583{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_306 : 0000029BC75A2900
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_306 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para521_x) {
  %1(CNode_585) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para521_x, %para82_layer3.0.bn3.gamma, %para83_layer3.0.bn3.beta, %para336_layer3.0.bn3.moving_mean, %para337_layer3.0.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_306:CNode_585{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.0.bn3.gamma, [3]: param_layer3.0.bn3.beta, [4]: param_layer3.0.bn3.moving_mean, [5]: param_layer3.0.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_306:CNode_586{[0]: ValueNode<Primitive> Return, [1]: CNode_585}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_305 : 0000029BC75959D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_305 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para522_x) {
  %1(CNode_588) = call @mindspore_nn_layer_conv_Conv2d_construct_587()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_305:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.0.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_305:CNode_588{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_587}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_305:CNode_589{[0]: ValueNode<Primitive> Return, [1]: CNode_588}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_302 : 0000029BC75989A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_302(%para523_x) {
  %1(CNode_590) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para523_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_302:CNode_590{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_302:CNode_591{[0]: ValueNode<Primitive> Return, [1]: CNode_590}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_304 : 0000029BC7596F10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_304 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para524_x) {
  %1(CNode_593) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para524_x, %para79_layer3.0.bn2.gamma, %para80_layer3.0.bn2.beta, %para368_layer3.0.bn2.moving_mean, %para369_layer3.0.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_304:CNode_593{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.0.bn2.gamma, [3]: param_layer3.0.bn2.beta, [4]: param_layer3.0.bn2.moving_mean, [5]: param_layer3.0.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_304:CNode_594{[0]: ValueNode<Primitive> Return, [1]: CNode_593}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_303 : 0000029BC7596470
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_303 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para525_x) {
  %1(CNode_596) = call @mindspore_nn_layer_conv_Conv2d_construct_595()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_303:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.0.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_303:CNode_596{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_595}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_303:CNode_597{[0]: ValueNode<Primitive> Return, [1]: CNode_596}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_301 : 0000029BC759B970
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_301 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para526_x) {
  %1(CNode_598) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para526_x, %para76_layer3.0.bn1.gamma, %para77_layer3.0.bn1.beta, %para400_layer3.0.bn1.moving_mean, %para401_layer3.0.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_301:CNode_598{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.0.bn1.gamma, [3]: param_layer3.0.bn1.beta, [4]: param_layer3.0.bn1.moving_mean, [5]: param_layer3.0.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_301:CNode_599{[0]: ValueNode<Primitive> Return, [1]: CNode_598}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_300 : 0000029BC7597460
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_300 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para527_x) {
  %1(CNode_601) = call @mindspore_nn_layer_conv_Conv2d_construct_600()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_300:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.0.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_300:CNode_601{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_600}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_300:CNode_602{[0]: ValueNode<Primitive> Return, [1]: CNode_601}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_298 : 0000029BC75A23B0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_298 parent: [subgraph @__main___ResidualBlock_construct_297]() {
  %1(CNode_604) = call @__main___ResidualBlock_construct_603()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
}
# Order:
#   1: @__main___ResidualBlock_construct_298:identity{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_605, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_298:CNode_604{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_603}
#   3: @__main___ResidualBlock_construct_298:CNode_606{[0]: ValueNode<Primitive> Return, [1]: CNode_604}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_317 : 0000029BC7595F20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_317 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para528_x) {
  %1(CNode_607) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para528_x, %para94_layer3.1.bn3.gamma, %para95_layer3.1.bn3.beta, %para338_layer3.1.bn3.moving_mean, %para339_layer3.1.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_317:CNode_607{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.1.bn3.gamma, [3]: param_layer3.1.bn3.beta, [4]: param_layer3.1.bn3.moving_mean, [5]: param_layer3.1.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_317:CNode_608{[0]: ValueNode<Primitive> Return, [1]: CNode_607}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_316 : 0000029BC759A980
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_316 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para529_x) {
  %1(CNode_610) = call @mindspore_nn_layer_conv_Conv2d_construct_609()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_316:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.1.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_316:CNode_610{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_609}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_316:CNode_611{[0]: ValueNode<Primitive> Return, [1]: CNode_610}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_313 : 0000029BC759CEB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_313(%para530_x) {
  %1(CNode_612) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para530_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_313:CNode_612{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_313:CNode_613{[0]: ValueNode<Primitive> Return, [1]: CNode_612}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_315 : 0000029BC7595480
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_315 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para531_x) {
  %1(CNode_614) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para531_x, %para91_layer3.1.bn2.gamma, %para92_layer3.1.bn2.beta, %para370_layer3.1.bn2.moving_mean, %para371_layer3.1.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_315:CNode_614{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.1.bn2.gamma, [3]: param_layer3.1.bn2.beta, [4]: param_layer3.1.bn2.moving_mean, [5]: param_layer3.1.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_315:CNode_615{[0]: ValueNode<Primitive> Return, [1]: CNode_614}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_314 : 0000029BC7599440
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_314 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para532_x) {
  %1(CNode_617) = call @mindspore_nn_layer_conv_Conv2d_construct_616()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_314:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.1.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_314:CNode_617{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_616}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_314:CNode_618{[0]: ValueNode<Primitive> Return, [1]: CNode_617}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_312 : 0000029BC759C410
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_312 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para533_x) {
  %1(CNode_619) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para533_x, %para88_layer3.1.bn1.gamma, %para89_layer3.1.bn1.beta, %para402_layer3.1.bn1.moving_mean, %para403_layer3.1.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_312:CNode_619{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.1.bn1.gamma, [3]: param_layer3.1.bn1.beta, [4]: param_layer3.1.bn1.moving_mean, [5]: param_layer3.1.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_312:CNode_620{[0]: ValueNode<Primitive> Return, [1]: CNode_619}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_311 : 0000029BC74E0F30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_311 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para534_x) {
  %1(CNode_622) = call @mindspore_nn_layer_conv_Conv2d_construct_621()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_311:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.1.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_311:CNode_622{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_621}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_311:CNode_623{[0]: ValueNode<Primitive> Return, [1]: CNode_622}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_309 : 0000029BC7598EF0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_309 parent: [subgraph @__main___ResidualBlock_construct_308]() {
  %1(CNode_625) = call @__main___ResidualBlock_construct_624()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_309:CNode_625{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_624}
#   2: @__main___ResidualBlock_construct_309:CNode_626{[0]: ValueNode<Primitive> Return, [1]: CNode_625}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_328 : 0000029BC74DF9F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_328 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para535_x) {
  %1(CNode_627) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para535_x, %para103_layer3.2.bn3.gamma, %para104_layer3.2.bn3.beta, %para340_layer3.2.bn3.moving_mean, %para341_layer3.2.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_328:CNode_627{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.2.bn3.gamma, [3]: param_layer3.2.bn3.beta, [4]: param_layer3.2.bn3.moving_mean, [5]: param_layer3.2.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_328:CNode_628{[0]: ValueNode<Primitive> Return, [1]: CNode_627}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_327 : 0000029BC74DEF50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_327 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para536_x) {
  %1(CNode_630) = call @mindspore_nn_layer_conv_Conv2d_construct_629()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_327:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.2.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_327:CNode_630{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_629}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_327:CNode_631{[0]: ValueNode<Primitive> Return, [1]: CNode_630}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_324 : 0000029BC74DEA00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_324(%para537_x) {
  %1(CNode_632) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para537_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_324:CNode_632{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_324:CNode_633{[0]: ValueNode<Primitive> Return, [1]: CNode_632}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_326 : 0000029BC74D6A80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_326 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para538_x) {
  %1(CNode_634) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para538_x, %para100_layer3.2.bn2.gamma, %para101_layer3.2.bn2.beta, %para372_layer3.2.bn2.moving_mean, %para373_layer3.2.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_326:CNode_634{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.2.bn2.gamma, [3]: param_layer3.2.bn2.beta, [4]: param_layer3.2.bn2.moving_mean, [5]: param_layer3.2.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_326:CNode_635{[0]: ValueNode<Primitive> Return, [1]: CNode_634}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_325 : 0000029BC74DCA20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_325 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para539_x) {
  %1(CNode_637) = call @mindspore_nn_layer_conv_Conv2d_construct_636()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_325:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.2.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_325:CNode_637{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_636}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_325:CNode_638{[0]: ValueNode<Primitive> Return, [1]: CNode_637}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_323 : 0000029BC74D7FC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_323 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para540_x) {
  %1(CNode_639) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para540_x, %para97_layer3.2.bn1.gamma, %para98_layer3.2.bn1.beta, %para404_layer3.2.bn1.moving_mean, %para405_layer3.2.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_323:CNode_639{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.2.bn1.gamma, [3]: param_layer3.2.bn1.beta, [4]: param_layer3.2.bn1.moving_mean, [5]: param_layer3.2.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_323:CNode_640{[0]: ValueNode<Primitive> Return, [1]: CNode_639}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_322 : 0000029BC74DBF80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_322 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para541_x) {
  %1(CNode_642) = call @mindspore_nn_layer_conv_Conv2d_construct_641()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_322:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.2.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_322:CNode_642{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_641}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_322:CNode_643{[0]: ValueNode<Primitive> Return, [1]: CNode_642}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_320 : 0000029BC74DFF40
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_320 parent: [subgraph @__main___ResidualBlock_construct_319]() {
  %1(CNode_645) = call @__main___ResidualBlock_construct_644()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_320:CNode_645{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_644}
#   2: @__main___ResidualBlock_construct_320:CNode_646{[0]: ValueNode<Primitive> Return, [1]: CNode_645}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_339 : 0000029BC74D9A50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_339 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para542_x) {
  %1(CNode_647) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para542_x, %para112_layer3.3.bn3.gamma, %para113_layer3.3.bn3.beta, %para342_layer3.3.bn3.moving_mean, %para343_layer3.3.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_339:CNode_647{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.3.bn3.gamma, [3]: param_layer3.3.bn3.beta, [4]: param_layer3.3.bn3.moving_mean, [5]: param_layer3.3.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_339:CNode_648{[0]: ValueNode<Primitive> Return, [1]: CNode_647}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_338 : 0000029BC74D9FA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_338 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para543_x) {
  %1(CNode_650) = call @mindspore_nn_layer_conv_Conv2d_construct_649()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_338:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.3.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_338:CNode_650{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_649}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_338:CNode_651{[0]: ValueNode<Primitive> Return, [1]: CNode_650}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_335 : 0000029BC74D9500
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_335(%para544_x) {
  %1(CNode_652) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para544_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_335:CNode_652{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_335:CNode_653{[0]: ValueNode<Primitive> Return, [1]: CNode_652}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_337 : 0000029BC74D8A60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_337 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para545_x) {
  %1(CNode_654) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para545_x, %para109_layer3.3.bn2.gamma, %para110_layer3.3.bn2.beta, %para374_layer3.3.bn2.moving_mean, %para375_layer3.3.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_337:CNode_654{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.3.bn2.gamma, [3]: param_layer3.3.bn2.beta, [4]: param_layer3.3.bn2.moving_mean, [5]: param_layer3.3.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_337:CNode_655{[0]: ValueNode<Primitive> Return, [1]: CNode_654}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_336 : 0000029BC74DAA40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_336 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para546_x) {
  %1(CNode_657) = call @mindspore_nn_layer_conv_Conv2d_construct_656()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_336:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.3.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_336:CNode_657{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_656}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_336:CNode_658{[0]: ValueNode<Primitive> Return, [1]: CNode_657}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_334 : 0000029BC74DCF70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_334 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para547_x) {
  %1(CNode_659) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para547_x, %para106_layer3.3.bn1.gamma, %para107_layer3.3.bn1.beta, %para406_layer3.3.bn1.moving_mean, %para407_layer3.3.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_334:CNode_659{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.3.bn1.gamma, [3]: param_layer3.3.bn1.beta, [4]: param_layer3.3.bn1.moving_mean, [5]: param_layer3.3.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_334:CNode_660{[0]: ValueNode<Primitive> Return, [1]: CNode_659}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_333 : 0000029BC74DD4C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_333 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para548_x) {
  %1(CNode_662) = call @mindspore_nn_layer_conv_Conv2d_construct_661()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_333:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.3.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_333:CNode_662{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_661}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_333:CNode_663{[0]: ValueNode<Primitive> Return, [1]: CNode_662}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_331 : 0000029BC74D8FB0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_331 parent: [subgraph @__main___ResidualBlock_construct_330]() {
  %1(CNode_665) = call @__main___ResidualBlock_construct_664()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_331:CNode_665{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_664}
#   2: @__main___ResidualBlock_construct_331:CNode_666{[0]: ValueNode<Primitive> Return, [1]: CNode_665}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_350 : 0000029BC74D5A90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_350 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para549_x) {
  %1(CNode_667) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para549_x, %para121_layer3.4.bn3.gamma, %para122_layer3.4.bn3.beta, %para344_layer3.4.bn3.moving_mean, %para345_layer3.4.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_350:CNode_667{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.4.bn3.gamma, [3]: param_layer3.4.bn3.beta, [4]: param_layer3.4.bn3.moving_mean, [5]: param_layer3.4.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_350:CNode_668{[0]: ValueNode<Primitive> Return, [1]: CNode_667}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_349 : 0000029BC74D0AE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_349 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para550_x) {
  %1(CNode_670) = call @mindspore_nn_layer_conv_Conv2d_construct_669()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_349:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.4.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_349:CNode_670{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_669}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_349:CNode_671{[0]: ValueNode<Primitive> Return, [1]: CNode_670}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_346 : 0000029BC74D5540
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_346(%para551_x) {
  %1(CNode_672) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para551_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_346:CNode_672{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_346:CNode_673{[0]: ValueNode<Primitive> Return, [1]: CNode_672}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_348 : 0000029BC74D0590
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_348 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para552_x) {
  %1(CNode_674) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para552_x, %para118_layer3.4.bn2.gamma, %para119_layer3.4.bn2.beta, %para376_layer3.4.bn2.moving_mean, %para377_layer3.4.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_348:CNode_674{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.4.bn2.gamma, [3]: param_layer3.4.bn2.beta, [4]: param_layer3.4.bn2.moving_mean, [5]: param_layer3.4.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_348:CNode_675{[0]: ValueNode<Primitive> Return, [1]: CNode_674}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_347 : 0000029BC74CD5C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_347 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para553_x) {
  %1(CNode_677) = call @mindspore_nn_layer_conv_Conv2d_construct_676()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_347:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.4.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_347:CNode_677{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_676}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_347:CNode_678{[0]: ValueNode<Primitive> Return, [1]: CNode_677}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_345 : 0000029BC74D4000
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_345 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para554_x) {
  %1(CNode_679) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para554_x, %para115_layer3.4.bn1.gamma, %para116_layer3.4.bn1.beta, %para408_layer3.4.bn1.moving_mean, %para409_layer3.4.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_345:CNode_679{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.4.bn1.gamma, [3]: param_layer3.4.bn1.beta, [4]: param_layer3.4.bn1.moving_mean, [5]: param_layer3.4.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_345:CNode_680{[0]: ValueNode<Primitive> Return, [1]: CNode_679}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_344 : 0000029BC74D4FF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_344 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para555_x) {
  %1(CNode_682) = call @mindspore_nn_layer_conv_Conv2d_construct_681()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_344:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.4.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_344:CNode_682{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_681}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_344:CNode_683{[0]: ValueNode<Primitive> Return, [1]: CNode_682}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_342 : 0000029BC74CF050
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_342 parent: [subgraph @__main___ResidualBlock_construct_341]() {
  %1(CNode_685) = call @__main___ResidualBlock_construct_684()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_342:CNode_685{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_684}
#   2: @__main___ResidualBlock_construct_342:CNode_686{[0]: ValueNode<Primitive> Return, [1]: CNode_685}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_361 : 0000029BC74D1030
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_361 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para556_x) {
  %1(CNode_687) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para556_x, %para130_layer3.5.bn3.gamma, %para131_layer3.5.bn3.beta, %para346_layer3.5.bn3.moving_mean, %para347_layer3.5.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_361:CNode_687{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584, [1]: param_x, [2]: param_layer3.5.bn3.gamma, [3]: param_layer3.5.bn3.beta, [4]: param_layer3.5.bn3.moving_mean, [5]: param_layer3.5.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_361:CNode_688{[0]: ValueNode<Primitive> Return, [1]: CNode_687}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_360 : 0000029BC74D3560
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_360 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para557_x) {
  %1(CNode_690) = call @mindspore_nn_layer_conv_Conv2d_construct_689()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_360:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.5.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_360:CNode_690{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_689}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_360:CNode_691{[0]: ValueNode<Primitive> Return, [1]: CNode_690}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_357 : 0000029BC750C1D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_357(%para558_x) {
  %1(CNode_692) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para558_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_357:CNode_692{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_357:CNode_693{[0]: ValueNode<Primitive> Return, [1]: CNode_692}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_359 : 0000029BC750BC80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_359 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para559_x) {
  %1(CNode_694) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para559_x, %para127_layer3.5.bn2.gamma, %para128_layer3.5.bn2.beta, %para378_layer3.5.bn2.moving_mean, %para379_layer3.5.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_359:CNode_694{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.5.bn2.gamma, [3]: param_layer3.5.bn2.beta, [4]: param_layer3.5.bn2.moving_mean, [5]: param_layer3.5.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_359:CNode_695{[0]: ValueNode<Primitive> Return, [1]: CNode_694}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_358 : 0000029BC750A740
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_358 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para560_x) {
  %1(CNode_697) = call @mindspore_nn_layer_conv_Conv2d_construct_696()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_358:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.5.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_358:CNode_697{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_696}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_358:CNode_698{[0]: ValueNode<Primitive> Return, [1]: CNode_697}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_356 : 0000029BC750A1F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_356 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para561_x) {
  %1(CNode_699) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para561_x, %para124_layer3.5.bn1.gamma, %para125_layer3.5.bn1.beta, %para410_layer3.5.bn1.moving_mean, %para411_layer3.5.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_356:CNode_699{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer3.5.bn1.gamma, [3]: param_layer3.5.bn1.beta, [4]: param_layer3.5.bn1.moving_mean, [5]: param_layer3.5.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_356:CNode_700{[0]: ValueNode<Primitive> Return, [1]: CNode_699}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_355 : 0000029BC7509200
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_355 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para562_x) {
  %1(CNode_702) = call @mindspore_nn_layer_conv_Conv2d_construct_701()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_355:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.5.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_355:CNode_702{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_701}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_355:CNode_703{[0]: ValueNode<Primitive> Return, [1]: CNode_702}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_353 : 0000029BC74CFAF0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_353 parent: [subgraph @__main___ResidualBlock_construct_352]() {
  %1(CNode_705) = call @__main___ResidualBlock_construct_704()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_353:CNode_705{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_704}
#   2: @__main___ResidualBlock_construct_353:CNode_706{[0]: ValueNode<Primitive> Return, [1]: CNode_705}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_366 : 0000029BC75A4390
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_366 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_169]() {
  %1(CNode_364) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para481_@CNode_364, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_707) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_172) = $(mindspore_nn_layer_container_SequentialCell_construct_42):MakeTuple(@__main___ResidualBlock_construct_297, @__main___ResidualBlock_construct_308, @__main___ResidualBlock_construct_319, @__main___ResidualBlock_construct_330, @__main___ResidualBlock_construct_341, @__main___ResidualBlock_construct_352)
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_708) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para481_@CNode_364)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para482_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_709) = call @mindspore_nn_layer_container_SequentialCell_construct_169(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_710) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_366:CNode_708{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_172}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_366:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_708, [2]: param_@CNode_364}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_366:CNode_364{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_364, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_366:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_366:CNode_709{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_169, [1]: CNode_364, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_366:CNode_711{[0]: ValueNode<Primitive> Return, [1]: CNode_710}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_367 : 0000029BC75A3E40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_367 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_169]() {
  Return(%para482_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_367:CNode_712{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_379 : 0000029BC7503D00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_379 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para563_x) {
  %1(CNode_714) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713(%para563_x, %para43_layer2.0.bn3.gamma, %para44_layer2.0.bn3.beta, %para348_layer2.0.bn3.moving_mean, %para349_layer2.0.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_379:CNode_714{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713, [1]: param_x, [2]: param_layer2.0.bn3.gamma, [3]: param_layer2.0.bn3.beta, [4]: param_layer2.0.bn3.moving_mean, [5]: param_layer2.0.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_379:CNode_715{[0]: ValueNode<Primitive> Return, [1]: CNode_714}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_378 : 0000029BC74FFD40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_378 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para564_x) {
  %1(CNode_717) = call @mindspore_nn_layer_conv_Conv2d_construct_716()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_378:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.0.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_378:CNode_717{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_716}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_378:CNode_718{[0]: ValueNode<Primitive> Return, [1]: CNode_717}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_375 : 0000029BC74FA2F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_375(%para565_x) {
  %1(CNode_719) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para565_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_375:CNode_719{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_375:CNode_720{[0]: ValueNode<Primitive> Return, [1]: CNode_719}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_377 : 0000029BC7501D20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_377 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para566_x) {
  %1(CNode_722) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para566_x, %para40_layer2.0.bn2.gamma, %para41_layer2.0.bn2.beta, %para380_layer2.0.bn2.moving_mean, %para381_layer2.0.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_377:CNode_722{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.0.bn2.gamma, [3]: param_layer2.0.bn2.beta, [4]: param_layer2.0.bn2.moving_mean, [5]: param_layer2.0.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_377:CNode_723{[0]: ValueNode<Primitive> Return, [1]: CNode_722}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_376 : 0000029BC74FAD90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_376 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para567_x) {
  %1(CNode_725) = call @mindspore_nn_layer_conv_Conv2d_construct_724()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_376:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.0.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_376:CNode_725{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_724}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_376:CNode_726{[0]: ValueNode<Primitive> Return, [1]: CNode_725}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_374 : 0000029BC74F9DA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_374 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para568_x) {
  %1(CNode_727) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para568_x, %para37_layer2.0.bn1.gamma, %para38_layer2.0.bn1.beta, %para412_layer2.0.bn1.moving_mean, %para413_layer2.0.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_374:CNode_727{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.0.bn1.gamma, [3]: param_layer2.0.bn1.beta, [4]: param_layer2.0.bn1.moving_mean, [5]: param_layer2.0.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_374:CNode_728{[0]: ValueNode<Primitive> Return, [1]: CNode_727}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_373 : 0000029BC75017D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_373 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para569_x) {
  %1(CNode_730) = call @mindspore_nn_layer_conv_Conv2d_construct_729()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_373:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.0.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_373:CNode_730{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_729}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_373:CNode_731{[0]: ValueNode<Primitive> Return, [1]: CNode_730}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_371 : 0000029BC75027C0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_371 parent: [subgraph @__main___ResidualBlock_construct_370]() {
  %1(CNode_733) = call @__main___ResidualBlock_construct_732()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
}
# Order:
#   1: @__main___ResidualBlock_construct_371:identity{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_734, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_371:CNode_733{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_732}
#   3: @__main___ResidualBlock_construct_371:CNode_735{[0]: ValueNode<Primitive> Return, [1]: CNode_733}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_390 : 0000029BC74FE800
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_390 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para570_x) {
  %1(CNode_736) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713(%para570_x, %para55_layer2.1.bn3.gamma, %para56_layer2.1.bn3.beta, %para350_layer2.1.bn3.moving_mean, %para351_layer2.1.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_390:CNode_736{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713, [1]: param_x, [2]: param_layer2.1.bn3.gamma, [3]: param_layer2.1.bn3.beta, [4]: param_layer2.1.bn3.moving_mean, [5]: param_layer2.1.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_390:CNode_737{[0]: ValueNode<Primitive> Return, [1]: CNode_736}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_389 : 0000029BC74FE2B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_389 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para571_x) {
  %1(CNode_739) = call @mindspore_nn_layer_conv_Conv2d_construct_738()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_389:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.1.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_389:CNode_739{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_738}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_389:CNode_740{[0]: ValueNode<Primitive> Return, [1]: CNode_739}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_386 : 0000029BC74FDD60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_386(%para572_x) {
  %1(CNode_741) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para572_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_386:CNode_741{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_386:CNode_742{[0]: ValueNode<Primitive> Return, [1]: CNode_741}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_388 : 0000029BC74FD810
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_388 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para573_x) {
  %1(CNode_743) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para573_x, %para52_layer2.1.bn2.gamma, %para53_layer2.1.bn2.beta, %para382_layer2.1.bn2.moving_mean, %para383_layer2.1.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_388:CNode_743{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.1.bn2.gamma, [3]: param_layer2.1.bn2.beta, [4]: param_layer2.1.bn2.moving_mean, [5]: param_layer2.1.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_388:CNode_744{[0]: ValueNode<Primitive> Return, [1]: CNode_743}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_387 : 0000029BC74FC820
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_387 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para574_x) {
  %1(CNode_746) = call @mindspore_nn_layer_conv_Conv2d_construct_745()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_387:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.1.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_387:CNode_746{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_745}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_387:CNode_747{[0]: ValueNode<Primitive> Return, [1]: CNode_746}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_385 : 0000029BC74FC2D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_385 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para575_x) {
  %1(CNode_748) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para575_x, %para49_layer2.1.bn1.gamma, %para50_layer2.1.bn1.beta, %para414_layer2.1.bn1.moving_mean, %para415_layer2.1.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_385:CNode_748{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.1.bn1.gamma, [3]: param_layer2.1.bn1.beta, [4]: param_layer2.1.bn1.moving_mean, [5]: param_layer2.1.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_385:CNode_749{[0]: ValueNode<Primitive> Return, [1]: CNode_748}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_384 : 0000029BC74F6880
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_384 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para576_x) {
  %1(CNode_751) = call @mindspore_nn_layer_conv_Conv2d_construct_750()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_384:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.1.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_384:CNode_751{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_750}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_384:CNode_752{[0]: ValueNode<Primitive> Return, [1]: CNode_751}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_382 : 0000029BC74F8310
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_382 parent: [subgraph @__main___ResidualBlock_construct_381]() {
  %1(CNode_754) = call @__main___ResidualBlock_construct_753()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_382:CNode_754{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_753}
#   2: @__main___ResidualBlock_construct_382:CNode_755{[0]: ValueNode<Primitive> Return, [1]: CNode_754}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_401 : 0000029BC74F7320
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_401 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para577_x) {
  %1(CNode_756) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713(%para577_x, %para64_layer2.2.bn3.gamma, %para65_layer2.2.bn3.beta, %para352_layer2.2.bn3.moving_mean, %para353_layer2.2.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_401:CNode_756{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713, [1]: param_x, [2]: param_layer2.2.bn3.gamma, [3]: param_layer2.2.bn3.beta, [4]: param_layer2.2.bn3.moving_mean, [5]: param_layer2.2.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_401:CNode_757{[0]: ValueNode<Primitive> Return, [1]: CNode_756}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_400 : 0000029BC74F6330
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_400 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para578_x) {
  %1(CNode_759) = call @mindspore_nn_layer_conv_Conv2d_construct_758()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_400:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.2.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_400:CNode_759{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_758}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_400:CNode_760{[0]: ValueNode<Primitive> Return, [1]: CNode_759}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_397 : 0000029BC74F4DF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_397(%para579_x) {
  %1(CNode_761) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para579_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_397:CNode_761{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_397:CNode_762{[0]: ValueNode<Primitive> Return, [1]: CNode_761}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_399 : 0000029BC74F5340
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_399 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para580_x) {
  %1(CNode_763) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para580_x, %para61_layer2.2.bn2.gamma, %para62_layer2.2.bn2.beta, %para384_layer2.2.bn2.moving_mean, %para385_layer2.2.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_399:CNode_763{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.2.bn2.gamma, [3]: param_layer2.2.bn2.beta, [4]: param_layer2.2.bn2.moving_mean, [5]: param_layer2.2.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_399:CNode_764{[0]: ValueNode<Primitive> Return, [1]: CNode_763}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_398 : 0000029BC74F4350
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_398 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para581_x) {
  %1(CNode_766) = call @mindspore_nn_layer_conv_Conv2d_construct_765()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_398:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.2.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_398:CNode_766{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_765}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_398:CNode_767{[0]: ValueNode<Primitive> Return, [1]: CNode_766}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_396 : 0000029BC74F5890
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_396 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para582_x) {
  %1(CNode_768) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para582_x, %para58_layer2.2.bn1.gamma, %para59_layer2.2.bn1.beta, %para416_layer2.2.bn1.moving_mean, %para417_layer2.2.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_396:CNode_768{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.2.bn1.gamma, [3]: param_layer2.2.bn1.beta, [4]: param_layer2.2.bn1.moving_mean, [5]: param_layer2.2.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_396:CNode_769{[0]: ValueNode<Primitive> Return, [1]: CNode_768}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_395 : 0000029BC74EBE80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_395 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para583_x) {
  %1(CNode_771) = call @mindspore_nn_layer_conv_Conv2d_construct_770()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_395:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.2.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_395:CNode_771{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_770}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_395:CNode_772{[0]: ValueNode<Primitive> Return, [1]: CNode_771}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_393 : 0000029BC74F9850
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_393 parent: [subgraph @__main___ResidualBlock_construct_392]() {
  %1(CNode_774) = call @__main___ResidualBlock_construct_773()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_393:CNode_774{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_773}
#   2: @__main___ResidualBlock_construct_393:CNode_775{[0]: ValueNode<Primitive> Return, [1]: CNode_774}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_412 : 0000029BC74EEE50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_412 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para584_x) {
  %1(CNode_776) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713(%para584_x, %para73_layer2.3.bn3.gamma, %para74_layer2.3.bn3.beta, %para354_layer2.3.bn3.moving_mean, %para355_layer2.3.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_412:CNode_776{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713, [1]: param_x, [2]: param_layer2.3.bn3.gamma, [3]: param_layer2.3.bn3.beta, [4]: param_layer2.3.bn3.moving_mean, [5]: param_layer2.3.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_412:CNode_777{[0]: ValueNode<Primitive> Return, [1]: CNode_776}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_411 : 0000029BC74EC920
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_411 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para585_x) {
  %1(CNode_779) = call @mindspore_nn_layer_conv_Conv2d_construct_778()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_411:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.3.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_411:CNode_779{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_778}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_411:CNode_780{[0]: ValueNode<Primitive> Return, [1]: CNode_779}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_408 : 0000029BC74EE900
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_408(%para586_x) {
  %1(CNode_781) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para586_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_408:CNode_781{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_408:CNode_782{[0]: ValueNode<Primitive> Return, [1]: CNode_781}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_410 : 0000029BC74F08E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_410 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para587_x) {
  %1(CNode_783) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para587_x, %para70_layer2.3.bn2.gamma, %para71_layer2.3.bn2.beta, %para386_layer2.3.bn2.moving_mean, %para387_layer2.3.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_410:CNode_783{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.3.bn2.gamma, [3]: param_layer2.3.bn2.beta, [4]: param_layer2.3.bn2.moving_mean, [5]: param_layer2.3.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_410:CNode_784{[0]: ValueNode<Primitive> Return, [1]: CNode_783}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_409 : 0000029BC74ED910
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_409 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para588_x) {
  %1(CNode_786) = call @mindspore_nn_layer_conv_Conv2d_construct_785()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_409:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.3.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_409:CNode_786{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_785}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_409:CNode_787{[0]: ValueNode<Primitive> Return, [1]: CNode_786}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_407 : 0000029BC74E9EA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_407 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para589_x) {
  %1(CNode_788) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para589_x, %para67_layer2.3.bn1.gamma, %para68_layer2.3.bn1.beta, %para418_layer2.3.bn1.moving_mean, %para419_layer2.3.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_407:CNode_788{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721, [1]: param_x, [2]: param_layer2.3.bn1.gamma, [3]: param_layer2.3.bn1.beta, [4]: param_layer2.3.bn1.moving_mean, [5]: param_layer2.3.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_407:CNode_789{[0]: ValueNode<Primitive> Return, [1]: CNode_788}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_406 : 0000029BC74E19D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_406 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para590_x) {
  %1(CNode_791) = call @mindspore_nn_layer_conv_Conv2d_construct_790()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_406:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.3.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_406:CNode_791{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_790}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_406:CNode_792{[0]: ValueNode<Primitive> Return, [1]: CNode_791}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_404 : 0000029BC74F2E10
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_404 parent: [subgraph @__main___ResidualBlock_construct_403]() {
  %1(CNode_794) = call @__main___ResidualBlock_construct_793()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_404:CNode_794{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_793}
#   2: @__main___ResidualBlock_construct_404:CNode_795{[0]: ValueNode<Primitive> Return, [1]: CNode_794}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_417 : 0000029BC7508760
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_417 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_174]() {
  %1(CNode_415) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para487_@CNode_415, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_796) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_177) = $(mindspore_nn_layer_container_SequentialCell_construct_41):MakeTuple(@__main___ResidualBlock_construct_370, @__main___ResidualBlock_construct_381, @__main___ResidualBlock_construct_392, @__main___ResidualBlock_construct_403)
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_797) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para487_@CNode_415)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para488_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_798) = call @mindspore_nn_layer_container_SequentialCell_construct_174(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_799) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_417:CNode_797{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_177}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_417:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_797, [2]: param_@CNode_415}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_417:CNode_415{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_415, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_417:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_417:CNode_798{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_174, [1]: CNode_415, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_417:CNode_800{[0]: ValueNode<Primitive> Return, [1]: CNode_799}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_418 : 0000029BC750B730
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_418 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_174]() {
  Return(%para488_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_418:CNode_801{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_430 : 0000029BC729C9F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_430 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para591_x) {
  %1(CNode_803) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802(%para591_x, %para13_layer1.0.bn3.gamma, %para14_layer1.0.bn3.beta, %para362_layer1.0.bn3.moving_mean, %para363_layer1.0.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_803{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802, [1]: param_x, [2]: param_layer1.0.bn3.gamma, [3]: param_layer1.0.bn3.beta, [4]: param_layer1.0.bn3.moving_mean, [5]: param_layer1.0.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_430:CNode_804{[0]: ValueNode<Primitive> Return, [1]: CNode_803}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_429 : 0000029BC729C4A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_429 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para592_x) {
  %1(CNode_806) = call @mindspore_nn_layer_conv_Conv2d_construct_805()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_429:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.0.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_429:CNode_806{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_805}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_429:CNode_807{[0]: ValueNode<Primitive> Return, [1]: CNode_806}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_426 : 0000029BC729BF50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_426(%para593_x) {
  %1(CNode_808) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para593_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_426:CNode_808{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_426:CNode_809{[0]: ValueNode<Primitive> Return, [1]: CNode_808}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_428 : 0000029BC7293FD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_428 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para594_x) {
  %1(CNode_810) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para594_x, %para10_layer1.0.bn2.gamma, %para11_layer1.0.bn2.beta, %para394_layer1.0.bn2.moving_mean, %para395_layer1.0.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_428:CNode_810{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.0.bn2.gamma, [3]: param_layer1.0.bn2.beta, [4]: param_layer1.0.bn2.moving_mean, [5]: param_layer1.0.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_428:CNode_811{[0]: ValueNode<Primitive> Return, [1]: CNode_810}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_427 : 0000029BC729B4B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_427 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para595_x) {
  %1(CNode_813) = call @mindspore_nn_layer_conv_Conv2d_construct_812()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_427:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.0.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_427:CNode_813{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_812}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_427:CNode_814{[0]: ValueNode<Primitive> Return, [1]: CNode_813}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_425 : 0000029BC7293530
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_425 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para596_x) {
  %1(CNode_815) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para596_x, %para7_layer1.0.bn1.gamma, %para8_layer1.0.bn1.beta, %para420_layer1.0.bn1.moving_mean, %para421_layer1.0.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_425:CNode_815{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.0.bn1.gamma, [3]: param_layer1.0.bn1.beta, [4]: param_layer1.0.bn1.moving_mean, [5]: param_layer1.0.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_425:CNode_816{[0]: ValueNode<Primitive> Return, [1]: CNode_815}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_424 : 0000029BC7299F70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_424 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para597_x) {
  %1(CNode_818) = call @mindspore_nn_layer_conv_Conv2d_construct_817()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_424:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.0.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_424:CNode_818{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_817}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_424:CNode_819{[0]: ValueNode<Primitive> Return, [1]: CNode_818}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_422 : 0000029BC729CF40
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_422 parent: [subgraph @__main___ResidualBlock_construct_421]() {
  %1(CNode_821) = call @__main___ResidualBlock_construct_820()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
}
# Order:
#   1: @__main___ResidualBlock_construct_422:identity{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_822, [1]: param_identity}
#   2: @__main___ResidualBlock_construct_422:CNode_821{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_820}
#   3: @__main___ResidualBlock_construct_422:CNode_823{[0]: ValueNode<Primitive> Return, [1]: CNode_821}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_441 : 0000029BC7297F90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_441 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para598_x) {
  %1(CNode_824) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802(%para598_x, %para25_layer1.1.bn3.gamma, %para26_layer1.1.bn3.beta, %para364_layer1.1.bn3.moving_mean, %para365_layer1.1.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_441:CNode_824{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802, [1]: param_x, [2]: param_layer1.1.bn3.gamma, [3]: param_layer1.1.bn3.beta, [4]: param_layer1.1.bn3.moving_mean, [5]: param_layer1.1.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_441:CNode_825{[0]: ValueNode<Primitive> Return, [1]: CNode_824}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_440 : 0000029BC7296500
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_440 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para599_x) {
  %1(CNode_827) = call @mindspore_nn_layer_conv_Conv2d_construct_826()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_440:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.1.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_440:CNode_827{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_826}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_440:CNode_828{[0]: ValueNode<Primitive> Return, [1]: CNode_827}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_437 : 0000029BC7298F80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_437(%para600_x) {
  %1(CNode_829) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para600_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_437:CNode_829{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_437:CNode_830{[0]: ValueNode<Primitive> Return, [1]: CNode_829}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_439 : 0000029BC729AA10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_439 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para601_x) {
  %1(CNode_831) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para601_x, %para22_layer1.1.bn2.gamma, %para23_layer1.1.bn2.beta, %para396_layer1.1.bn2.moving_mean, %para397_layer1.1.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_439:CNode_831{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.1.bn2.gamma, [3]: param_layer1.1.bn2.beta, [4]: param_layer1.1.bn2.moving_mean, [5]: param_layer1.1.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_439:CNode_832{[0]: ValueNode<Primitive> Return, [1]: CNode_831}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_438 : 0000029BC7294FC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_438 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para602_x) {
  %1(CNode_834) = call @mindspore_nn_layer_conv_Conv2d_construct_833()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_438:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.1.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_438:CNode_834{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_833}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_438:CNode_835{[0]: ValueNode<Primitive> Return, [1]: CNode_834}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_436 : 0000029BC72984E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_436 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para603_x) {
  %1(CNode_836) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para603_x, %para19_layer1.1.bn1.gamma, %para20_layer1.1.bn1.beta, %para422_layer1.1.bn1.moving_mean, %para423_layer1.1.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_836{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.1.bn1.gamma, [3]: param_layer1.1.bn1.beta, [4]: param_layer1.1.bn1.moving_mean, [5]: param_layer1.1.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_837{[0]: ValueNode<Primitive> Return, [1]: CNode_836}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_435 : 0000029BC7294520
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_435 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para604_x) {
  %1(CNode_839) = call @mindspore_nn_layer_conv_Conv2d_construct_838()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_435:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.1.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_435:CNode_839{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_838}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_435:CNode_840{[0]: ValueNode<Primitive> Return, [1]: CNode_839}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_433 : 0000029BC72974F0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_433 parent: [subgraph @__main___ResidualBlock_construct_432]() {
  %1(CNode_842) = call @__main___ResidualBlock_construct_841()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_433:CNode_842{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_841}
#   2: @__main___ResidualBlock_construct_433:CNode_843{[0]: ValueNode<Primitive> Return, [1]: CNode_842}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_452 : 0000029BC728FAC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_452 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para605_x) {
  %1(CNode_844) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802(%para605_x, %para34_layer1.2.bn3.gamma, %para35_layer1.2.bn3.beta, %para366_layer1.2.bn3.moving_mean, %para367_layer1.2.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_452:CNode_844{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802, [1]: param_x, [2]: param_layer1.2.bn3.gamma, [3]: param_layer1.2.bn3.beta, [4]: param_layer1.2.bn3.moving_mean, [5]: param_layer1.2.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_452:CNode_845{[0]: ValueNode<Primitive> Return, [1]: CNode_844}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_451 : 0000029BC7289080
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_451 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para606_x) {
  %1(CNode_847) = call @mindspore_nn_layer_conv_Conv2d_construct_846()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_451:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.2.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_451:CNode_847{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_846}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_451:CNode_848{[0]: ValueNode<Primitive> Return, [1]: CNode_847}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_448 : 0000029BC728C050
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_448(%para607_x) {
  %1(CNode_849) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para607_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/relu-ReLU)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_448:CNode_849{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_448:CNode_850{[0]: ValueNode<Primitive> Return, [1]: CNode_849}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_450 : 0000029BC728A070
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_450 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para608_x) {
  %1(CNode_851) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para608_x, %para31_layer1.2.bn2.gamma, %para32_layer1.2.bn2.beta, %para398_layer1.2.bn2.moving_mean, %para399_layer1.2.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_450:CNode_851{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.2.bn2.gamma, [3]: param_layer1.2.bn2.beta, [4]: param_layer1.2.bn2.moving_mean, [5]: param_layer1.2.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_450:CNode_852{[0]: ValueNode<Primitive> Return, [1]: CNode_851}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_449 : 0000029BC728BB00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_449 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para609_x) {
  %1(CNode_854) = call @mindspore_nn_layer_conv_Conv2d_construct_853()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_449:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.2.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_449:CNode_854{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_853}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_449:CNode_855{[0]: ValueNode<Primitive> Return, [1]: CNode_854}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_447 : 0000029BC7288B30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_447 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para610_x) {
  %1(CNode_856) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192(%para610_x, %para28_layer1.2.bn1.gamma, %para29_layer1.2.bn1.beta, %para424_layer1.2.bn1.moving_mean, %para425_layer1.2.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.beta>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_447:CNode_856{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192, [1]: param_x, [2]: param_layer1.2.bn1.gamma, [3]: param_layer1.2.bn1.beta, [4]: param_layer1.2.bn1.moving_mean, [5]: param_layer1.2.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_447:CNode_857{[0]: ValueNode<Primitive> Return, [1]: CNode_856}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_446 : 0000029BC728E030
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_446 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para611_x) {
  %1(CNode_859) = call @mindspore_nn_layer_conv_Conv2d_construct_858()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_446:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.2.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_446:CNode_859{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_858}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_446:CNode_860{[0]: ValueNode<Primitive> Return, [1]: CNode_859}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_444 : 0000029BC728AB10
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_444 parent: [subgraph @__main___ResidualBlock_construct_443]() {
  %1(CNode_862) = call @__main___ResidualBlock_construct_861()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:103/
}
# Order:
#   1: @__main___ResidualBlock_construct_444:CNode_862{[0]: ValueNode<FuncGraph> __main___ResidualBlock_construct_861}
#   2: @__main___ResidualBlock_construct_444:CNode_863{[0]: ValueNode<Primitive> Return, [1]: CNode_862}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_457 : 0000029BC74E29C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_457 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_179]() {
  %1(CNode_455) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para492_@CNode_455, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_864) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_182) = $(mindspore_nn_layer_container_SequentialCell_construct_40):MakeTuple(@__main___ResidualBlock_construct_421, @__main___ResidualBlock_construct_432, @__main___ResidualBlock_construct_443)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_865) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para492_@CNode_455)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para493_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_866) = call @mindspore_nn_layer_container_SequentialCell_construct_179(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_867) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_457:CNode_865{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_182}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_457:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_865, [2]: param_@CNode_455}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_457:CNode_455{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_455, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_457:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_457:CNode_866{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_179, [1]: CNode_455, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_457:CNode_868{[0]: ValueNode<Primitive> Return, [1]: CNode_867}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_458 : 0000029BC74E39B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_458 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_179]() {
  Return(%para493_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_458:CNode_869{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_462 : 0000029BC72C6CA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_462(%para612_, %para613_) {
  %1(CNode_871) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_870()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_871{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_870}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_462:CNode_872{[0]: ValueNode<Primitive> Return, [1]: CNode_871}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473 : 0000029BC72C71F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_874) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473:CNode_874{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_473:CNode_875{[0]: ValueNode<Primitive> Return, [1]: CNode_874}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474 : 0000029BC72C3CD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_877) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474:CNode_877{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_474:CNode_878{[0]: ValueNode<Primitive> Return, [1]: CNode_877}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_weight_decay_477 : 0000029BC72BF7C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:720/    def get_weight_decay(self):/
subgraph @get_weight_decay_477() {
  Return(Tensor(shape=[], dtype=Float32, value=0.1024))
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:737/        return self.weight_decay/
}
# Order:
#   1: @get_weight_decay_477:CNode_879{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.1024)}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_480 : 0000029BC72B9820
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_480 parent: [subgraph @decay_weight_106]() {
  %1(weight_decay) = $(decay_weight_76):call @get_weight_decay_108()
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:444/            weight_decay = self.get_weight_decay()/
  %2(CNode_202) = $(decay_weight_106):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_apply_decay, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:448/                gradients = self.map_(F.partial(_apply_decay, weight_decay), self.decay_flags, params, gradients)/
  %3(CNode_203) = $(decay_weight_59):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_layer1.0.conv1.weight, %para7_layer1.0.bn1.gamma, %para8_layer1.0.bn1.beta, %para9_layer1.0.conv2.weight, %para10_layer1.0.bn2.gamma, %para11_layer1.0.bn2.beta, %para12_layer1.0.conv3.weight, %para13_layer1.0.bn3.gamma, %para14_layer1.0.bn3.beta, %para15_layer1.0.down_sample_layer.0.weight, %para16_layer1.0.down_sample_layer.1.gamma, %para17_layer1.0.down_sample_layer.1.beta, %para18_layer1.1.conv1.weight, %para19_layer1.1.bn1.gamma, %para20_layer1.1.bn1.beta, %para21_layer1.1.conv2.weight, %para22_layer1.1.bn2.gamma, %para23_layer1.1.bn2.beta, %para24_layer1.1.conv3.weight, %para25_layer1.1.bn3.gamma, %para26_layer1.1.bn3.beta, %para27_layer1.2.conv1.weight, %para28_layer1.2.bn1.gamma, %para29_layer1.2.bn1.beta, %para30_layer1.2.conv2.weight, %para31_layer1.2.bn2.gamma, %para32_layer1.2.bn2.beta, %para33_layer1.2.conv3.weight, %para34_layer1.2.bn3.gamma, %para35_layer1.2.bn3.beta, %para36_layer2.0.conv1.weight, %para37_layer2.0.bn1.gamma, %para38_layer2.0.bn1.beta, %para39_layer2.0.conv2.weight, %para40_layer2.0.bn2.gamma, %para41_layer2.0.bn2.beta, %para42_layer2.0.conv3.weight, %para43_layer2.0.bn3.gamma, %para44_layer2.0.bn3.beta, %para45_layer2.0.down_sample_layer.0.weight, %para46_layer2.0.down_sample_layer.1.gamma, %para47_layer2.0.down_sample_layer.1.beta, %para48_layer2.1.conv1.weight, %para49_layer2.1.bn1.gamma, %para50_layer2.1.bn1.beta, %para51_layer2.1.conv2.weight, %para52_layer2.1.bn2.gamma, %para53_layer2.1.bn2.beta, %para54_layer2.1.conv3.weight, %para55_layer2.1.bn3.gamma, %para56_layer2.1.bn3.beta, %para57_layer2.2.conv1.weight, %para58_layer2.2.bn1.gamma, %para59_layer2.2.bn1.beta, %para60_layer2.2.conv2.weight, %para61_layer2.2.bn2.gamma, %para62_layer2.2.bn2.beta, %para63_layer2.2.conv3.weight, %para64_layer2.2.bn3.gamma, %para65_layer2.2.bn3.beta, %para66_layer2.3.conv1.weight, %para67_layer2.3.bn1.gamma, %para68_layer2.3.bn1.beta, %para69_layer2.3.conv2.weight, %para70_layer2.3.bn2.gamma, %para71_layer2.3.bn2.beta, %para72_layer2.3.conv3.weight, %para73_layer2.3.bn3.gamma, %para74_layer2.3.bn3.beta, %para75_layer3.0.conv1.weight, %para76_layer3.0.bn1.gamma, %para77_layer3.0.bn1.beta, %para78_layer3.0.conv2.weight, %para79_layer3.0.bn2.gamma, %para80_layer3.0.bn2.beta, %para81_layer3.0.conv3.weight, %para82_layer3.0.bn3.gamma, %para83_layer3.0.bn3.beta, %para84_layer3.0.down_sample_layer.0.weight, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para87_layer3.1.conv1.weight, %para88_layer3.1.bn1.gamma, %para89_layer3.1.bn1.beta, %para90_layer3.1.conv2.weight, %para91_layer3.1.bn2.gamma, %para92_layer3.1.bn2.beta, %para93_layer3.1.conv3.weight, %para94_layer3.1.bn3.gamma, %para95_layer3.1.bn3.beta, %para96_layer3.2.conv1.weight, %para97_layer3.2.bn1.gamma, %para98_layer3.2.bn1.beta, %para99_layer3.2.conv2.weight, %para100_layer3.2.bn2.gamma, %para101_layer3.2.bn2.beta, %para102_layer3.2.conv3.weight, %para103_layer3.2.bn3.gamma, %para104_layer3.2.bn3.beta, %para105_layer3.3.conv1.weight, %para106_layer3.3.bn1.gamma, %para107_layer3.3.bn1.beta, %para108_layer3.3.conv2.weight, %para109_layer3.3.bn2.gamma, %para110_layer3.3.bn2.beta, %para111_layer3.3.conv3.weight, %para112_layer3.3.bn3.gamma, %para113_layer3.3.bn3.beta, %para114_layer3.4.conv1.weight, %para115_layer3.4.bn1.gamma, %para116_layer3.4.bn1.beta, %para117_layer3.4.conv2.weight, %para118_layer3.4.bn2.gamma, %para119_layer3.4.bn2.beta, %para120_layer3.4.conv3.weight, %para121_layer3.4.bn3.gamma, %para122_layer3.4.bn3.beta, %para123_layer3.5.conv1.weight, %para124_layer3.5.bn1.gamma, %para125_layer3.5.bn1.beta, %para126_layer3.5.conv2.weight, %para127_layer3.5.bn2.gamma, %para128_layer3.5.bn2.beta, %para129_layer3.5.conv3.weight, %para130_layer3.5.bn3.gamma, %para131_layer3.5.bn3.beta, %para132_layer4.0.conv1.weight, %para133_layer4.0.bn1.gamma, %para134_layer4.0.bn1.beta, %para135_layer4.0.conv2.weight, %para136_layer4.0.bn2.gamma, %para137_layer4.0.bn2.beta, %para138_layer4.0.conv3.weight, %para139_layer4.0.bn3.gamma, %para140_layer4.0.bn3.beta, %para141_layer4.0.down_sample_layer.0.weight, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para144_layer4.1.conv1.weight, %para145_layer4.1.bn1.gamma, %para146_layer4.1.bn1.beta, %para147_layer4.1.conv2.weight, %para148_layer4.1.bn2.gamma, %para149_layer4.1.bn2.beta, %para150_layer4.1.conv3.weight, %para151_layer4.1.bn3.gamma, %para152_layer4.1.bn3.beta, %para153_layer4.2.conv1.weight, %para154_layer4.2.bn1.gamma, %para155_layer4.2.bn1.beta, %para156_layer4.2.conv2.weight, %para157_layer4.2.bn2.gamma, %para158_layer4.2.bn2.beta, %para159_layer4.2.conv3.weight, %para160_layer4.2.bn3.gamma, %para161_layer4.2.bn3.beta, %para162_end_point.weight, %para163_end_point.bias)
      : (<Ref[Tensor[Float32]], (64, 3, 7, 7), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:layer1.0.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.0.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.0.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.bn3.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.1.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.1.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.1.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.1.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.1.bn3.beta>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.2.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.2.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:layer1.2.bn2.beta>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.2.conv3.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:layer2.0.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.0.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.0.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.0.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:layer2.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.1.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.1.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.1.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.1.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.1.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.2.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.2.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.2.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.2.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.2.bn3.beta>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.3.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.3.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:layer2.3.bn2.beta>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.3.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:layer3.0.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.0.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.0.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.0.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.bn3.beta>, <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:layer3.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.1.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.1.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.1.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.1.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.1.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.2.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.2.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.2.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.2.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.2.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.3.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.3.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.3.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.3.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.3.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.4.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.4.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.4.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.4.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.4.bn3.beta>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.5.conv1.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn1.beta>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.5.conv2.weight>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer3.5.bn2.beta>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.5.conv3.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.5.bn3.beta>, <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:layer4.0.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.0.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.0.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.0.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.bn3.beta>, <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:layer4.0.down_sample_layer.0.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.1.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.1.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.1.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.1.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.1.bn3.beta>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.2.conv1.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn1.beta>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.2.conv2.weight>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer4.2.bn2.beta>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.2.conv3.weight>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.2.bn3.beta>, <Ref[Tensor[Float32]], (4, 2048), ref_key=:end_point.weight>, <Ref[Tensor[Float32]], (4), ref_key=:end_point.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:443/            params = self._parameters/
  %4(gradients) = $(decay_weight_106):S_Prim_map(%2, (Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(0), Bool(0), Bool(1), Bool(1)), %3, %para447_gradients)
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:448/                gradients = self.map_(F.partial(_apply_decay, weight_decay), self.decay_flags, params, gradients)/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:450/        return gradients/
}
# Order:
#   1: @decay_weight_480:CNode_880{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_485 : 0000029BC72BED20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_485 parent: [subgraph @get_lr_113]() {
  %1(CNode_208) = $(get_lr_113):call @mindspore_nn_optim_optimizer__IteratorLearningRate_construct_209(%para164_global_step)
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:755/                lr = self.learning_rate(self.global_step).reshape(())/
  %2(CNode_210) = $(get_lr_113):getattr(%1, "reshape")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:755/                lr = self.learning_rate(self.global_step).reshape(())/
  %3(lr) = $(get_lr_113):%2(())
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:755/                lr = self.learning_rate(self.global_step).reshape(())/
  Return(%3)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:756/        return lr/
}
# Order:
#   1: @get_lr_485:CNode_881{[0]: ValueNode<Primitive> Return, [1]: lr}


subgraph attr:
subgraph instance: shape_492 : 0000029BC75949E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1484/def shape(input_x):/
subgraph @shape_492(%para614_input_x) {
  %1(CNode_882) = S_Prim_Shape(%para614_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_492:CNode_882{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_492:CNode_883{[0]: ValueNode<Primitive> Return, [1]: CNode_882}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489 : 0000029BC779EB20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219]() {
  %1(CNode_491) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219):call @shape_492(%para450_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_493) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_494) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_219):S_Prim_OneHot[axis: I64(-1), input_names: ["indices", "depth", "on_value", "off_value"], output_names: ["output"]](%para451_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_884) = S_Prim_SoftmaxCrossEntropyWithLogits(%para450_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_886) = call @get_loss_885(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489:CNode_884{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_884, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489:CNode_886{[0]: ValueNode<FuncGraph> get_loss_885, [1]: x}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_489:CNode_887{[0]: ValueNode<Primitive> Return, [1]: CNode_886}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_496 : 0000029BC7594490
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_496 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_222]() {
  %1(CNode_889) = call @mindspore_nn_layer_basic_Dense_construct_888()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_496:CNode_889{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_888}
#   2: @mindspore_nn_layer_basic_Dense_construct_496:CNode_890{[0]: ValueNode<Primitive> Return, [1]: CNode_889}


subgraph attr:
training : 1
after_block : 1
subgraph instance: check_axis_valid_501 : 0000029BC75C0720
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_501() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @check_axis_valid_501:CNode_891{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_510 : 0000029BC7586020
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_510 parent: [subgraph @flatten_160]() {
  %1(CNode_892) = S_Prim_isinstance(%para468_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_893) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_894) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_895) = Switch(%3, @flatten_896, @flatten_897)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_898) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_899) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_900) = Switch(%6, @flatten_901, @flatten_902)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_903) = %7()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_510:CNode_892{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_510:CNode_893{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_892}
#   3: @flatten_510:CNode_894{[0]: ValueNode<Primitive> Cond, [1]: CNode_893, [2]: ValueNode<BoolImm> false}
#   4: @flatten_510:CNode_895{[0]: ValueNode<Primitive> Switch, [1]: CNode_894, [2]: ValueNode<FuncGraph> flatten_896, [3]: ValueNode<FuncGraph> flatten_897}
#   5: @flatten_510:CNode_898{[0]: CNode_895}
#   6: @flatten_510:CNode_899{[0]: ValueNode<Primitive> Cond, [1]: CNode_898, [2]: ValueNode<BoolImm> false}
#   7: @flatten_510:CNode_900{[0]: ValueNode<Primitive> Switch, [1]: CNode_899, [2]: ValueNode<FuncGraph> flatten_901, [3]: ValueNode<FuncGraph> flatten_902}
#   8: @flatten_510:CNode_903{[0]: CNode_900}
#   9: @flatten_510:CNode_904{[0]: ValueNode<Primitive> Return, [1]: CNode_903}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513 : 0000029BC75A9DE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513(%para615_x, %para616_, %para617_, %para618_, %para619_) {
  %1(CNode_905) = S_Prim_Shape(%para615_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_906) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_907) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_908) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_909) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_910) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_913) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_914) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_905{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_906{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_905, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_908{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_909{[0]: ValueNode<Primitive> Cond, [1]: CNode_908, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_910{[0]: ValueNode<Primitive> Switch, [1]: CNode_909, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_913{[0]: CNode_910}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513:CNode_515{[0]: ValueNode<Primitive> Return, [1]: CNode_914}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_516 : 0000029BC75B2D50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_516 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_264]() {
  %1(CNode_916) = call @mindspore_nn_layer_conv_Conv2d_construct_915()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_516:CNode_916{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_915}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_516:CNode_917{[0]: ValueNode<Primitive> Return, [1]: CNode_916}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521 : 0000029BC7507220
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para620_x, %para621_, %para622_, %para623_, %para624_) {
  %1(CNode_918) = S_Prim_Shape(%para620_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_919) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_920) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_921) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_922) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_923) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_926) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_927) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_918{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_919{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_918, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_921{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_922{[0]: ValueNode<Primitive> Cond, [1]: CNode_921, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_923{[0]: ValueNode<Primitive> Switch, [1]: CNode_922, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_926{[0]: CNode_923}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521:CNode_523{[0]: ValueNode<Primitive> Return, [1]: CNode_927}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_524 : 0000029BC75B6D10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_524 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_262]() {
  %1(CNode_929) = call @mindspore_nn_layer_conv_Conv2d_construct_928()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_524:CNode_929{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_928}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_524:CNode_930{[0]: ValueNode<Primitive> Return, [1]: CNode_929}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_529 : 0000029BC75B77B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_529 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_259]() {
  %1(CNode_932) = call @mindspore_nn_layer_conv_Conv2d_construct_931()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_529:CNode_932{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_931}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_529:CNode_933{[0]: ValueNode<Primitive> Return, [1]: CNode_932}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_534 : 0000029BC75B87A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_534 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para625_input_data) {
  %1(CNode_935) = call @mindspore_nn_layer_container_SequentialCell_construct_934(I64(0), %para625_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_534:CNode_936{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_937}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_534:CNode_935{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_934, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_534:CNode_938{[0]: ValueNode<Primitive> Return, [1]: CNode_935}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_532 : 0000029BC75BC210
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_532 parent: [subgraph @__main___ResidualBlock_construct_257]() {
  %1(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_conv_Conv2d_construct_259(%para470_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_260(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_activation_ReLU_construct_261(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_conv_Conv2d_construct_262(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_263(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_activation_ReLU_construct_261(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_conv_Conv2d_construct_264(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_256):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_265(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(identity) = $(__main___ResidualBlock_construct_257):call @mindspore_nn_layer_container_SequentialCell_construct_534(%para470_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  %10(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_261(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_532:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: identity}
#   2: @__main___ResidualBlock_construct_532:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_261, [1]: out}
#   3: @__main___ResidualBlock_construct_532:CNode_939{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_538 : 0000029BC75B1D60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_538 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_275]() {
  %1(CNode_941) = call @mindspore_nn_layer_conv_Conv2d_construct_940()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_538:CNode_941{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_940}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_538:CNode_942{[0]: ValueNode<Primitive> Return, [1]: CNode_941}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_545 : 0000029BC75B22B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_545 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_273]() {
  %1(CNode_944) = call @mindspore_nn_layer_conv_Conv2d_construct_943()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_545:CNode_944{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_943}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_545:CNode_945{[0]: ValueNode<Primitive> Return, [1]: CNode_944}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_550 : 0000029BC75B1810
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_550 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_270]() {
  %1(CNode_947) = call @mindspore_nn_layer_conv_Conv2d_construct_946()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_550:CNode_947{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_946}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_550:CNode_948{[0]: ValueNode<Primitive> Return, [1]: CNode_947}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_553 : 0000029BC75B2800
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_553 parent: [subgraph @__main___ResidualBlock_construct_267]() {
  %1(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_conv_Conv2d_construct_270(%para471_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_271(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_activation_ReLU_construct_272(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_conv_Conv2d_construct_273(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_274(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_activation_ReLU_construct_272(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_conv_Conv2d_construct_275(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_267):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_276(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para471_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_272(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_553:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_553:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_272, [1]: out}
#   3: @__main___ResidualBlock_construct_553:CNode_949{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_558 : 0000029BC75A8DF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_558 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_286]() {
  %1(CNode_951) = call @mindspore_nn_layer_conv_Conv2d_construct_950()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_558:CNode_951{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_950}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_558:CNode_952{[0]: ValueNode<Primitive> Return, [1]: CNode_951}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_565 : 0000029BC75A78B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_565 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_284]() {
  %1(CNode_954) = call @mindspore_nn_layer_conv_Conv2d_construct_953()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_565:CNode_954{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_953}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_565:CNode_955{[0]: ValueNode<Primitive> Return, [1]: CNode_954}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_570 : 0000029BC75A68C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_570 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_281]() {
  %1(CNode_957) = call @mindspore_nn_layer_conv_Conv2d_construct_956()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_570:CNode_957{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_956}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_570:CNode_958{[0]: ValueNode<Primitive> Return, [1]: CNode_957}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_573 : 0000029BC75AED90
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_573 parent: [subgraph @__main___ResidualBlock_construct_278]() {
  %1(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_conv_Conv2d_construct_281(%para472_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_282(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_activation_ReLU_construct_283(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_conv_Conv2d_construct_284(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_285(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_activation_ReLU_construct_283(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_conv_Conv2d_construct_286(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_278):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_287(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para472_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_283(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_573:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_573:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_283, [1]: out}
#   3: @__main___ResidualBlock_construct_573:CNode_959{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
subgraph instance: ms_iter_577 : 0000029BC74E7EC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\_extends\parse\standard_method.py:2345/def ms_iter(xs):/
subgraph @ms_iter_577(%para626_xs) {
  %1(CNode_960) = getattr(%para626_xs, "__ms_iter__")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
}
# Order:
#   1: @ms_iter_577:CNode_960{[0]: ValueNode<Primitive> getattr, [1]: param_xs, [2]: ValueNode<StringImm> __ms_iter__}
#   2: @ms_iter_577:CNode_961{[0]: ValueNode<Primitive> Return, [1]: CNode_960}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584 : 0000029BC74CDB10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584(%para627_x, %para628_, %para629_, %para630_, %para631_) {
  %1(CNode_962) = S_Prim_Shape(%para627_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_963) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_964) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_965) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_966) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_967) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_970) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_971) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_962{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_963{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_962, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_965{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_966{[0]: ValueNode<Primitive> Cond, [1]: CNode_965, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_967{[0]: ValueNode<Primitive> Switch, [1]: CNode_966, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_970{[0]: CNode_967}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584:CNode_586{[0]: ValueNode<Primitive> Return, [1]: CNode_971}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_587 : 0000029BC759E940
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_587 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_305]() {
  %1(CNode_973) = call @mindspore_nn_layer_conv_Conv2d_construct_972()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_587:CNode_973{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_972}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_587:CNode_974{[0]: ValueNode<Primitive> Return, [1]: CNode_973}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592 : 0000029BC74E4EF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para632_x, %para633_, %para634_, %para635_, %para636_) {
  %1(CNode_975) = S_Prim_Shape(%para632_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_976) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_977) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_978) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_979) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_980) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_983) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_984) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_975{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_976{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_975, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_978{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_979{[0]: ValueNode<Primitive> Cond, [1]: CNode_978, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_980{[0]: ValueNode<Primitive> Switch, [1]: CNode_979, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_983{[0]: CNode_980}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592:CNode_594{[0]: ValueNode<Primitive> Return, [1]: CNode_984}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_595 : 0000029BC75969C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_595 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_303]() {
  %1(CNode_986) = call @mindspore_nn_layer_conv_Conv2d_construct_985()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_595:CNode_986{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_985}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_595:CNode_987{[0]: ValueNode<Primitive> Return, [1]: CNode_986}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_600 : 0000029BC7598450
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_600 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_300]() {
  %1(CNode_989) = call @mindspore_nn_layer_conv_Conv2d_construct_988()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_600:CNode_989{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_988}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_600:CNode_990{[0]: ValueNode<Primitive> Return, [1]: CNode_989}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_605 : 0000029BC75A13C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_605 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para637_input_data) {
  %1(CNode_992) = call @mindspore_nn_layer_container_SequentialCell_construct_991(I64(0), %para637_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_605:CNode_993{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_994}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_605:CNode_992{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_991, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_605:CNode_995{[0]: ValueNode<Primitive> Return, [1]: CNode_992}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_603 : 0000029BC75A38F0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_603 parent: [subgraph @__main___ResidualBlock_construct_298]() {
  %1(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_conv_Conv2d_construct_300(%para475_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_301(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_activation_ReLU_construct_302(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_conv_Conv2d_construct_303(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_304(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_activation_ReLU_construct_302(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_conv_Conv2d_construct_305(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_297):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_306(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(identity) = $(__main___ResidualBlock_construct_298):call @mindspore_nn_layer_container_SequentialCell_construct_605(%para475_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  %10(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_302(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_603:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: identity}
#   2: @__main___ResidualBlock_construct_603:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_302, [1]: out}
#   3: @__main___ResidualBlock_construct_603:CNode_996{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_609 : 0000029BC7597F00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_609 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_316]() {
  %1(CNode_998) = call @mindspore_nn_layer_conv_Conv2d_construct_997()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_609:CNode_998{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_997}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_609:CNode_999{[0]: ValueNode<Primitive> Return, [1]: CNode_998}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_616 : 0000029BC759AED0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_616 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_314]() {
  %1(CNode_1001) = call @mindspore_nn_layer_conv_Conv2d_construct_1000()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_616:CNode_1001{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1000}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_616:CNode_1002{[0]: ValueNode<Primitive> Return, [1]: CNode_1001}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_621 : 0000029BC759A430
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_621 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_311]() {
  %1(CNode_1004) = call @mindspore_nn_layer_conv_Conv2d_construct_1003()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_621:CNode_1004{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1003}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_621:CNode_1005{[0]: ValueNode<Primitive> Return, [1]: CNode_1004}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_624 : 0000029BC7599990
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_624 parent: [subgraph @__main___ResidualBlock_construct_308]() {
  %1(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_conv_Conv2d_construct_311(%para476_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_312(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_activation_ReLU_construct_313(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_conv_Conv2d_construct_314(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_315(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_activation_ReLU_construct_313(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_conv_Conv2d_construct_316(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_308):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_317(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para476_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_313(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_624:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_624:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_313, [1]: out}
#   3: @__main___ResidualBlock_construct_624:CNode_1006{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_629 : 0000029BC74DF4A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_629 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_327]() {
  %1(CNode_1008) = call @mindspore_nn_layer_conv_Conv2d_construct_1007()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_629:CNode_1008{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1007}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_629:CNode_1009{[0]: ValueNode<Primitive> Return, [1]: CNode_1008}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_636 : 0000029BC74DE4B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_636 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_325]() {
  %1(CNode_1011) = call @mindspore_nn_layer_conv_Conv2d_construct_1010()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_636:CNode_1011{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1010}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_636:CNode_1012{[0]: ValueNode<Primitive> Return, [1]: CNode_1011}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_641 : 0000029BC74D6FD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_641 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_322]() {
  %1(CNode_1014) = call @mindspore_nn_layer_conv_Conv2d_construct_1013()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_641:CNode_1014{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1013}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_641:CNode_1015{[0]: ValueNode<Primitive> Return, [1]: CNode_1014}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_644 : 0000029BC74E0490
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_644 parent: [subgraph @__main___ResidualBlock_construct_319]() {
  %1(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_conv_Conv2d_construct_322(%para477_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_323(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_activation_ReLU_construct_324(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_conv_Conv2d_construct_325(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_326(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_activation_ReLU_construct_324(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_conv_Conv2d_construct_327(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_319):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_328(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para477_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_324(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_644:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_644:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_324, [1]: out}
#   3: @__main___ResidualBlock_construct_644:CNode_1016{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_649 : 0000029BC74D8510
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_649 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_338]() {
  %1(CNode_1018) = call @mindspore_nn_layer_conv_Conv2d_construct_1017()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_649:CNode_1018{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1017}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_649:CNode_1019{[0]: ValueNode<Primitive> Return, [1]: CNode_1018}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_656 : 0000029BC74DDF60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_656 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_336]() {
  %1(CNode_1021) = call @mindspore_nn_layer_conv_Conv2d_construct_1020()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_656:CNode_1021{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1020}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_656:CNode_1022{[0]: ValueNode<Primitive> Return, [1]: CNode_1021}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_661 : 0000029BC74DAF90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_661 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_333]() {
  %1(CNode_1024) = call @mindspore_nn_layer_conv_Conv2d_construct_1023()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_661:CNode_1024{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1023}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_661:CNode_1025{[0]: ValueNode<Primitive> Return, [1]: CNode_1024}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_664 : 0000029BC74DA4F0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_664 parent: [subgraph @__main___ResidualBlock_construct_330]() {
  %1(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_conv_Conv2d_construct_333(%para478_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_334(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_activation_ReLU_construct_335(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_conv_Conv2d_construct_336(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_337(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_activation_ReLU_construct_335(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_conv_Conv2d_construct_338(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_330):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_339(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para478_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_335(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_664:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_664:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_335, [1]: out}
#   3: @__main___ResidualBlock_construct_664:CNode_1026{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_669 : 0000029BC74D1AD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_669 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_349]() {
  %1(CNode_1028) = call @mindspore_nn_layer_conv_Conv2d_construct_1027()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_669:CNode_1028{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1027}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_669:CNode_1029{[0]: ValueNode<Primitive> Return, [1]: CNode_1028}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_676 : 0000029BC74D4550
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_676 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_347]() {
  %1(CNode_1031) = call @mindspore_nn_layer_conv_Conv2d_construct_1030()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_676:CNode_1031{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1030}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_676:CNode_1032{[0]: ValueNode<Primitive> Return, [1]: CNode_1031}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_681 : 0000029BC74D1580
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_681 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_344]() {
  %1(CNode_1034) = call @mindspore_nn_layer_conv_Conv2d_construct_1033()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_681:CNode_1034{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1033}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_681:CNode_1035{[0]: ValueNode<Primitive> Return, [1]: CNode_1034}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_684 : 0000029BC74D5FE0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_684 parent: [subgraph @__main___ResidualBlock_construct_341]() {
  %1(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_conv_Conv2d_construct_344(%para479_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_345(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_activation_ReLU_construct_346(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_conv_Conv2d_construct_347(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_348(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_activation_ReLU_construct_346(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_conv_Conv2d_construct_349(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_341):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_350(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para479_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_346(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_684:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_684:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_346, [1]: out}
#   3: @__main___ResidualBlock_construct_684:CNode_1036{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_689 : 0000029BC74CEB00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_689 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_360]() {
  %1(CNode_1038) = call @mindspore_nn_layer_conv_Conv2d_construct_1037()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_689:CNode_1038{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1037}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_689:CNode_1039{[0]: ValueNode<Primitive> Return, [1]: CNode_1038}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_696 : 0000029BC750B1E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_696 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_358]() {
  %1(CNode_1041) = call @mindspore_nn_layer_conv_Conv2d_construct_1040()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_696:CNode_1041{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1040}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_696:CNode_1042{[0]: ValueNode<Primitive> Return, [1]: CNode_1041}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_701 : 0000029BC7509750
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_701 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_355]() {
  %1(CNode_1044) = call @mindspore_nn_layer_conv_Conv2d_construct_1043()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_701:CNode_1044{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1043}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_701:CNode_1045{[0]: ValueNode<Primitive> Return, [1]: CNode_1044}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_704 : 0000029BC74D3AB0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_704 parent: [subgraph @__main___ResidualBlock_construct_352]() {
  %1(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_conv_Conv2d_construct_355(%para480_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_356(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_activation_ReLU_construct_357(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_conv_Conv2d_construct_358(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_359(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_activation_ReLU_construct_357(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_conv_Conv2d_construct_360(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_352):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_361(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para480_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_357(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_704:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_704:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_357, [1]: out}
#   3: @__main___ResidualBlock_construct_704:CNode_1046{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713 : 0000029BC74EA3F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713(%para638_x, %para639_, %para640_, %para641_, %para642_) {
  %1(CNode_1047) = S_Prim_Shape(%para638_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1048) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1049) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1050) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1051) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1052) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1055) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1056) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1047{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1048{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1047, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1050{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1051{[0]: ValueNode<Primitive> Cond, [1]: CNode_1050, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1052{[0]: ValueNode<Primitive> Switch, [1]: CNode_1051, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_1055{[0]: CNode_1052}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713:CNode_715{[0]: ValueNode<Primitive> Return, [1]: CNode_1056}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_716 : 0000029BC7502270
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_716 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_378]() {
  %1(CNode_1058) = call @mindspore_nn_layer_conv_Conv2d_construct_1057()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_716:CNode_1058{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1057}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_716:CNode_1059{[0]: ValueNode<Primitive> Return, [1]: CNode_1058}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721 : 0000029BC74E2470
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721(%para643_x, %para644_, %para645_, %para646_, %para647_) {
  %1(CNode_1060) = S_Prim_Shape(%para643_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1061) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1062) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1063) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1064) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1065) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1068) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1069) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1060{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1061{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1060, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1063{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1064{[0]: ValueNode<Primitive> Cond, [1]: CNode_1063, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1065{[0]: ValueNode<Primitive> Switch, [1]: CNode_1064, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_1068{[0]: CNode_1065}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721:CNode_723{[0]: ValueNode<Primitive> Return, [1]: CNode_1069}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_724 : 0000029BC74FF7F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_724 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_376]() {
  %1(CNode_1071) = call @mindspore_nn_layer_conv_Conv2d_construct_1070()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_724:CNode_1071{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1070}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_724:CNode_1072{[0]: ValueNode<Primitive> Return, [1]: CNode_1071}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_729 : 0000029BC74FED50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_729 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_373]() {
  %1(CNode_1074) = call @mindspore_nn_layer_conv_Conv2d_construct_1073()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_729:CNode_1074{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1073}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_729:CNode_1075{[0]: ValueNode<Primitive> Return, [1]: CNode_1074}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_734 : 0000029BC7504CF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_734 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para648_input_data) {
  %1(CNode_1077) = call @mindspore_nn_layer_container_SequentialCell_construct_1076(I64(0), %para648_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_734:CNode_1078{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1079}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_734:CNode_1077{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1076, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_734:CNode_1080{[0]: ValueNode<Primitive> Return, [1]: CNode_1077}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_732 : 0000029BC7508210
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_732 parent: [subgraph @__main___ResidualBlock_construct_371]() {
  %1(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_conv_Conv2d_construct_373(%para483_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_374(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_activation_ReLU_construct_375(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_conv_Conv2d_construct_376(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_377(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_activation_ReLU_construct_375(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_conv_Conv2d_construct_378(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_370):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_379(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(identity) = $(__main___ResidualBlock_construct_371):call @mindspore_nn_layer_container_SequentialCell_construct_734(%para483_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  %10(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_375(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_732:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: identity}
#   2: @__main___ResidualBlock_construct_732:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_375, [1]: out}
#   3: @__main___ResidualBlock_construct_732:CNode_1081{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_738 : 0000029BC74F8DB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_738 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_389]() {
  %1(CNode_1083) = call @mindspore_nn_layer_conv_Conv2d_construct_1082()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_738:CNode_1083{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1082}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_738:CNode_1084{[0]: ValueNode<Primitive> Return, [1]: CNode_1083}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_745 : 0000029BC74FA840
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_745 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_387]() {
  %1(CNode_1086) = call @mindspore_nn_layer_conv_Conv2d_construct_1085()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_745:CNode_1086{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1085}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_745:CNode_1087{[0]: ValueNode<Primitive> Return, [1]: CNode_1086}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_750 : 0000029BC74F7DC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_750 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_384]() {
  %1(CNode_1089) = call @mindspore_nn_layer_conv_Conv2d_construct_1088()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_750:CNode_1089{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1088}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_750:CNode_1090{[0]: ValueNode<Primitive> Return, [1]: CNode_1089}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_753 : 0000029BC74F6DD0
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_753 parent: [subgraph @__main___ResidualBlock_construct_381]() {
  %1(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_conv_Conv2d_construct_384(%para484_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_385(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_activation_ReLU_construct_386(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_conv_Conv2d_construct_387(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_388(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_activation_ReLU_construct_386(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_conv_Conv2d_construct_389(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_381):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_390(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para484_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_386(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_753:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_753:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_386, [1]: out}
#   3: @__main___ResidualBlock_construct_753:CNode_1091{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_758 : 0000029BC74FD2C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_758 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_400]() {
  %1(CNode_1093) = call @mindspore_nn_layer_conv_Conv2d_construct_1092()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_758:CNode_1093{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1092}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_758:CNode_1094{[0]: ValueNode<Primitive> Return, [1]: CNode_1093}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_765 : 0000029BC74F48A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_765 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_398]() {
  %1(CNode_1096) = call @mindspore_nn_layer_conv_Conv2d_construct_1095()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_765:CNode_1096{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1095}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_765:CNode_1097{[0]: ValueNode<Primitive> Return, [1]: CNode_1096}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_770 : 0000029BC74F3E00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_770 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_395]() {
  %1(CNode_1099) = call @mindspore_nn_layer_conv_Conv2d_construct_1098()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_770:CNode_1099{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1098}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_770:CNode_1100{[0]: ValueNode<Primitive> Return, [1]: CNode_1099}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_773 : 0000029BC74FBD80
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_773 parent: [subgraph @__main___ResidualBlock_construct_392]() {
  %1(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_conv_Conv2d_construct_395(%para485_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_396(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_activation_ReLU_construct_397(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_conv_Conv2d_construct_398(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_399(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_activation_ReLU_construct_397(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_conv_Conv2d_construct_400(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_392):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_401(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para485_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_397(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_773:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_773:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_397, [1]: out}
#   3: @__main___ResidualBlock_construct_773:CNode_1101{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_778 : 0000029BC74ECE70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_778 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_411]() {
  %1(CNode_1103) = call @mindspore_nn_layer_conv_Conv2d_construct_1102()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_778:CNode_1103{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1102}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_778:CNode_1104{[0]: ValueNode<Primitive> Return, [1]: CNode_1103}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_785 : 0000029BC74F0390
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_785 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_409]() {
  %1(CNode_1106) = call @mindspore_nn_layer_conv_Conv2d_construct_1105()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_785:CNode_1106{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1105}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_785:CNode_1107{[0]: ValueNode<Primitive> Return, [1]: CNode_1106}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_790 : 0000029BC74E9950
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_790 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_406]() {
  %1(CNode_1109) = call @mindspore_nn_layer_conv_Conv2d_construct_1108()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_790:CNode_1109{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1108}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_790:CNode_1110{[0]: ValueNode<Primitive> Return, [1]: CNode_1109}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_793 : 0000029BC74EB930
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_793 parent: [subgraph @__main___ResidualBlock_construct_403]() {
  %1(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_conv_Conv2d_construct_406(%para486_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_407(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_activation_ReLU_construct_408(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_conv_Conv2d_construct_409(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_410(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_activation_ReLU_construct_408(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_conv_Conv2d_construct_411(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_403):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_412(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para486_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_408(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_793:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_793:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_408, [1]: out}
#   3: @__main___ResidualBlock_construct_793:CNode_1111{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802 : 0000029BC7291000
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802(%para649_x, %para650_, %para651_, %para652_, %para653_) {
  %1(CNode_1112) = S_Prim_Shape(%para649_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1113) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1114) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1115) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1116) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1117) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1120) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1121) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1112{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1113{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1112, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1115{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1116{[0]: ValueNode<Primitive> Cond, [1]: CNode_1115, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1117{[0]: ValueNode<Primitive> Switch, [1]: CNode_1116, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_1120{[0]: CNode_1117}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802:CNode_804{[0]: ValueNode<Primitive> Return, [1]: CNode_1121}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_805 : 0000029BC7295510
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_805 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_429]() {
  %1(CNode_1123) = call @mindspore_nn_layer_conv_Conv2d_construct_1122()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_805:CNode_1123{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1122}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_805:CNode_1124{[0]: ValueNode<Primitive> Return, [1]: CNode_1123}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_812 : 0000029BC729BA00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_812 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_427]() {
  %1(CNode_1126) = call @mindspore_nn_layer_conv_Conv2d_construct_1125()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_812:CNode_1126{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1125}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_812:CNode_1127{[0]: ValueNode<Primitive> Return, [1]: CNode_1126}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_817 : 0000029BC729A4C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_817 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_424]() {
  %1(CNode_1129) = call @mindspore_nn_layer_conv_Conv2d_construct_1128()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_817:CNode_1129{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1128}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_817:CNode_1130{[0]: ValueNode<Primitive> Return, [1]: CNode_1129}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_822 : 0000029BC7295A60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_822 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para654_input_data) {
  %1(CNode_1132) = call @mindspore_nn_layer_container_SequentialCell_construct_1131(I64(0), %para654_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_822:CNode_1133{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1134}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_822:CNode_1132{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1131, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_822:CNode_1135{[0]: ValueNode<Primitive> Return, [1]: CNode_1132}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_820 : 0000029BC74E8410
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_820 parent: [subgraph @__main___ResidualBlock_construct_422]() {
  %1(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_conv_Conv2d_construct_424(%para489_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_425(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_activation_ReLU_construct_426(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_conv_Conv2d_construct_427(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_428(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_activation_ReLU_construct_426(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_conv_Conv2d_construct_429(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_421):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_430(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(identity) = $(__main___ResidualBlock_construct_422):call @mindspore_nn_layer_container_SequentialCell_construct_822(%para489_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:104/
  %10(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_426(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_820:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: identity}
#   2: @__main___ResidualBlock_construct_820:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_426, [1]: out}
#   3: @__main___ResidualBlock_construct_820:CNode_1136{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_826 : 0000029BC7296FA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_826 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_440]() {
  %1(CNode_1138) = call @mindspore_nn_layer_conv_Conv2d_construct_1137()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_826:CNode_1138{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1137}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_826:CNode_1139{[0]: ValueNode<Primitive> Return, [1]: CNode_1138}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_833 : 0000029BC7295FB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_833 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_438]() {
  %1(CNode_1141) = call @mindspore_nn_layer_conv_Conv2d_construct_1140()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_833:CNode_1141{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1140}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_833:CNode_1142{[0]: ValueNode<Primitive> Return, [1]: CNode_1141}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_838 : 0000029BC7298A30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_838 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_435]() {
  %1(CNode_1144) = call @mindspore_nn_layer_conv_Conv2d_construct_1143()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_838:CNode_1144{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1143}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_838:CNode_1145{[0]: ValueNode<Primitive> Return, [1]: CNode_1144}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_841 : 0000029BC7299A20
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_841 parent: [subgraph @__main___ResidualBlock_construct_432]() {
  %1(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_conv_Conv2d_construct_435(%para490_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_436(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_activation_ReLU_construct_437(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_conv_Conv2d_construct_438(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_439(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_activation_ReLU_construct_437(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_conv_Conv2d_construct_440(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_432):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_441(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para490_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_437(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_841:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_841:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_437, [1]: out}
#   3: @__main___ResidualBlock_construct_841:CNode_1146{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_846 : 0000029BC728EAD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_846 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_451]() {
  %1(CNode_1148) = call @mindspore_nn_layer_conv_Conv2d_construct_1147()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_846:CNode_1148{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1147}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_846:CNode_1149{[0]: ValueNode<Primitive> Return, [1]: CNode_1148}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_853 : 0000029BC728E580
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_853 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_449]() {
  %1(CNode_1151) = call @mindspore_nn_layer_conv_Conv2d_construct_1150()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_853:CNode_1151{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1150}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_853:CNode_1152{[0]: ValueNode<Primitive> Return, [1]: CNode_1151}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_858 : 0000029BC728B5B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_858 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_446]() {
  %1(CNode_1154) = call @mindspore_nn_layer_conv_Conv2d_construct_1153()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_858:CNode_1154{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1153}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_858:CNode_1155{[0]: ValueNode<Primitive> Return, [1]: CNode_1154}


subgraph attr:
training : 1
subgraph instance: __main___ResidualBlock_construct_861 : 0000029BC7292A90
# In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:88/
subgraph @__main___ResidualBlock_construct_861 parent: [subgraph @__main___ResidualBlock_construct_443]() {
  %1(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_conv_Conv2d_construct_446(%para491_identity)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:92/
  %2(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_447(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:93/
  %3(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_activation_ReLU_construct_448(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:94/
  %4(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_conv_Conv2d_construct_449(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:96/
  %5(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_450(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:97/
  %6(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_activation_ReLU_construct_448(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:98/
  %7(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_conv_Conv2d_construct_451(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:100/
  %8(out) = $(__main___ResidualBlock_construct_443):call @mindspore_nn_layer_normalization_BatchNorm2d_construct_452(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:101/
  %9(out) = S_Prim_Add[input_names: ["x", "y"], output_names: ["output"]](%8, %para491_identity)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:106/
  %10(out) = call @mindspore_nn_layer_activation_ReLU_construct_448(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:108/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:109/
}
# Order:
#   1: @__main___ResidualBlock_construct_861:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Add, [1]: out, [2]: param_identity}
#   2: @__main___ResidualBlock_construct_861:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_448, [1]: out}
#   3: @__main___ResidualBlock_construct_861:CNode_1156{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_870 : 0000029BC72C7740
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_462]() {
  %1(CNode_1158) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1157()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_870:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_870:CNode_1158{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1157}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_870:CNode_1159{[0]: ValueNode<Primitive> Return, [1]: CNode_1158}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873 : 0000029BC72C6200
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_1160) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para494_x, %para495_L_bn1.gamma, %para496_L_bn1.beta, %para497_L_bn1.moving_mean, %para498_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1161) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873:CNode_1160{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873:CNode_1161{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1160, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_873:CNode_1162{[0]: ValueNode<Primitive> Return, [1]: CNode_1161}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876 : 0000029BC72C4220
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_1163) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1164) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1167) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876:CNode_1163{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876:CNode_1164{[0]: ValueNode<Primitive> Switch, [1]: CNode_1163, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876:CNode_1167{[0]: CNode_1164}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_876:CNode_1168{[0]: ValueNode<Primitive> Return, [1]: CNode_1167}


subgraph attr:
training : 1
subgraph instance: get_loss_885 : 0000029BC779F070
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_885(%para655_x, %para656_weights) {
  %1(CNode_1170) = call @get_loss_1169()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_885:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_885:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_885:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_885:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_885:CNode_1170{[0]: ValueNode<FuncGraph> get_loss_1169}
#   6: @get_loss_885:CNode_1171{[0]: ValueNode<Primitive> Return, [1]: CNode_1170}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_888 : 0000029BC75924B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_888 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_222]() {
  %1(CNode_1173) = call @mindspore_nn_layer_basic_Dense_construct_1172()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_888:CNode_1173{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_1172}
#   2: @mindspore_nn_layer_basic_Dense_construct_888:CNode_1174{[0]: ValueNode<Primitive> Return, [1]: CNode_1173}


subgraph attr:
subgraph instance: flatten_901 : 0000029BC758AA80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_901() {
  %1(CNode_1175) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_1176) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @flatten_901:CNode_1175{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @flatten_901:CNode_1176{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_1175, [3]: ValueNode<StringImm> None}
#   3: @flatten_901:CNode_1177{[0]: ValueNode<Primitive> Return, [1]: CNode_1176}


subgraph attr:
subgraph instance: flatten_902 : 0000029BC7587560
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_902 parent: [subgraph @flatten_160]() {
  %1(CNode_1179) = call @flatten_1178()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_902:CNode_1179{[0]: ValueNode<FuncGraph> flatten_1178}
#   2: @flatten_902:CNode_1180{[0]: ValueNode<Primitive> Return, [1]: CNode_1179}


subgraph attr:
subgraph instance: flatten_896 : 0000029BC75805D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_896 parent: [subgraph @flatten_510]() {
  %1(CNode_892) = $(flatten_510):S_Prim_isinstance(%para468_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_893) = $(flatten_510):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_896:CNode_1181{[0]: ValueNode<Primitive> Return, [1]: CNode_893}


subgraph attr:
subgraph instance: flatten_897 : 0000029BC7583050
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_897 parent: [subgraph @flatten_160]() {
  %1(CNode_1182) = S_Prim_isinstance(%para469_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1183) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_1184) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_1185) = Switch(%3, @flatten_1186, @flatten_1187)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_1188) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_897:CNode_1182{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_897:CNode_1183{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_1182}
#   3: @flatten_897:CNode_1184{[0]: ValueNode<Primitive> Cond, [1]: CNode_1183, [2]: ValueNode<BoolImm> false}
#   4: @flatten_897:CNode_1185{[0]: ValueNode<Primitive> Switch, [1]: CNode_1184, [2]: ValueNode<FuncGraph> flatten_1186, [3]: ValueNode<FuncGraph> flatten_1187}
#   5: @flatten_897:CNode_1188{[0]: CNode_1185}
#   6: @flatten_897:CNode_1189{[0]: ValueNode<Primitive> Return, [1]: CNode_1188}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911 : 0000029BC75ACDB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1191) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911:CNode_1191{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_911:CNode_1192{[0]: ValueNode<Primitive> Return, [1]: CNode_1191}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912 : 0000029BC75AE840
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1194) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912:CNode_1194{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_912:CNode_1195{[0]: ValueNode<Primitive> Return, [1]: CNode_1194}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_915 : 0000029BC75B47E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_915 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_264]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_264):S_Prim_Conv2D[out_channel: I64(2048), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para501_x, %para138_layer4.0.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.0.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_915:CNode_1196{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924 : 0000029BC7505790
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1198) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924:CNode_1198{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_924:CNode_1199{[0]: ValueNode<Primitive> Return, [1]: CNode_1198}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925 : 0000029BC7504250
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1201) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925:CNode_1201{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_925:CNode_1202{[0]: ValueNode<Primitive> Return, [1]: CNode_1201}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_928 : 0000029BC75B4D30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_928 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_262]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_262):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para504_x, %para135_layer4.0.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.0.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_928:CNode_1203{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_931 : 0000029BC75BA780
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_931 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_259]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_259):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para506_x, %para132_layer4.0.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 1024, 1, 1), ref_key=:layer4.0.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_931:CNode_1204{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1205 : 0000029BC75BCCB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1205 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para657_x) {
  %1(CNode_1207) = call @mindspore_nn_layer_conv_Conv2d_construct_1206()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1205:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer4.0.down_sample_layer.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1205:CNode_1207{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1206}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1205:CNode_1208{[0]: ValueNode<Primitive> Return, [1]: CNode_1207}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1209 : 0000029BC75B9790
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para658_x) {
  %1(CNode_1210) = S_Prim_Shape(%para658_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1211) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1212) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1213) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1214) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1215) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1216, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1217)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1218) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1219) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1210{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1211{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1210, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1213{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1214{[0]: ValueNode<Primitive> Cond, [1]: CNode_1213, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1215{[0]: ValueNode<Primitive> Switch, [1]: CNode_1214, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1216, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1217}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1218{[0]: CNode_1215}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209:CNode_1220{[0]: ValueNode<Primitive> Return, [1]: CNode_1219}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_934 : 0000029BC75BB220
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_934 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_534](%para659_, %para660_) {
  %1(CNode_937) = $(mindspore_nn_layer_container_SequentialCell_construct_534):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1205, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_936) = $(mindspore_nn_layer_container_SequentialCell_construct_534):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1221) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para659_@CNode_1222, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1223) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1224, @mindspore_nn_layer_container_SequentialCell_construct_1225)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1226) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_934:CNode_1221{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_1222, [2]: CNode_936}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_934:CNode_1223{[0]: ValueNode<Primitive> Switch, [1]: CNode_1221, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1224, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1225}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_934:CNode_1226{[0]: CNode_1223}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_934:CNode_1227{[0]: ValueNode<Primitive> Return, [1]: CNode_1226}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_940 : 0000029BC75AF2E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_940 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_275]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_275):S_Prim_Conv2D[out_channel: I64(2048), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para508_x, %para150_layer4.1.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.1.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_940:CNode_1228{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_943 : 0000029BC75B02D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_943 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_273]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_273):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para511_x, %para147_layer4.1.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.1.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_943:CNode_1229{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_946 : 0000029BC75AE2F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_946 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_270]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_270):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para513_x, %para144_layer4.1.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.1.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_946:CNode_1230{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_950 : 0000029BC75A9340
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_950 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_286]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_286):S_Prim_Conv2D[out_channel: I64(2048), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para515_x, %para159_layer4.2.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=:layer4.2.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_950:CNode_1231{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_953 : 0000029BC75A7E00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_953 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_284]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_284):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para518_x, %para156_layer4.2.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=:layer4.2.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_953:CNode_1232{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_956 : 0000029BC759EE90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_956 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_281]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_281):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para520_x, %para153_layer4.2.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=:layer4.2.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_956:CNode_1233{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968 : 0000029BC74CCB20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1235) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968:CNode_1235{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_968:CNode_1236{[0]: ValueNode<Primitive> Return, [1]: CNode_1235}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969 : 0000029BC74D0040
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1238) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969:CNode_1238{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_969:CNode_1239{[0]: ValueNode<Primitive> Return, [1]: CNode_1238}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_972 : 0000029BC75A0E70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_972 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_305]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_305):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para522_x, %para81_layer3.0.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.0.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_972:CNode_1240{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981 : 0000029BC74E4450
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1242) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981:CNode_1242{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_981:CNode_1243{[0]: ValueNode<Primitive> Return, [1]: CNode_1242}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982 : 0000029BC74E6ED0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1245) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982:CNode_1245{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_982:CNode_1246{[0]: ValueNode<Primitive> Return, [1]: CNode_1245}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_985 : 0000029BC759BEC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_985 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_303]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_303):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para525_x, %para78_layer3.0.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.0.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_985:CNode_1247{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_988 : 0000029BC759D400
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_988 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_300]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_300):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para527_x, %para75_layer3.0.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 512, 1, 1), ref_key=:layer3.0.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_988:CNode_1248{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1249 : 0000029BC759FE80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1249 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para661_x) {
  %1(CNode_1251) = call @mindspore_nn_layer_conv_Conv2d_construct_1250()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1249:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer3.0.down_sample_layer.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1249:CNode_1251{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1250}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1249:CNode_1252{[0]: ValueNode<Primitive> Return, [1]: CNode_1251}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1253 : 0000029BC759E3F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para662_x) {
  %1(CNode_1254) = S_Prim_Shape(%para662_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1255) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1256) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1257) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1258) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1259) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1260, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1261)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1262) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1263) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1254{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1255{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1254, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1257{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1258{[0]: ValueNode<Primitive> Cond, [1]: CNode_1257, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1259{[0]: ValueNode<Primitive> Switch, [1]: CNode_1258, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1260, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1261}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1262{[0]: CNode_1259}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253:CNode_1264{[0]: ValueNode<Primitive> Return, [1]: CNode_1263}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_991 : 0000029BC75A48E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_991 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_605](%para663_, %para664_) {
  %1(CNode_994) = $(mindspore_nn_layer_container_SequentialCell_construct_605):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1249, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_993) = $(mindspore_nn_layer_container_SequentialCell_construct_605):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1265) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para663_@CNode_1266, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1267) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1268, @mindspore_nn_layer_container_SequentialCell_construct_1269)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1270) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_991:CNode_1265{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_1266, [2]: CNode_993}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_991:CNode_1267{[0]: ValueNode<Primitive> Switch, [1]: CNode_1265, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1268, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1269}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_991:CNode_1270{[0]: CNode_1267}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_991:CNode_1271{[0]: ValueNode<Primitive> Return, [1]: CNode_1270}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_997 : 0000029BC759B420
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_997 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_316]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_316):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para529_x, %para93_layer3.1.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.1.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_997:CNode_1272{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1000 : 0000029BC75979B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1000 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_314]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_314):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para532_x, %para90_layer3.1.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.1.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1000:CNode_1273{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1003 : 0000029BC759C960
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1003 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_311]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_311):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para534_x, %para87_layer3.1.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.1.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1003:CNode_1274{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1007 : 0000029BC74D7A70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1007 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_327]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_327):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para536_x, %para102_layer3.2.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.2.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1007:CNode_1275{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1010 : 0000029BC74D7520
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1010 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_325]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_325):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para539_x, %para99_layer3.2.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.2.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1010:CNode_1276{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1013 : 0000029BC74DC4D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1013 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_322]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_322):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para541_x, %para96_layer3.2.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.2.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1013:CNode_1277{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1017 : 0000029BC74DB4E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1017 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_338]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_338):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para543_x, %para111_layer3.3.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.3.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1017:CNode_1278{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1020 : 0000029BC74D6530
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1020 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_336]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_336):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para546_x, %para108_layer3.3.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.3.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1020:CNode_1279{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1023 : 0000029BC74DDA10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1023 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_333]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_333):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para548_x, %para105_layer3.3.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.3.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1023:CNode_1280{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1027 : 0000029BC74D4AA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1027 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_349]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_349):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para550_x, %para120_layer3.4.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.4.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1027:CNode_1281{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1030 : 0000029BC74CF5A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1030 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_347]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_347):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para553_x, %para117_layer3.4.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.4.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1030:CNode_1282{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1033 : 0000029BC74D2570
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1033 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_344]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_344):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para555_x, %para114_layer3.4.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.4.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/4-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1033:CNode_1283{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1037 : 0000029BC74CD070
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1037 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_360]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_360):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para557_x, %para129_layer3.5.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 256, 1, 1), ref_key=:layer3.5.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1037:CNode_1284{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1040 : 0000029BC750C720
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1040 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_358]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_358):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para560_x, %para126_layer3.5.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=:layer3.5.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1040:CNode_1285{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1043 : 0000029BC7509CA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1043 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_355]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_355):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para562_x, %para123_layer3.5.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 1024, 1, 1), ref_key=:layer3.5.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/5-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1043:CNode_1286{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053 : 0000029BC74F2370
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1288) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053:CNode_1288{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1053:CNode_1289{[0]: ValueNode<Primitive> Return, [1]: CNode_1288}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054 : 0000029BC74F1380
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1291) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054:CNode_1291{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1054:CNode_1292{[0]: ValueNode<Primitive> Return, [1]: CNode_1291}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1057 : 0000029BC7500D30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1057 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_378]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_378):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para564_x, %para42_layer2.0.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.0.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1057:CNode_1293{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066 : 0000029BC74F1E20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1295) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066:CNode_1295{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1066:CNode_1296{[0]: ValueNode<Primitive> Return, [1]: CNode_1295}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067 : 0000029BC74EDE60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1298) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067:CNode_1298{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1067:CNode_1299{[0]: ValueNode<Primitive> Return, [1]: CNode_1298}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1070 : 0000029BC7501280
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1070 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_376]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_376):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para567_x, %para39_layer2.0.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.0.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1070:CNode_1300{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1073 : 0000029BC74FF2A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1073 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_373]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_373):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para569_x, %para36_layer2.0.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 256, 1, 1), ref_key=:layer2.0.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1073:CNode_1301{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1302 : 0000029BC7507770
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1302 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para665_x) {
  %1(CNode_1304) = call @mindspore_nn_layer_conv_Conv2d_construct_1303()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1302:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer2.0.down_sample_layer.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1302:CNode_1304{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1303}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1302:CNode_1305{[0]: ValueNode<Primitive> Return, [1]: CNode_1304}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1306 : 0000029BC7506230
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1306 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para666_x) {
  %1(CNode_1307) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521(%para666_x, %para46_layer2.0.down_sample_layer.1.gamma, %para47_layer2.0.down_sample_layer.1.beta, %para430_layer2.0.down_sample_layer.1.moving_mean, %para431_layer2.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:layer2.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1306:CNode_1307{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521, [1]: param_x, [2]: param_layer2.0.down_sample_layer.1.gamma, [3]: param_layer2.0.down_sample_layer.1.beta, [4]: param_layer2.0.down_sample_layer.1.moving_mean, [5]: param_layer2.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1306:CNode_1308{[0]: ValueNode<Primitive> Return, [1]: CNode_1307}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1076 : 0000029BC7500290
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1076 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_734](%para667_, %para668_) {
  %1(CNode_1079) = $(mindspore_nn_layer_container_SequentialCell_construct_734):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1302, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1306)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1078) = $(mindspore_nn_layer_container_SequentialCell_construct_734):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1309) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para667_@CNode_1310, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1311) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1312, @mindspore_nn_layer_container_SequentialCell_construct_1313)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1314) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1076:CNode_1309{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_1310, [2]: CNode_1078}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1076:CNode_1311{[0]: ValueNode<Primitive> Switch, [1]: CNode_1309, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1312, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1313}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1076:CNode_1314{[0]: CNode_1311}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1076:CNode_1315{[0]: ValueNode<Primitive> Return, [1]: CNode_1314}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1082 : 0000029BC74F9300
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1082 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_389]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_389):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para571_x, %para54_layer2.1.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.1.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1082:CNode_1316{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1085 : 0000029BC74FCD70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1085 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_387]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_387):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para574_x, %para51_layer2.1.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.1.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1085:CNode_1317{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1088 : 0000029BC74FB830
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1088 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_384]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_384):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para576_x, %para48_layer2.1.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.1.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1088:CNode_1318{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1092 : 0000029BC74F7870
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1092 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_400]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_400):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para578_x, %para63_layer2.2.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.2.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1092:CNode_1319{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1095 : 0000029BC74F5DE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1095 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_398]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_398):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para581_x, %para60_layer2.2.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.2.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1095:CNode_1320{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1098 : 0000029BC74EC3D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1098 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_395]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_395):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para583_x, %para57_layer2.2.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.2.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1098:CNode_1321{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1102 : 0000029BC74EFE40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1102 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_411]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_411):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para585_x, %para72_layer2.3.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 128, 1, 1), ref_key=:layer2.3.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1102:CNode_1322{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1105 : 0000029BC74EF3A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1105 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_409]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_409):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para588_x, %para69_layer2.3.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:layer2.3.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1105:CNode_1323{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1108 : 0000029BC74E1F20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1108 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_406]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_406):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para590_x, %para66_layer2.3.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 512, 1, 1), ref_key=:layer2.3.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/3-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1108:CNode_1324{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118 : 0000029BC728A5C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1326) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118:CNode_1326{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1118:CNode_1327{[0]: ValueNode<Primitive> Return, [1]: CNode_1326}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119 : 0000029BC72895D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1329) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119:CNode_1329{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1119:CNode_1330{[0]: ValueNode<Primitive> Return, [1]: CNode_1329}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1122 : 0000029BC7294A70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1122 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_429]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_429):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para592_x, %para12_layer1.0.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1122:CNode_1331{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1125 : 0000029BC7297A40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1125 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_427]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_427):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para595_x, %para9_layer1.0.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.0.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1125:CNode_1332{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1128 : 0000029BC729AF60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1128 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_424]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_424):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para597_x, %para6_layer1.0.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 1, 1), ref_key=:layer1.0.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1128:CNode_1333{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1334 : 0000029BC74E5440
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1334 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para669_x) {
  %1(CNode_1336) = call @mindspore_nn_layer_conv_Conv2d_construct_1335()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1334:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_layer1.0.down_sample_layer.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1334:CNode_1336{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1335}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1334:CNode_1337{[0]: ValueNode<Primitive> Return, [1]: CNode_1336}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1338 : 0000029BC74E3F00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_2](%para670_x) {
  %1(CNode_1339) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592(%para670_x, %para16_layer1.0.down_sample_layer.1.gamma, %para17_layer1.0.down_sample_layer.1.beta, %para432_layer1.0.down_sample_layer.1.moving_mean, %para433_layer1.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (256), ref_key=:layer1.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338:CNode_1339{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592, [1]: param_x, [2]: param_layer1.0.down_sample_layer.1.gamma, [3]: param_layer1.0.down_sample_layer.1.beta, [4]: param_layer1.0.down_sample_layer.1.moving_mean, [5]: param_layer1.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338:CNode_1340{[0]: ValueNode<Primitive> Return, [1]: CNode_1339}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1131 : 0000029BC74E49A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1131 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_822](%para671_, %para672_) {
  %1(CNode_1134) = $(mindspore_nn_layer_container_SequentialCell_construct_822):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1334, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1133) = $(mindspore_nn_layer_container_SequentialCell_construct_822):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1341) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para671_@CNode_1342, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1343) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1344, @mindspore_nn_layer_container_SequentialCell_construct_1345)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1346) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1131:CNode_1341{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.295, [1]: param_@CNode_1342, [2]: CNode_1133}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1131:CNode_1343{[0]: ValueNode<Primitive> Switch, [1]: CNode_1341, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1344, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1345}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1131:CNode_1346{[0]: CNode_1343}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1131:CNode_1347{[0]: ValueNode<Primitive> Return, [1]: CNode_1346}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1137 : 0000029BC72994D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1137 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_440]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_440):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para599_x, %para24_layer1.1.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.1.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1137:CNode_1348{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1140 : 0000029BC7296A50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1140 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_438]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_438):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para602_x, %para21_layer1.1.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.1.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1140:CNode_1349{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1143 : 0000029BC7293A80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1143 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_435]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_435):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para604_x, %para18_layer1.1.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.1.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/1-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1143:CNode_1350{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1147 : 0000029BC728F570
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1147 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_451]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_451):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para606_x, %para33_layer1.2.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.2.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv3-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1147:CNode_1351{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1150 : 0000029BC7290AB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1150 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_449]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_449):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para609_x, %para30_layer1.2.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:layer1.2.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv2-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1150:CNode_1352{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1153 : 0000029BC728F020
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1153 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_446]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_446):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para611_x, %para27_layer1.2.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:layer1.2.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/2-ResidualBlock/conv1-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1153:CNode_1353{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1157 : 0000029BC72C8730
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1157 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870]() {
  %1(CNode_1354) = Cond(%para613_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_1355) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_1356, @mindspore_nn_layer_pooling_MaxPool2d_construct_1357)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_1358) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_1360) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1359(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:210/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1157:CNode_1354{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1157:CNode_1355{[0]: ValueNode<Primitive> Switch, [1]: CNode_1354, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1356, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1357}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1157:CNode_1358{[0]: CNode_1355}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1157:CNode_1360{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1359, [1]: CNode_1358}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1157:CNode_1361{[0]: ValueNode<Primitive> Return, [1]: CNode_1360}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165 : 0000029BC72C5210
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_1362) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para494_x, %para495_L_bn1.gamma, %para496_L_bn1.beta, %para497_L_bn1.moving_mean, %para498_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1363) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165:CNode_1362{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165:CNode_1363{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1362, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1165:CNode_1364{[0]: ValueNode<Primitive> Return, [1]: CNode_1363}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166 : 0000029BC72C4CC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_1366) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166:CNode_1366{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1166:CNode_1367{[0]: ValueNode<Primitive> Return, [1]: CNode_1366}


subgraph attr:
training : 1
subgraph instance: get_loss_1169 : 0000029BC77A5010
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1169 parent: [subgraph @get_loss_885]() {
  %1(CNode_1369) = call @get_loss_1368()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @get_loss_1169:CNode_1370{[0]: ValueNode<FuncGraph> get_axis_1371, [1]: x}
#   2: @get_loss_1169:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_1370}
#   3: @get_loss_1169:CNode_1369{[0]: ValueNode<FuncGraph> get_loss_1368}
#   4: @get_loss_1169:CNode_1372{[0]: ValueNode<Primitive> Return, [1]: CNode_1369}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1172 : 0000029BC7592F50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1172 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_222]() {
  %1(x_shape) = $(mindspore_nn_layer_basic_Dense_construct_46):S_Prim_Shape(%para452_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_1373) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_1374) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_1375) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_1376) = Switch(%4, @mindspore_nn_layer_basic_Dense_construct_1377, @mindspore_nn_layer_basic_Dense_construct_1378)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_1379) = %5()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_1381) = call @mindspore_nn_layer_basic_Dense_construct_1380(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:217/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1373{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1374{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_1373, [2]: ValueNode<Int64Imm> 2}
#   3: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1375{[0]: ValueNode<Primitive> Cond, [1]: CNode_1374, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1376{[0]: ValueNode<Primitive> Switch, [1]: CNode_1375, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_1377, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_1378}
#   5: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1379{[0]: CNode_1376}
#   6: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1381{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_1380, [1]: CNode_1379}
#   7: @mindspore_nn_layer_basic_Dense_construct_1172:CNode_1382{[0]: ValueNode<Primitive> Return, [1]: CNode_1381}


subgraph attr:
after_block : 1
subgraph instance: flatten_1178 : 0000029BC7585580
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1178 parent: [subgraph @flatten_160]() {
  %1(CNode_1383) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para467_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_1384) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_1385) = S_Prim_equal(%para467_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %4(CNode_1386) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %5(CNode_1387) = Switch(%4, @flatten_1388, @flatten_1389)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %6(CNode_1390) = %5()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %7(CNode_1391) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_1178:CNode_1383{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @flatten_1178:CNode_1385{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @flatten_1178:CNode_1386{[0]: ValueNode<Primitive> Cond, [1]: CNode_1385, [2]: ValueNode<BoolImm> false}
#   4: @flatten_1178:CNode_1387{[0]: ValueNode<Primitive> Switch, [1]: CNode_1386, [2]: ValueNode<FuncGraph> flatten_1388, [3]: ValueNode<FuncGraph> flatten_1389}
#   5: @flatten_1178:CNode_1390{[0]: CNode_1387}
#   6: @flatten_1178:CNode_1392{[0]: ValueNode<Primitive> Return, [1]: CNode_1391}


subgraph attr:
subgraph instance: flatten_1186 : 0000029BC7585030
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1186 parent: [subgraph @flatten_897]() {
  %1(CNode_1182) = $(flatten_897):S_Prim_isinstance(%para469_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_1183) = $(flatten_897):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_1186:CNode_1393{[0]: ValueNode<Primitive> Return, [1]: CNode_1183}


subgraph attr:
subgraph instance: flatten_1187 : 0000029BC7582B00
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1187 parent: [subgraph @flatten_160]() {
  %1(CNode_1394) = S_Prim_isinstance(%para468_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_1395) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_1396) = Switch(%2, @flatten_1397, @flatten_1398)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_1399) = %3()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_1187:CNode_1394{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_1187:CNode_1395{[0]: ValueNode<Primitive> Cond, [1]: CNode_1394, [2]: ValueNode<BoolImm> false}
#   3: @flatten_1187:CNode_1396{[0]: ValueNode<Primitive> Switch, [1]: CNode_1395, [2]: ValueNode<FuncGraph> flatten_1397, [3]: ValueNode<FuncGraph> flatten_1398}
#   4: @flatten_1187:CNode_1399{[0]: CNode_1396}
#   5: @flatten_1187:CNode_1400{[0]: ValueNode<Primitive> Return, [1]: CNode_1399}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190 : 0000029BC75AB320
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1401) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para615_x, %para616_L_layer4.0.bn3.gamma, %para617_L_layer4.0.bn3.beta, %para618_L_layer4.0.bn3.moving_mean, %para619_L_layer4.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1402) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190:CNode_1401{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn3.gamma, [3]: param_L_layer4.0.bn3.beta, [4]: param_L_layer4.0.bn3.moving_mean, [5]: param_L_layer4.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190:CNode_1402{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1401, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1190:CNode_1403{[0]: ValueNode<Primitive> Return, [1]: CNode_1402}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193 : 0000029BC75AFD80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1404) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1405) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1408) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193:CNode_1404{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193:CNode_1405{[0]: ValueNode<Primitive> Switch, [1]: CNode_1404, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193:CNode_1408{[0]: CNode_1405}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1193:CNode_1409{[0]: ValueNode<Primitive> Return, [1]: CNode_1408}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197 : 0000029BC75037B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1410) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para620_x, %para621_L_layer4.0.bn2.gamma, %para622_L_layer4.0.bn2.beta, %para623_L_layer4.0.bn2.moving_mean, %para624_L_layer4.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1411) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197:CNode_1410{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn2.gamma, [3]: param_L_layer4.0.bn2.beta, [4]: param_L_layer4.0.bn2.moving_mean, [5]: param_L_layer4.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197:CNode_1411{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1410, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1197:CNode_1412{[0]: ValueNode<Primitive> Return, [1]: CNode_1411}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200 : 0000029BC7502D10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1413) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1414) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1417) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200:CNode_1413{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200:CNode_1414{[0]: ValueNode<Primitive> Switch, [1]: CNode_1413, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200:CNode_1417{[0]: CNode_1414}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1200:CNode_1418{[0]: ValueNode<Primitive> Return, [1]: CNode_1417}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1206 : 0000029BC75BD200
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1206 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1205]() {
  %1(CNode_1420) = call @mindspore_nn_layer_conv_Conv2d_construct_1419()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1206:CNode_1420{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1419}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1206:CNode_1421{[0]: ValueNode<Primitive> Return, [1]: CNode_1420}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1216 : 0000029BC75B67C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1216 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1423) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1422()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1216:CNode_1423{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1422}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1216:CNode_1424{[0]: ValueNode<Primitive> Return, [1]: CNode_1423}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1217 : 0000029BC75B9CE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1217 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1426) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1217:CNode_1426{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1425}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1217:CNode_1427{[0]: ValueNode<Primitive> Return, [1]: CNode_1426}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1224 : 0000029BC75B57D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1224 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_934]() {
  %1(CNode_1222) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para659_@CNode_1222, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1428) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_937) = $(mindspore_nn_layer_container_SequentialCell_construct_534):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1205, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1429) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para659_@CNode_1222)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para660_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1430) = call @mindspore_nn_layer_container_SequentialCell_construct_934(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1431) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1224:CNode_1429{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_937}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1224:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1429, [2]: param_@CNode_1222}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1224:CNode_1222{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_1222, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1224:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1224:CNode_1430{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_934, [1]: CNode_1222, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1224:CNode_1432{[0]: ValueNode<Primitive> Return, [1]: CNode_1431}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1225 : 0000029BC75BB770
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1225 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_934]() {
  Return(%para660_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1225:CNode_1433{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234 : 0000029BC74D2AC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1434) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para627_x, %para628_L_layer3.0.bn3.gamma, %para629_L_layer3.0.bn3.beta, %para630_L_layer3.0.bn3.moving_mean, %para631_L_layer3.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1435) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234:CNode_1434{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn3.gamma, [3]: param_L_layer3.0.bn3.beta, [4]: param_L_layer3.0.bn3.moving_mean, [5]: param_L_layer3.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234:CNode_1435{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1434, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1234:CNode_1436{[0]: ValueNode<Primitive> Return, [1]: CNode_1435}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237 : 0000029BC74CE060
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1437) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1438) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1441) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237:CNode_1437{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237:CNode_1438{[0]: ValueNode<Primitive> Switch, [1]: CNode_1437, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237:CNode_1441{[0]: CNode_1438}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1237:CNode_1442{[0]: ValueNode<Primitive> Return, [1]: CNode_1441}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241 : 0000029BC74E3460
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1443) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para632_x, %para633_L_layer3.0.bn2.gamma, %para634_L_layer3.0.bn2.beta, %para635_L_layer3.0.bn2.moving_mean, %para636_L_layer3.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1444) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241:CNode_1443{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn2.gamma, [3]: param_L_layer3.0.bn2.beta, [4]: param_L_layer3.0.bn2.moving_mean, [5]: param_L_layer3.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241:CNode_1444{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1443, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1241:CNode_1445{[0]: ValueNode<Primitive> Return, [1]: CNode_1444}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244 : 0000029BC74E2F10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1446) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1447) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1450) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244:CNode_1446{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244:CNode_1447{[0]: ValueNode<Primitive> Switch, [1]: CNode_1446, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244:CNode_1450{[0]: CNode_1447}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1244:CNode_1451{[0]: ValueNode<Primitive> Return, [1]: CNode_1450}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1250 : 0000029BC75A1E60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1250 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1249]() {
  %1(CNode_1453) = call @mindspore_nn_layer_conv_Conv2d_construct_1452()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1250:CNode_1453{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1452}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1250:CNode_1454{[0]: ValueNode<Primitive> Return, [1]: CNode_1453}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1260 : 0000029BC75A1910
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1260 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1456) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1455()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1260:CNode_1456{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1455}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1260:CNode_1457{[0]: ValueNode<Primitive> Return, [1]: CNode_1456}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1261 : 0000029BC759DEA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1261 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1459) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1261:CNode_1459{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1458}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1261:CNode_1460{[0]: ValueNode<Primitive> Return, [1]: CNode_1459}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1268 : 0000029BC759F3E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1268 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_991]() {
  %1(CNode_1266) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para663_@CNode_1266, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1461) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_994) = $(mindspore_nn_layer_container_SequentialCell_construct_605):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1249, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1462) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para663_@CNode_1266)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para664_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1463) = call @mindspore_nn_layer_container_SequentialCell_construct_991(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1464) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1268:CNode_1462{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_994}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1268:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1462, [2]: param_@CNode_1266}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1268:CNode_1266{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_1266, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1268:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1268:CNode_1463{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_991, [1]: CNode_1266, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1268:CNode_1465{[0]: ValueNode<Primitive> Return, [1]: CNode_1464}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1269 : 0000029BC759D950
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1269 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_991]() {
  Return(%para664_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1269:CNode_1466{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287 : 0000029BC74F28C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1467) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para638_x, %para639_L_layer2.0.bn3.gamma, %para640_L_layer2.0.bn3.beta, %para641_L_layer2.0.bn3.moving_mean, %para642_L_layer2.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1468) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287:CNode_1467{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn3.gamma, [3]: param_L_layer2.0.bn3.beta, [4]: param_L_layer2.0.bn3.moving_mean, [5]: param_L_layer2.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287:CNode_1468{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1467, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1287:CNode_1469{[0]: ValueNode<Primitive> Return, [1]: CNode_1468}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290 : 0000029BC74F3360
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1470) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1471) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1474) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290:CNode_1470{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290:CNode_1471{[0]: ValueNode<Primitive> Switch, [1]: CNode_1470, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290:CNode_1474{[0]: CNode_1471}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1290:CNode_1475{[0]: ValueNode<Primitive> Return, [1]: CNode_1474}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294 : 0000029BC74EA940
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1476) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para643_x, %para644_L_layer2.0.bn2.gamma, %para645_L_layer2.0.bn2.beta, %para646_L_layer2.0.bn2.moving_mean, %para647_L_layer2.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1477) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294:CNode_1476{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn2.gamma, [3]: param_L_layer2.0.bn2.beta, [4]: param_L_layer2.0.bn2.moving_mean, [5]: param_L_layer2.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294:CNode_1477{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1476, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1294:CNode_1478{[0]: ValueNode<Primitive> Return, [1]: CNode_1477}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297 : 0000029BC74EAE90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1479) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1480) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1483) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297:CNode_1479{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297:CNode_1480{[0]: ValueNode<Primitive> Switch, [1]: CNode_1479, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297:CNode_1483{[0]: CNode_1480}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1297:CNode_1484{[0]: ValueNode<Primitive> Return, [1]: CNode_1483}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1303 : 0000029BC7505CE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1303 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1302]() {
  %1(CNode_1486) = call @mindspore_nn_layer_conv_Conv2d_construct_1485()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1303:CNode_1486{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1485}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1303:CNode_1487{[0]: ValueNode<Primitive> Return, [1]: CNode_1486}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1312 : 0000029BC7507CC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1312 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1076]() {
  %1(CNode_1310) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para667_@CNode_1310, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1488) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1079) = $(mindspore_nn_layer_container_SequentialCell_construct_734):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1302, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1306)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1489) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para667_@CNode_1310)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para668_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1490) = call @mindspore_nn_layer_container_SequentialCell_construct_1076(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1491) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1312:CNode_1489{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_1079}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1312:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1489, [2]: param_@CNode_1310}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1312:CNode_1310{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_1310, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1312:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1312:CNode_1490{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1076, [1]: CNode_1310, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1312:CNode_1492{[0]: ValueNode<Primitive> Return, [1]: CNode_1491}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1313 : 0000029BC7506CD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1313 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1076]() {
  Return(%para668_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1313:CNode_1493{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325 : 0000029BC7292540
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1494) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para649_x, %para650_L_layer1.0.bn3.gamma, %para651_L_layer1.0.bn3.beta, %para652_L_layer1.0.bn3.moving_mean, %para653_L_layer1.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1495) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325:CNode_1494{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer1.0.bn3.gamma, [3]: param_L_layer1.0.bn3.beta, [4]: param_L_layer1.0.bn3.moving_mean, [5]: param_L_layer1.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325:CNode_1495{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1494, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1325:CNode_1496{[0]: ValueNode<Primitive> Return, [1]: CNode_1495}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328 : 0000029BC7291550
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1497) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1498) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1501) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328:CNode_1497{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328:CNode_1498{[0]: ValueNode<Primitive> Switch, [1]: CNode_1497, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328:CNode_1501{[0]: CNode_1498}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1328:CNode_1502{[0]: ValueNode<Primitive> Return, [1]: CNode_1501}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1335 : 0000029BC74E8960
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1335 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1334]() {
  %1(CNode_1504) = call @mindspore_nn_layer_conv_Conv2d_construct_1503()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1335:CNode_1504{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1503}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1335:CNode_1505{[0]: ValueNode<Primitive> Return, [1]: CNode_1504}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1344 : 0000029BC74E6980
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1344 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1131]() {
  %1(CNode_1342) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para671_@CNode_1342, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1506) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1134) = $(mindspore_nn_layer_container_SequentialCell_construct_822):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1334, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1507) = call @ms_iter_577(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para671_@CNode_1342)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para672_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1508) = call @mindspore_nn_layer_container_SequentialCell_construct_1131(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1509) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1507{[0]: ValueNode<FuncGraph> ms_iter_577, [1]: CNode_1134}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1344:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1507, [2]: param_@CNode_1342}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1342{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.581, [1]: param_@CNode_1342, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1344:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1508{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1131, [1]: CNode_1342, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1510{[0]: ValueNode<Primitive> Return, [1]: CNode_1509}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1345 : 0000029BC74E7970
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1345 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1131]() {
  Return(%para672_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1345:CNode_1511{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1359 : 0000029BC728CAF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1359(%para673_) {
  %1(CNode_1513) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1512()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1359:CNode_1513{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1512}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1359:CNode_1514{[0]: ValueNode<Primitive> Return, [1]: CNode_1513}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1356 : 0000029BC72C81E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1356 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_870):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para612_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1515) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_1516) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_1517) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_1518, @mindspore_nn_layer_pooling_MaxPool2d_construct_1519)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_1520) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_1522) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1521(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet)
      # In file C:\Users\Azra\AppData\Local\Temp\ipykernel_2160\3918918428.py:210/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1515{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1516{[0]: ValueNode<Primitive> Cond, [1]: CNode_1515, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1517{[0]: ValueNode<Primitive> Switch, [1]: CNode_1516, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1518, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1519}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1520{[0]: CNode_1517}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1522{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1521, [1]: CNode_1520}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1356:CNode_1523{[0]: ValueNode<Primitive> Return, [1]: CNode_1522}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1357 : 0000029BC72C7C90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1357 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_870):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para612_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1357:CNode_1524{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365 : 0000029BC72C4770
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_192]() {
  %1(CNode_1525) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para494_x, %para495_L_bn1.gamma, %para496_L_bn1.beta, %para497_L_bn1.moving_mean, %para498_L_bn1.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1526) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/bn1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365:CNode_1525{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn1.gamma, [3]: param_L_bn1.beta, [4]: param_L_bn1.moving_mean, [5]: param_L_bn1.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365:CNode_1526{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1525, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1365:CNode_1527{[0]: ValueNode<Primitive> Return, [1]: CNode_1526}


subgraph attr:
training : 1
subgraph instance: get_axis_1371 : 0000029BC77A5560
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_1371(%para674_x) {
  %1(shape) = call @shape_492(%para674_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_1371:shape{[0]: ValueNode<FuncGraph> shape_492, [1]: param_x}
#   2: @get_axis_1371:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_1371:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_1371:CNode_1528{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: get_loss_1368 : 0000029BC77A6000
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1368 parent: [subgraph @get_loss_1169]() {
  %1(CNode_1530) = call @get_loss_1529()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_1368:CNode_1530{[0]: ValueNode<FuncGraph> get_loss_1529}
#   2: @get_loss_1368:CNode_1531{[0]: ValueNode<Primitive> Return, [1]: CNode_1530}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1380 : 0000029BC779FB10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1380(%para675_) {
  Return(%para675_phi_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:635/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1380:CNode_1532{[0]: ValueNode<Primitive> Return, [1]: param_phi_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1377 : 0000029BC75939F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1377 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_222]() {
  %1(x) = $(mindspore_nn_layer_basic_Dense_construct_140):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para462_phi_x, %para162_end_point.weight)
      : (<null>, <Ref[Tensor[Float32]], (4, 2048), ref_key=:end_point.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(mindspore_nn_layer_basic_Dense_construct_222):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para163_end_point.bias)
      : (<null>, <Ref[Tensor[Float32]], (4), ref_key=:end_point.bias>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(mindspore_nn_layer_basic_Dense_construct_46):S_Prim_Shape(%para452_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_1533) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_1534) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_1535) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_1536) = call @shape_492(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_1537) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_1538) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_1539) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1533{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1534{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_1533, [3]: ValueNode<None> None}
#   3: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1535{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1534}
#   4: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1536{[0]: ValueNode<FuncGraph> shape_492, [1]: x}
#   5: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1537{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1538{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1536, [2]: CNode_1537}
#   7: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1539{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1538}
#   8: @mindspore_nn_layer_basic_Dense_construct_1377:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1535, [2]: CNode_1539}
#   9: @mindspore_nn_layer_basic_Dense_construct_1377:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @mindspore_nn_layer_basic_Dense_construct_1377:CNode_1540{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1378 : 0000029BC7593F40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1378 parent: [subgraph @mindspore_nn_layer_basic_Dense_construct_222]() {
  %1(x) = $(mindspore_nn_layer_basic_Dense_construct_140):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para462_phi_x, %para162_end_point.weight)
      : (<null>, <Ref[Tensor[Float32]], (4, 2048), ref_key=:end_point.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(mindspore_nn_layer_basic_Dense_construct_222):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para163_end_point.bias)
      : (<null>, <Ref[Tensor[Float32]], (4), ref_key=:end_point.bias>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/end_point-Dense)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1378:CNode_1541{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: flatten_1388 : 0000029BC758FA30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1388 parent: [subgraph @flatten_160]() {
  %1(x_rank) = S_Prim_Rank(%para466_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_1542) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_1543) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_1544) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_1545) = Switch(%4, @flatten_1546, @flatten_1547)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_1548) = %5()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1388:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @flatten_1388:CNode_1542{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @flatten_1388:CNode_1543{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1542}
#   4: @flatten_1388:CNode_1544{[0]: ValueNode<Primitive> Cond, [1]: CNode_1543, [2]: ValueNode<BoolImm> false}
#   5: @flatten_1388:CNode_1545{[0]: ValueNode<Primitive> Switch, [1]: CNode_1544, [2]: ValueNode<FuncGraph> flatten_1546, [3]: ValueNode<FuncGraph> flatten_1547}
#   6: @flatten_1388:CNode_1548{[0]: CNode_1545}
#   7: @flatten_1388:CNode_1549{[0]: ValueNode<Primitive> Return, [1]: CNode_1548}


subgraph attr:
subgraph instance: flatten_1389 : 0000029BC7585AD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1389 parent: [subgraph @flatten_160]() {
  %1(CNode_1551) = call @flatten_1550(%para466_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_1389:CNode_1552{[0]: ValueNode<Primitive> Return, [1]: CNode_1551}
#   2: @flatten_1389:CNode_1551{[0]: ValueNode<FuncGraph> flatten_1550, [1]: param_input}


subgraph attr:
subgraph instance: flatten_1397 : 0000029BC7584040
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1397 parent: [subgraph @flatten_1187]() {
  %1(CNode_1394) = $(flatten_1187):S_Prim_isinstance(%para468_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_1397:CNode_1553{[0]: ValueNode<Primitive> Return, [1]: CNode_1394}


subgraph attr:
subgraph instance: flatten_1398 : 0000029BC7587010
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1398 parent: [subgraph @flatten_160]() {
  %1(CNode_1554) = S_Prim_isinstance(%para469_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_1398:CNode_1554{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_1398:CNode_1555{[0]: ValueNode<Primitive> Return, [1]: CNode_1554}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406 : 0000029BC75ABDC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1556) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para615_x, %para616_L_layer4.0.bn3.gamma, %para617_L_layer4.0.bn3.beta, %para618_L_layer4.0.bn3.moving_mean, %para619_L_layer4.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1557) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406:CNode_1556{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn3.gamma, [3]: param_L_layer4.0.bn3.beta, [4]: param_L_layer4.0.bn3.moving_mean, [5]: param_L_layer4.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406:CNode_1557{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1556, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1406:CNode_1558{[0]: ValueNode<Primitive> Return, [1]: CNode_1557}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407 : 0000029BC75AD850
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1560) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407:CNode_1560{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1407:CNode_1561{[0]: ValueNode<Primitive> Return, [1]: CNode_1560}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415 : 0000029BC7505240
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1562) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para620_x, %para621_L_layer4.0.bn2.gamma, %para622_L_layer4.0.bn2.beta, %para623_L_layer4.0.bn2.moving_mean, %para624_L_layer4.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1563) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415:CNode_1562{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn2.gamma, [3]: param_L_layer4.0.bn2.beta, [4]: param_L_layer4.0.bn2.moving_mean, [5]: param_L_layer4.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415:CNode_1563{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1562, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1415:CNode_1564{[0]: ValueNode<Primitive> Return, [1]: CNode_1563}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416 : 0000029BC75047A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1566) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416:CNode_1566{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1416:CNode_1567{[0]: ValueNode<Primitive> Return, [1]: CNode_1566}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1419 : 0000029BC75BBCC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1419 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1205]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1205):S_Prim_Conv2D[out_channel: I64(2048), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para657_x, %para141_layer4.0.down_sample_layer.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (2048, 1024, 1, 1), ref_key=:layer4.0.down_sample_layer.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1419:CNode_1568{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1422 : 0000029BC75B8250
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1422 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1569) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para658_x, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para426_layer4.0.down_sample_layer.1.moving_mean, %para427_layer4.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1570) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1422:CNode_1569{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer4.0.down_sample_layer.1.gamma, [3]: param_layer4.0.down_sample_layer.1.beta, [4]: param_layer4.0.down_sample_layer.1.moving_mean, [5]: param_layer4.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1422:CNode_1570{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1569, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1422:CNode_1571{[0]: ValueNode<Primitive> Return, [1]: CNode_1570}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1425 : 0000029BC75BACD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1572) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1573) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1574, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1575)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1576) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425:CNode_1572{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425:CNode_1573{[0]: ValueNode<Primitive> Switch, [1]: CNode_1572, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1574, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1575}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425:CNode_1576{[0]: CNode_1573}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1425:CNode_1577{[0]: ValueNode<Primitive> Return, [1]: CNode_1576}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439 : 0000029BC74D2020
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1578) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para627_x, %para628_L_layer3.0.bn3.gamma, %para629_L_layer3.0.bn3.beta, %para630_L_layer3.0.bn3.moving_mean, %para631_L_layer3.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1579) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439:CNode_1578{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn3.gamma, [3]: param_L_layer3.0.bn3.beta, [4]: param_L_layer3.0.bn3.moving_mean, [5]: param_L_layer3.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439:CNode_1579{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1578, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1439:CNode_1580{[0]: ValueNode<Primitive> Return, [1]: CNode_1579}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440 : 0000029BC74CC5D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1582) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440:CNode_1582{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1440:CNode_1583{[0]: ValueNode<Primitive> Return, [1]: CNode_1582}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448 : 0000029BC74E5EE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1584) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para632_x, %para633_L_layer3.0.bn2.gamma, %para634_L_layer3.0.bn2.beta, %para635_L_layer3.0.bn2.moving_mean, %para636_L_layer3.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1585) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1584{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn2.gamma, [3]: param_L_layer3.0.bn2.beta, [4]: param_L_layer3.0.bn2.moving_mean, [5]: param_L_layer3.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1585{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1584, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1586{[0]: ValueNode<Primitive> Return, [1]: CNode_1585}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449 : 0000029BC74E6430
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1588) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449:CNode_1588{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1449:CNode_1589{[0]: ValueNode<Primitive> Return, [1]: CNode_1588}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1452 : 0000029BC75A33A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1452 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1249]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1249):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para661_x, %para84_layer3.0.down_sample_layer.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 512, 1, 1), ref_key=:layer3.0.down_sample_layer.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1452:CNode_1590{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1455 : 0000029BC75A5380
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1455 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1591) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para662_x, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para428_layer3.0.down_sample_layer.1.moving_mean, %para429_layer3.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1592) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1455:CNode_1591{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer3.0.down_sample_layer.1.gamma, [3]: param_layer3.0.down_sample_layer.1.beta, [4]: param_layer3.0.down_sample_layer.1.moving_mean, [5]: param_layer3.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1455:CNode_1592{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1591, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1455:CNode_1593{[0]: ValueNode<Primitive> Return, [1]: CNode_1592}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1458 : 0000029BC75A4E30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1594) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1595) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1597)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1598) = %2()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458:CNode_1594{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458:CNode_1595{[0]: ValueNode<Primitive> Switch, [1]: CNode_1594, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1596, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1597}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458:CNode_1598{[0]: CNode_1595}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1458:CNode_1599{[0]: ValueNode<Primitive> Return, [1]: CNode_1598}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472 : 0000029BC74F18D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1600) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para638_x, %para639_L_layer2.0.bn3.gamma, %para640_L_layer2.0.bn3.beta, %para641_L_layer2.0.bn3.moving_mean, %para642_L_layer2.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1601) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472:CNode_1600{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn3.gamma, [3]: param_L_layer2.0.bn3.beta, [4]: param_L_layer2.0.bn3.moving_mean, [5]: param_L_layer2.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472:CNode_1601{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1600, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1472:CNode_1602{[0]: ValueNode<Primitive> Return, [1]: CNode_1601}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473 : 0000029BC74EE3B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1604) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473:CNode_1604{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1473:CNode_1605{[0]: ValueNode<Primitive> Return, [1]: CNode_1604}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481 : 0000029BC74ED3C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1606) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para643_x, %para644_L_layer2.0.bn2.gamma, %para645_L_layer2.0.bn2.beta, %para646_L_layer2.0.bn2.moving_mean, %para647_L_layer2.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1607) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1606{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn2.gamma, [3]: param_L_layer2.0.bn2.beta, [4]: param_L_layer2.0.bn2.moving_mean, [5]: param_L_layer2.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1607{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1606, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1608{[0]: ValueNode<Primitive> Return, [1]: CNode_1607}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482 : 0000029BC74EF8F0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1610) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482:CNode_1610{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1482:CNode_1611{[0]: ValueNode<Primitive> Return, [1]: CNode_1610}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1485 : 0000029BC7506780
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1485 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1302]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1302):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(2), I64(2)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para665_x, %para45_layer2.0.down_sample_layer.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:layer2.0.down_sample_layer.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1485:CNode_1612{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499 : 0000029BC7291FF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1613) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para649_x, %para650_L_layer1.0.bn3.gamma, %para651_L_layer1.0.bn3.beta, %para652_L_layer1.0.bn3.moving_mean, %para653_L_layer1.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1614) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499:CNode_1613{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer1.0.bn3.gamma, [3]: param_L_layer1.0.bn3.beta, [4]: param_L_layer1.0.bn3.moving_mean, [5]: param_L_layer1.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499:CNode_1614{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1613, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1499:CNode_1615{[0]: ValueNode<Primitive> Return, [1]: CNode_1614}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500 : 0000029BC7289B20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1617) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500:CNode_1617{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1500:CNode_1618{[0]: ValueNode<Primitive> Return, [1]: CNode_1617}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1503 : 0000029BC74E5990
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1503 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1334]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1334):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para669_x, %para15_layer1.0.down_sample_layer.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 64, 1, 1), ref_key=:layer1.0.down_sample_layer.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/0-Conv2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1503:CNode_1619{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1512 : 0000029BC728D040
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1512 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1359]() {
  %1(CNode_1621) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1620()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1512:CNode_1621{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1620}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1512:CNode_1622{[0]: ValueNode<Primitive> Return, [1]: CNode_1621}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1521 : 0000029BC72885E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1521(%para676_) {
  Return(%para676_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1521:CNode_1623{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1518 : 0000029BC728C5A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1518 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_870):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para612_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1624) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_1625) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_1626) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_1627) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_1628) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_1629) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1624{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1625{[0]: ValueNode<Primitive> getattr, [1]: CNode_1624, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1626{[0]: CNode_1625, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1627{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1628{[0]: ValueNode<Primitive> getattr, [1]: CNode_1627, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1629{[0]: CNode_1628, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1626, [2]: CNode_1629}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_1518:CNode_1630{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1519 : 0000029BC7288090
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1519 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_870]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_870):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para612_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1631) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1519:CNode_1631{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1519:out{[0]: CNode_1631, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1519:CNode_1632{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_loss_1529 : 0000029BC77A0060
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1529 parent: [subgraph @get_loss_1169]() {
  %1(CNode_1634) = call @get_loss_1633()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_1529:CNode_1634{[0]: ValueNode<FuncGraph> get_loss_1633}
#   2: @get_loss_1529:CNode_1635{[0]: ValueNode<Primitive> Return, [1]: CNode_1634}


subgraph attr:
subgraph instance: flatten_1546 : 0000029BC758DA50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1546 parent: [subgraph @flatten_160]() {
  %1(CNode_1636) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_1637) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_1638) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para466_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_1546:CNode_1636{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_1546:CNode_1637{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1636}
#   3: @flatten_1546:CNode_1638{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_1637}
#   4: @flatten_1546:CNode_1639{[0]: ValueNode<Primitive> Return, [1]: CNode_1638}


subgraph attr:
subgraph instance: flatten_1547 : 0000029BC758CA60
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1547 parent: [subgraph @flatten_1388]() {
  %1(CNode_1641) = call @flatten_1640()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1547:CNode_1641{[0]: ValueNode<FuncGraph> flatten_1640}
#   2: @flatten_1547:CNode_1642{[0]: ValueNode<Primitive> Return, [1]: CNode_1641}


subgraph attr:
after_block : 1
subgraph instance: flatten_1550 : 0000029BC7586570
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1550 parent: [subgraph @flatten_160](%para677_) {
  %1(CNode_1643) = S_Prim_equal(%para468_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_1644) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_1645) = Switch(%2, @flatten_1646, @flatten_1647)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_1648) = %3()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_1649) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_1650) = Switch(%5, @flatten_1651, @flatten_1652)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_1653) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_1550:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_phi_input}
#   2: @flatten_1550:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_phi_input}
#   3: @flatten_1550:CNode_1643{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_1550:CNode_1644{[0]: ValueNode<Primitive> Cond, [1]: CNode_1643, [2]: ValueNode<BoolImm> false}
#   5: @flatten_1550:CNode_1645{[0]: ValueNode<Primitive> Switch, [1]: CNode_1644, [2]: ValueNode<FuncGraph> flatten_1646, [3]: ValueNode<FuncGraph> flatten_1647}
#   6: @flatten_1550:CNode_1648{[0]: CNode_1645}
#   7: @flatten_1550:CNode_1649{[0]: ValueNode<Primitive> Cond, [1]: CNode_1648, [2]: ValueNode<BoolImm> false}
#   8: @flatten_1550:CNode_1650{[0]: ValueNode<Primitive> Switch, [1]: CNode_1649, [2]: ValueNode<FuncGraph> flatten_1651, [3]: ValueNode<FuncGraph> flatten_1652}
#   9: @flatten_1550:CNode_1653{[0]: CNode_1650}
#  10: @flatten_1550:CNode_1654{[0]: ValueNode<Primitive> Return, [1]: CNode_1653}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559 : 0000029BC75AB870
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_513]() {
  %1(CNode_1655) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para615_x, %para616_L_layer4.0.bn3.gamma, %para617_L_layer4.0.bn3.beta, %para618_L_layer4.0.bn3.moving_mean, %para619_L_layer4.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1656) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559:CNode_1655{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn3.gamma, [3]: param_L_layer4.0.bn3.beta, [4]: param_L_layer4.0.bn3.moving_mean, [5]: param_L_layer4.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559:CNode_1656{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1655, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1559:CNode_1657{[0]: ValueNode<Primitive> Return, [1]: CNode_1656}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565 : 0000029BC7503260
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_521]() {
  %1(CNode_1658) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para620_x, %para621_L_layer4.0.bn2.gamma, %para622_L_layer4.0.bn2.beta, %para623_L_layer4.0.bn2.moving_mean, %para624_L_layer4.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1659) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565:CNode_1658{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer4.0.bn2.gamma, [3]: param_L_layer4.0.bn2.beta, [4]: param_L_layer4.0.bn2.moving_mean, [5]: param_L_layer4.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565:CNode_1659{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1658, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1565:CNode_1660{[0]: ValueNode<Primitive> Return, [1]: CNode_1659}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1574 : 0000029BC75BA230
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1574 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1661) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para658_x, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para426_layer4.0.down_sample_layer.1.moving_mean, %para427_layer4.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1662) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1574:CNode_1661{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer4.0.down_sample_layer.1.gamma, [3]: param_layer4.0.down_sample_layer.1.beta, [4]: param_layer4.0.down_sample_layer.1.moving_mean, [5]: param_layer4.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1574:CNode_1662{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1661, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1574:CNode_1663{[0]: ValueNode<Primitive> Return, [1]: CNode_1662}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1575 : 0000029BC75B5280
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1575 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1665) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1664()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1575:CNode_1665{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1664}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1575:CNode_1666{[0]: ValueNode<Primitive> Return, [1]: CNode_1665}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581 : 0000029BC74D3010
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_584]() {
  %1(CNode_1667) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para627_x, %para628_L_layer3.0.bn3.gamma, %para629_L_layer3.0.bn3.beta, %para630_L_layer3.0.bn3.moving_mean, %para631_L_layer3.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1668) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581:CNode_1667{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn3.gamma, [3]: param_L_layer3.0.bn3.beta, [4]: param_L_layer3.0.bn3.moving_mean, [5]: param_L_layer3.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581:CNode_1668{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1667, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1581:CNode_1669{[0]: ValueNode<Primitive> Return, [1]: CNode_1668}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587 : 0000029BC74E7420
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_592]() {
  %1(CNode_1670) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para632_x, %para633_L_layer3.0.bn2.gamma, %para634_L_layer3.0.bn2.beta, %para635_L_layer3.0.bn2.moving_mean, %para636_L_layer3.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1671) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587:CNode_1670{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer3.0.bn2.gamma, [3]: param_L_layer3.0.bn2.beta, [4]: param_L_layer3.0.bn2.moving_mean, [5]: param_L_layer3.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587:CNode_1671{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1670, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1587:CNode_1672{[0]: ValueNode<Primitive> Return, [1]: CNode_1671}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1596 : 0000029BC75A2E50
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1673) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para662_x, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para428_layer3.0.down_sample_layer.1.moving_mean, %para429_layer3.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1674) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596:CNode_1673{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer3.0.down_sample_layer.1.gamma, [3]: param_layer3.0.down_sample_layer.1.beta, [4]: param_layer3.0.down_sample_layer.1.moving_mean, [5]: param_layer3.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596:CNode_1674{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1673, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596:CNode_1675{[0]: ValueNode<Primitive> Return, [1]: CNode_1674}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1597 : 0000029BC75A0920
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1597 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1677) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1676()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1597:CNode_1677{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1676}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1597:CNode_1678{[0]: ValueNode<Primitive> Return, [1]: CNode_1677}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603 : 0000029BC74F0E30
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_713]() {
  %1(CNode_1679) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para638_x, %para639_L_layer2.0.bn3.gamma, %para640_L_layer2.0.bn3.beta, %para641_L_layer2.0.bn3.moving_mean, %para642_L_layer2.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1680) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603:CNode_1679{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn3.gamma, [3]: param_L_layer2.0.bn3.beta, [4]: param_L_layer2.0.bn3.moving_mean, [5]: param_L_layer2.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603:CNode_1680{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1679, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1603:CNode_1681{[0]: ValueNode<Primitive> Return, [1]: CNode_1680}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609 : 0000029BC74EB3E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_721]() {
  %1(CNode_1682) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para643_x, %para644_L_layer2.0.bn2.gamma, %para645_L_layer2.0.bn2.beta, %para646_L_layer2.0.bn2.moving_mean, %para647_L_layer2.0.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1683) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer2-SequentialCell/0-ResidualBlock/bn2-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609:CNode_1682{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer2.0.bn2.gamma, [3]: param_L_layer2.0.bn2.beta, [4]: param_L_layer2.0.bn2.moving_mean, [5]: param_L_layer2.0.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609:CNode_1683{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1682, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1609:CNode_1684{[0]: ValueNode<Primitive> Return, [1]: CNode_1683}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616 : 0000029BC7291AA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_802]() {
  %1(CNode_1685) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para649_x, %para650_L_layer1.0.bn3.gamma, %para651_L_layer1.0.bn3.beta, %para652_L_layer1.0.bn3.moving_mean, %para653_L_layer1.0.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1686) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer1-SequentialCell/0-ResidualBlock/bn3-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616:CNode_1685{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_layer1.0.bn3.gamma, [3]: param_L_layer1.0.bn3.beta, [4]: param_L_layer1.0.bn3.moving_mean, [5]: param_L_layer1.0.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616:CNode_1686{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1685, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1616:CNode_1687{[0]: ValueNode<Primitive> Return, [1]: CNode_1686}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1620 : 0000029BC7290010
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1620 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1359]() {
  Return(%para673_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/maxpool-MaxPool2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1620:CNode_1688{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: get_loss_1633 : 0000029BC779E5D0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1633 parent: [subgraph @get_loss_1169]() {
  %1(weights) = $(get_loss_885):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para656_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_885):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para655_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_885):S_Prim_Mul[input_names: ["x", "y"], output_names: ["output"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_1370) = $(get_loss_1169):call @get_axis_1371(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(get_loss_1169):S_Prim_ReduceMean[keep_dims: Bool(0), input_names: ["input_x", "axis"], output_names: ["y"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_885):getattr(%para655_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:148/        return x/
}
# Order:
#   1: @get_loss_1633:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @get_loss_1633:CNode_1689{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
after_block : 1
subgraph instance: flatten_1640 : 0000029BC758FF80
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1640 parent: [subgraph @flatten_1388]() {
  %1(CNode_1690) = call @_get_cache_prim_65(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_1691) = %1()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(flatten_1388):S_Prim_Rank(%para466_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para466_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_1692) = call @flatten_1550(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @flatten_1640:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @flatten_1640:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @flatten_1640:CNode_1690{[0]: ValueNode<FuncGraph> _get_cache_prim_65, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @flatten_1640:CNode_1691{[0]: CNode_1690}
#   5: @flatten_1640:input{[0]: CNode_1691, [1]: param_input, [2]: new_order}
#   6: @flatten_1640:CNode_1693{[0]: ValueNode<Primitive> Return, [1]: CNode_1692}
#   7: @flatten_1640:CNode_1692{[0]: ValueNode<FuncGraph> flatten_1550, [1]: input}


subgraph attr:
subgraph instance: flatten_1651 : 0000029BC758B520
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1651 parent: [subgraph @flatten_1550]() {
  %1(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_1694) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_1695) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_1696) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_1697) = Switch(%4, @flatten_1698, @flatten_1699)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_1700) = %5()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1651:CNode_1694{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @flatten_1651:CNode_1695{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1694}
#   3: @flatten_1651:CNode_1696{[0]: ValueNode<Primitive> Cond, [1]: CNode_1695, [2]: ValueNode<BoolImm> false}
#   4: @flatten_1651:CNode_1697{[0]: ValueNode<Primitive> Switch, [1]: CNode_1696, [2]: ValueNode<FuncGraph> flatten_1698, [3]: ValueNode<FuncGraph> flatten_1699}
#   5: @flatten_1651:CNode_1700{[0]: CNode_1697}
#   6: @flatten_1651:CNode_1701{[0]: ValueNode<Primitive> Return, [1]: CNode_1700}


subgraph attr:
subgraph instance: flatten_1652 : 0000029BC75835A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1652 parent: [subgraph @flatten_1550]() {
  %1(CNode_1703) = call @flatten_1702()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_1652:CNode_1703{[0]: ValueNode<FuncGraph> flatten_1702}
#   2: @flatten_1652:CNode_1704{[0]: ValueNode<Primitive> Return, [1]: CNode_1703}


subgraph attr:
subgraph instance: flatten_1646 : 0000029BC7586AC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1646 parent: [subgraph @flatten_160]() {
  %1(CNode_1705) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_1706) = S_Prim_equal(%para469_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_1646:CNode_1705{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_1646:CNode_1706{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_1705}
#   3: @flatten_1646:CNode_1707{[0]: ValueNode<Primitive> Return, [1]: CNode_1706}


subgraph attr:
subgraph instance: flatten_1647 : 0000029BC7580B20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1647 parent: [subgraph @flatten_1550]() {
  %1(CNode_1643) = $(flatten_1550):S_Prim_equal(%para468_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_1647:CNode_1708{[0]: ValueNode<Primitive> Return, [1]: CNode_1643}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1664 : 0000029BC75B32A0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1664 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1209]() {
  %1(CNode_1709) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para658_x, %para142_layer4.0.down_sample_layer.1.gamma, %para143_layer4.0.down_sample_layer.1.beta, %para426_layer4.0.down_sample_layer.1.moving_mean, %para427_layer4.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (2048), ref_key=:layer4.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1710) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer4-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1664:CNode_1709{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer4.0.down_sample_layer.1.gamma, [3]: param_layer4.0.down_sample_layer.1.beta, [4]: param_layer4.0.down_sample_layer.1.moving_mean, [5]: param_layer4.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1664:CNode_1710{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1709, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1664:CNode_1711{[0]: ValueNode<Primitive> Return, [1]: CNode_1710}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1676 : 0000029BC759F930
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1676 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1253]() {
  %1(CNode_1712) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(0.0001), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para662_x, %para85_layer3.0.down_sample_layer.1.gamma, %para86_layer3.0.down_sample_layer.1.beta, %para428_layer3.0.down_sample_layer.1.moving_mean, %para429_layer3.0.down_sample_layer.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.gamma>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.beta>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_mean>, <Ref[Tensor[Float32]], (1024), ref_key=:layer3.0.down_sample_layer.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1713) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/layer3-SequentialCell/0-ResidualBlock/down_sample_layer-SequentialCell/1-BatchNorm2d)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1676:CNode_1712{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_layer3.0.down_sample_layer.1.gamma, [3]: param_layer3.0.down_sample_layer.1.beta, [4]: param_layer3.0.down_sample_layer.1.moving_mean, [5]: param_layer3.0.down_sample_layer.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1676:CNode_1713{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1712, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1676:CNode_1714{[0]: ValueNode<Primitive> Return, [1]: CNode_1713}


subgraph attr:
subgraph instance: flatten_1698 : 0000029BC758C510
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1698 parent: [subgraph @flatten_1550]() {
  %1(CNode_1715) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_1716) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_1717) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para677_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_1698:CNode_1715{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_1698:CNode_1716{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1715}
#   3: @flatten_1698:CNode_1717{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_1716}
#   4: @flatten_1698:CNode_1718{[0]: ValueNode<Primitive> Return, [1]: CNode_1717}


subgraph attr:
subgraph instance: flatten_1699 : 0000029BC758EA40
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1699 parent: [subgraph @flatten_1550]() {
  %1(CNode_1720) = call @flatten_1719()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1699:CNode_1720{[0]: ValueNode<FuncGraph> flatten_1719}
#   2: @flatten_1699:CNode_1721{[0]: ValueNode<Primitive> Return, [1]: CNode_1720}


subgraph attr:
after_block : 1
subgraph instance: flatten_1702 : 0000029BC7587AB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1702 parent: [subgraph @flatten_1550]() {
  %1(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_1722(%para468_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_1722(%para469_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_1724) = call @check_dim_valid_1723(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_1725) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_1726) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_1727) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_1728) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_1729) = Switch(%8, @flatten_1730, @flatten_1731)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_1732) = %9()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_1733) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1702:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_1722, [1]: param_start_dim, [2]: x_rank}
#   2: @flatten_1702:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_1722, [1]: param_end_dim, [2]: x_rank}
#   3: @flatten_1702:CNode_1724{[0]: ValueNode<FuncGraph> check_dim_valid_1723, [1]: idx, [2]: end_dim}
#   4: @flatten_1702:CNode_1726{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @flatten_1702:CNode_1727{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_1726}
#   6: @flatten_1702:CNode_1728{[0]: ValueNode<Primitive> Cond, [1]: CNode_1727, [2]: ValueNode<BoolImm> false}
#   7: @flatten_1702:CNode_1729{[0]: ValueNode<Primitive> Switch, [1]: CNode_1728, [2]: ValueNode<FuncGraph> flatten_1730, [3]: ValueNode<FuncGraph> flatten_1731}
#   8: @flatten_1702:CNode_1732{[0]: CNode_1729}
#   9: @flatten_1702:CNode_1734{[0]: ValueNode<Primitive> Return, [1]: CNode_1733}


subgraph attr:
after_block : 1
subgraph instance: flatten_1719 : 0000029BC758AFD0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1719 parent: [subgraph @flatten_1550]() {
  %1(CNode_1735) = call @_get_cache_prim_65(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_1736) = %1()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_1737) = %2(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @flatten_1719:CNode_1735{[0]: ValueNode<FuncGraph> _get_cache_prim_65, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @flatten_1719:CNode_1736{[0]: CNode_1735}
#   3: @flatten_1719:CNode_1737{[0]: CNode_1736, [1]: param_phi_input}
#   4: @flatten_1719:CNode_1738{[0]: ValueNode<Primitive> Return, [1]: CNode_1737}


subgraph attr:
subgraph instance: check_dim_valid_1723 : 0000029BC758BFC0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_1723(%para678_start_dim, %para679_end_dim) {
  %1(CNode_1739) = S_Prim_greater(%para678_start_dim, %para679_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_1740) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_1741) = Switch(%2, @check_dim_valid_1742, @check_dim_valid_1743)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_1744) = %3()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_1723:CNode_1739{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_1723:CNode_1740{[0]: ValueNode<Primitive> Cond, [1]: CNode_1739, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_1723:CNode_1741{[0]: ValueNode<Primitive> Switch, [1]: CNode_1740, [2]: ValueNode<FuncGraph> check_dim_valid_1742, [3]: ValueNode<FuncGraph> check_dim_valid_1743}
#   4: @check_dim_valid_1723:CNode_1744{[0]: CNode_1741}
#   5: @check_dim_valid_1723:CNode_1745{[0]: ValueNode<Primitive> Return, [1]: CNode_1744}


subgraph attr:
subgraph instance: canonicalize_axis_1722 : 0000029BC7588FF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_1722(%para680_axis, %para681_x_rank) {
  %1(CNode_1746) = S_Prim_not_equal(%para681_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_1747) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_1748) = Switch(%2, @canonicalize_axis_1749, @canonicalize_axis_1750)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_1752) = call @check_axis_valid_1751(%para680_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_1753) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_1754) = S_Prim_greater_equal(%para680_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_1755) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_1756) = Switch(%8, @canonicalize_axis_1757, @canonicalize_axis_1758)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_1759) = %9()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_1760) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_1722:CNode_1746{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_1722:CNode_1747{[0]: ValueNode<Primitive> Cond, [1]: CNode_1746, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_1722:CNode_1748{[0]: ValueNode<Primitive> Switch, [1]: CNode_1747, [2]: ValueNode<FuncGraph> canonicalize_axis_1749, [3]: ValueNode<FuncGraph> canonicalize_axis_1750}
#   4: @canonicalize_axis_1722:ndim{[0]: CNode_1748}
#   5: @canonicalize_axis_1722:CNode_1752{[0]: ValueNode<FuncGraph> check_axis_valid_1751, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_1722:CNode_1754{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_1722:CNode_1755{[0]: ValueNode<Primitive> Cond, [1]: CNode_1754, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_1722:CNode_1756{[0]: ValueNode<Primitive> Switch, [1]: CNode_1755, [2]: ValueNode<FuncGraph> canonicalize_axis_1757, [3]: ValueNode<FuncGraph> canonicalize_axis_1758}
#   9: @canonicalize_axis_1722:CNode_1759{[0]: CNode_1756}
#  10: @canonicalize_axis_1722:CNode_1761{[0]: ValueNode<Primitive> Return, [1]: CNode_1760}


subgraph attr:
subgraph instance: flatten_1730 : 0000029BC75825B0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1730 parent: [subgraph @flatten_1550]() {
  %1(CNode_1762) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_1763) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_1764) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para677_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_1730:CNode_1762{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_1730:CNode_1763{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1762}
#   3: @flatten_1730:CNode_1764{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_1763}
#   4: @flatten_1730:CNode_1765{[0]: ValueNode<Primitive> Return, [1]: CNode_1764}


subgraph attr:
subgraph instance: flatten_1731 : 0000029BC7580080
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1731 parent: [subgraph @flatten_1702]() {
  %1(CNode_1767) = call @flatten_1766()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_1731:CNode_1767{[0]: ValueNode<FuncGraph> flatten_1766}
#   2: @flatten_1731:CNode_1768{[0]: ValueNode<Primitive> Return, [1]: CNode_1767}


subgraph attr:
subgraph instance: check_dim_valid_1742 : 0000029BC758F4E0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_1742() {
  %1(CNode_1769) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @check_dim_valid_1742:CNode_1769{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @check_dim_valid_1742:CNode_1770{[0]: ValueNode<Primitive> Return, [1]: CNode_1769}


subgraph attr:
subgraph instance: check_dim_valid_1743 : 0000029BC758BA70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_1743() {
  %1(CNode_1772) = call @check_dim_valid_1771()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_1743:CNode_1772{[0]: ValueNode<FuncGraph> check_dim_valid_1771}
#   2: @check_dim_valid_1743:CNode_1773{[0]: ValueNode<Primitive> Return, [1]: CNode_1772}


subgraph attr:
subgraph instance: check_axis_valid_1751 : 0000029BC7589FE0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1751(%para682_axis, %para683_ndim) {
  %1(CNode_1774) = S_Prim_negative(%para683_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1775) = S_Prim_less(%para682_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_1776) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_1777) = Switch(%3, @check_axis_valid_1778, @check_axis_valid_1779)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_1780) = %4()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_1781) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_1782) = Switch(%6, @check_axis_valid_1783, @check_axis_valid_1784)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_1785) = %7()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_1751:CNode_1774{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_1751:CNode_1775{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_1774}
#   3: @check_axis_valid_1751:CNode_1776{[0]: ValueNode<Primitive> Cond, [1]: CNode_1775, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_1751:CNode_1777{[0]: ValueNode<Primitive> Switch, [1]: CNode_1776, [2]: ValueNode<FuncGraph> check_axis_valid_1778, [3]: ValueNode<FuncGraph> check_axis_valid_1779}
#   5: @check_axis_valid_1751:CNode_1780{[0]: CNode_1777}
#   6: @check_axis_valid_1751:CNode_1781{[0]: ValueNode<Primitive> Cond, [1]: CNode_1780, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_1751:CNode_1782{[0]: ValueNode<Primitive> Switch, [1]: CNode_1781, [2]: ValueNode<FuncGraph> check_axis_valid_1783, [3]: ValueNode<FuncGraph> check_axis_valid_1784}
#   8: @check_axis_valid_1751:CNode_1785{[0]: CNode_1782}
#   9: @check_axis_valid_1751:CNode_1786{[0]: ValueNode<Primitive> Return, [1]: CNode_1785}


subgraph attr:
subgraph instance: canonicalize_axis_1749 : 0000029BC7590A20
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_1749 parent: [subgraph @canonicalize_axis_1722]() {
  Return(%para681_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_1749:CNode_1787{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: canonicalize_axis_1750 : 0000029BC7581B10
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_1750() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_1750:CNode_1788{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: canonicalize_axis_1757 : 0000029BC7589540
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_1757 parent: [subgraph @canonicalize_axis_1722]() {
  Return(%para680_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_1757:CNode_1789{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: canonicalize_axis_1758 : 0000029BC75815C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_1758 parent: [subgraph @canonicalize_axis_1722]() {
  %1(CNode_1746) = $(canonicalize_axis_1722):S_Prim_not_equal(%para681_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_1747) = $(canonicalize_axis_1722):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_1748) = $(canonicalize_axis_1722):Switch(%2, @canonicalize_axis_1749, @canonicalize_axis_1750)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_1722):%3()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_1790) = S_Prim_add(%para680_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_1758:CNode_1790{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @canonicalize_axis_1758:CNode_1791{[0]: ValueNode<Primitive> Return, [1]: CNode_1790}


subgraph attr:
after_block : 1
subgraph instance: flatten_1766 : 0000029BC7583AF0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1766 parent: [subgraph @flatten_1702]() {
  %1(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_1702):call @canonicalize_axis_1722(%para468_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(flatten_1702):call @canonicalize_axis_1722(%para469_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_1792) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_1793) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_1794) = Switch(%5, @flatten_1795, @flatten_1796)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_1797) = %6()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_1766:CNode_1792{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @flatten_1766:CNode_1793{[0]: ValueNode<Primitive> Cond, [1]: CNode_1792, [2]: ValueNode<BoolImm> false}
#   3: @flatten_1766:CNode_1794{[0]: ValueNode<Primitive> Switch, [1]: CNode_1793, [2]: ValueNode<FuncGraph> flatten_1795, [3]: ValueNode<FuncGraph> flatten_1796}
#   4: @flatten_1766:CNode_1797{[0]: CNode_1794}
#   5: @flatten_1766:CNode_1798{[0]: ValueNode<Primitive> Return, [1]: CNode_1797}


subgraph attr:
after_block : 1
subgraph instance: check_dim_valid_1771 : 0000029BC758EF90
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_1771() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_dim_valid_1771:CNode_1799{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: check_axis_valid_1783 : 0000029BC75914C0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1783() {
  %1(CNode_1800) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_1783:CNode_1800{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_1783:CNode_1801{[0]: ValueNode<Primitive> Return, [1]: CNode_1800}


subgraph attr:
subgraph instance: check_axis_valid_1784 : 0000029BC758D500
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1784() {
  %1(CNode_1803) = call @check_axis_valid_1802()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_1784:CNode_1803{[0]: ValueNode<FuncGraph> check_axis_valid_1802}
#   2: @check_axis_valid_1784:CNode_1804{[0]: ValueNode<Primitive> Return, [1]: CNode_1803}


subgraph attr:
subgraph instance: check_axis_valid_1778 : 0000029BC758A530
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1778 parent: [subgraph @check_axis_valid_1751]() {
  %1(CNode_1774) = $(check_axis_valid_1751):S_Prim_negative(%para683_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_1775) = $(check_axis_valid_1751):S_Prim_less(%para682_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_1778:CNode_1805{[0]: ValueNode<Primitive> Return, [1]: CNode_1775}


subgraph attr:
subgraph instance: check_axis_valid_1779 : 0000029BC758CFB0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1779 parent: [subgraph @check_axis_valid_1751]() {
  %1(CNode_1806) = S_Prim_greater_equal(%para682_axis, %para683_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_1779:CNode_1806{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_1779:CNode_1807{[0]: ValueNode<Primitive> Return, [1]: CNode_1806}


subgraph attr:
subgraph instance: flatten_1795 : 0000029BC7584590
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1795 parent: [subgraph @flatten_1550]() {
  Return(%para677_phi_input)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1763/        return input/
}
# Order:
#   1: @flatten_1795:CNode_1808{[0]: ValueNode<Primitive> Return, [1]: param_phi_input}


subgraph attr:
subgraph instance: flatten_1796 : 0000029BC7588AA0
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1796 parent: [subgraph @flatten_1702]() {
  %1(CNode_1810) = call @flatten_1809()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_1796:CNode_1810{[0]: ValueNode<FuncGraph> flatten_1809}
#   2: @flatten_1796:CNode_1811{[0]: ValueNode<Primitive> Return, [1]: CNode_1810}


subgraph attr:
after_block : 1
subgraph instance: check_axis_valid_1802 : 0000029BC7590F70
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1802() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_axis_valid_1802:CNode_1812{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_1809 : 0000029BC7588550
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1809 parent: [subgraph @flatten_1702]() {
  %1(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_1702):call @canonicalize_axis_1722(%para468_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_1814) = call @flatten_1813(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1809:CNode_1815{[0]: ValueNode<Primitive> Return, [1]: CNode_1814}
#   2: @flatten_1809:CNode_1814{[0]: ValueNode<FuncGraph> flatten_1813, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: flatten_1813 : 0000029BC7582060
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1813 parent: [subgraph @flatten_1702](%para684_, %para685_) {
  %1(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(flatten_1702):call @canonicalize_axis_1722(%para469_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_1816) = S_Prim_less_equal(%para684_phi_idx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_1816) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_1817) = Switch(%4, @flatten_1818, @flatten_1819)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_1820) = %5()
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1813:CNode_1816{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_phi_idx, [2]: end_dim}
#   2: @flatten_1813:force_while_cond_CNode_1816{[0]: ValueNode<Primitive> Cond, [1]: CNode_1816, [2]: ValueNode<BoolImm> true}
#   3: @flatten_1813:CNode_1817{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_1816, [2]: ValueNode<FuncGraph> flatten_1818, [3]: ValueNode<FuncGraph> flatten_1819}
#   4: @flatten_1813:CNode_1820{[0]: CNode_1817}
#   5: @flatten_1813:CNode_1821{[0]: ValueNode<Primitive> Return, [1]: CNode_1820}


subgraph attr:
subgraph instance: flatten_1818 : 0000029BC7581070
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1818 parent: [subgraph @flatten_1813]() {
  %1(idx) = S_Prim_add(%para684_phi_idx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1769/        idx += 1/
  %2(x_shape) = $(flatten_1550):S_Prim_Shape(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_1822) = S_Prim_getitem(%2, %para684_phi_idx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para685_phi_dim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_1823) = call @flatten_1813(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1818:CNode_1822{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_phi_idx}
#   2: @flatten_1818:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_phi_dim_length, [2]: CNode_1822}
#   3: @flatten_1818:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_phi_idx, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_1818:CNode_1824{[0]: ValueNode<Primitive> Return, [1]: CNode_1823}
#   5: @flatten_1818:CNode_1823{[0]: ValueNode<FuncGraph> flatten_1813, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: flatten_1819 : 0000029BC7588000
# In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1819 parent: [subgraph @flatten_1813]() {
  %1(x_shape) = $(flatten_1550):S_Prim_Shape(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(flatten_1550):S_Prim_Rank(%para677_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(flatten_1702):call @canonicalize_axis_1722(%para468_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_1825) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_1826) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_1827) = S_Prim_MakeTuple(%para685_phi_dim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_1828) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(flatten_1702):call @canonicalize_axis_1722(%para469_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_1829) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_1830) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_1831) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_1832) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para677_phi_input, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-ResNet/flatten-Flatten)
      # In file c:\Users\Azra\.vscode\venv\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @flatten_1819:CNode_1825{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @flatten_1819:CNode_1826{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1825}
#   3: @flatten_1819:CNode_1827{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_phi_dim_length}
#   4: @flatten_1819:CNode_1828{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1826, [2]: CNode_1827}
#   5: @flatten_1819:CNode_1829{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @flatten_1819:CNode_1830{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_1829, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @flatten_1819:CNode_1831{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1830}
#   8: @flatten_1819:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1828, [2]: CNode_1831}
#   9: @flatten_1819:CNode_1832{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: new_shape}
#  10: @flatten_1819:CNode_1833{[0]: ValueNode<Primitive> Return, [1]: CNode_1832}


